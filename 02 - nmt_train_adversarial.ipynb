{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generator with Adversarial Discriminator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31414"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "len(dictionary.word2idx)\n",
    "# with open(\"data/bow_1m_nmt.pkl\", \"rb\") as file:\n",
    "#     nmt_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/nmt_trainset.pkl\", \"rb\") as file:\n",
    "    nmt_trainset = pickle.load(file)\n",
    "with open(\"./data/nmt_validset.pkl\", \"rb\") as file:\n",
    "    nmt_validset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmt_trainset = nmt_trainset[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_trainset), len(nmt_validset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA & 3. Preprocessing\n",
    "- Done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.float32)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "generator = torch.Generator().manual_seed(6969)\n",
    "train_dataloader = DataLoader(nmt_trainset, batch_size=16, shuffle=True, collate_fn=collate_batch, generator=generator)\n",
    "valid_dataloader = DataLoader(nmt_validset, batch_size=16, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sen, syn, trg, adv in train_dataloader:\n",
    "#     print(sen.shape)\n",
    "#     print(syn.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(adv.shape)\n",
    "#     print(adv)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import random\n",
    "\n",
    "# random.seed(6969)\n",
    "# random.shuffle(nmt_dataset)\n",
    "\n",
    "# train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "# train_set = nmt_dataset[:train_range]\n",
    "# val_set   = nmt_dataset[train_range:]\n",
    "# # test_set = train_data[90:]\n",
    "\n",
    "# train_dataloader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "# valid_dataloader = DataLoader(val_set, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sen, syn, trg, adv in train_dataloader:\n",
    "#     print(sen.shape)\n",
    "#     print(syn.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(adv.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        # sent_embeddings : batch_size, seq_len, hid_dim\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1) # batch_size, hid_dim\n",
    "        x = self.adv(x) # batch_size, hid_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, input_token_ids):\n",
    "        mask = input_token_ids != pad_idx\n",
    "        mean_mask = mask.float()/mask.float().sum(1, keepdim=True)\n",
    "        \n",
    "        x = (x * mean_mask.unsqueeze(2)).sum(1, keepdim=True)\n",
    "\n",
    "        x = torch.where(torch.isnan(x), torch.zeros_like(x), x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "\n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "        self.pooling = MeanPooling()\n",
    "\n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def load_embedding(self, embedding): \n",
    "            self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "            self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        #synts : batch_size, synt_len, emb_dim\n",
    "        #trgs  : batch_size, trg_len, emb_dim \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1)   - 2  # without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale # sent_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale # synt_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings) # synt_len, batch_size, emb_dim\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0) # synt_len + seq_len, batch size, emb_size\n",
    "        # print(encoder_embeddings.shape)\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        sentence_src_mask = self.generate_square_mask(max_sent_len, -2).to(self.device)\n",
    "        # target => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings) # trg_len, batch_size, emb_dim\n",
    "\n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "\n",
    "        # encoder outputs\n",
    "        memory = self.transformer.encoder(sent_embeddings, mask=sentence_src_mask)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        sent_embeds = self.pooling(memory, sents)\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "        adv_outputs = self.adversary(sent_embeds).transpose(0, 1) # batch_size, 74   \n",
    "        \n",
    "        # forward\n",
    "        outputs = self.transformer(encoder_embeddings, decoder_embeddings, src_mask=src_mask, tgt_mask=trg_mask) # trg_len, batch_size, emb_dim\n",
    "        #decoder outputs # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1) # batch_size, trg_len, emb_dim\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim)) # batch_size*trg_len, input_dim\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim) # batch_size, trg_len, input_dim\n",
    "        \n",
    "        return outputs, adv_outputs\n",
    "    \n",
    "    def forward_adv(self, sents):\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, -2).to(self.device)\n",
    "\n",
    "        # encoder outputs\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale # sent_len, batch_size, emb_dim\n",
    "\n",
    "        memory = self.transformer.encoder(sent_embeddings, mask=src_mask)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        sent_embeds = self.pooling(memory, sents) #batch_size, 74\n",
    "        adv_outputs = self.adversary(sent_embeds)\n",
    "        return adv_outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len, emb_dim \n",
    "        #synts  : batch_size, seq_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings)\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode outputs\n",
    "        memory = self.transformer.encoder(encoder_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            # if i % 5 == 0:\n",
    "            #     print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(decoder_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            decoder_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = 'data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  \n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "49621464 parameters\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "    #     print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6} parameters')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_165431/3737267708.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for idx, (sents_, synts_, trgs_, adv_targs) in enumerate(tqdm(loader)):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        \n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "        #forward adversarial\n",
    "        outputs = model.forward_adv(sents_) #batch_size, 74\n",
    "        #optimize adversarial\n",
    "        adv_loss = adv_criterion(outputs, adv_targs)\n",
    "        # print('adv_loss',adv_loss)\n",
    "        adv_loss.backward()\n",
    "        adv_total_loss += adv_loss.item()\n",
    "\n",
    "        # if (idx+1) % accumulation_steps == 0:\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        if epoch > 0:\n",
    "            adv_optimizer.step()\n",
    "        adv_optimizer.zero_grad()\n",
    "\n",
    "        #foward model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #seq_len, batch_size, 74\n",
    "        \n",
    "        # calculate loss\n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "        \n",
    "        #optimize model\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "        # print('optim loss',loss)\n",
    "        if epoch > 0: \n",
    "            loss -= 1 * adv_criterion(adv_outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        # print('epoch_loss',epoch_loss)\n",
    "\n",
    "        # if (idx+1) % accumulation_steps == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        para_optimizer.step()\n",
    "        para_optimizer.zero_grad()\n",
    "\n",
    "    # return epoch_loss / loader_length, adv_total_loss / loader_length\n",
    "    return epoch_loss, adv_total_loss\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            \n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "\n",
    "            # if torch.isnan(adv_criterion(adv_outputs, adv_targs)) != True:\n",
    "            #     adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "\n",
    "\n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "# lr = 10e-4 #Following SynPG\n",
    "fast_lr = 1e-4\n",
    "lr = 2e-5\n",
    "wd = 1e-2 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=fast_lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62500, 313)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))\n",
    "\n",
    "train_loader_length, val_loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62500/62500 [1:30:14<00:00, 11.54it/s]\n",
      "100%|██████████| 313/313 [00:07<00:00, 41.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 90m 21s\n",
      "\tTrain Loss: 6.034 | Train PPL: 417.245\n",
      "\t Val. Loss: 6.767 |  Val. PPL: 868.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2641/62500 [04:16<1:36:47, 10.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m \u001b[39m# training \u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_loss, adv_total_loss \u001b[39m=\u001b[39m train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length, accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m para_loss, adv_loss \u001b[39m=\u001b[39m evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n\u001b[1;32m     21\u001b[0m valid_loss \u001b[39m=\u001b[39m para_loss \u001b[39m-\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m adv_loss\n",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \n\u001b[1;32m     49\u001b[0m     loss \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m*\u001b[39m adv_criterion(adv_outputs, adv_targs)\n\u001b[0;32m---> 50\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     51\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m \u001b[39m# print('epoch_loss',epoch_loss)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[39m# if (idx+1) % accumulation_steps == 0:\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt_modify.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "para_losses = []\n",
    "adv_losses = [] \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss, adv_total_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length, accumulation_steps=1)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(float(valid_loss))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, vocab_transform, device):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/mrpc_target_sents_adv.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/mrpc_target_synts_adv.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/mrpc_outputs_adv.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # Put input into device\n",
    "                sents_ = sents_.to(device)\n",
    "                synts_ = synts_.to(device)\n",
    "                trgs_ = trgs_.to(device)\n",
    "                adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    # fp1.write(targ+'\\n')\n",
    "                    # fp2.write(synt_+'\\n')\n",
    "                    # fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')\n",
    "                    \n",
    "                    convert_sent = reverse_bpe(sent2str(sent.detach().cpu().numpy(), vocab_transform).split()) + '\\n'\n",
    "                    convert_synt = synt2str(synt_[1:].tolist(), vocab_transform).replace(\"<pad>\", \"\") + '\\n' \n",
    "                    convert_idx = synt2str(idx, vocab_transform)\n",
    "                    convert_idx = reverse_bpe(convert_idx.split()) + '\\n'\n",
    "                    \n",
    "                    fp1.write(convert_sent)\n",
    "                    fp2.write(convert_synt)\n",
    "                    fp3.write(convert_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== loading data ====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['sents1', 'sents2', 'synts1', 'synts2']>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py, os\n",
    "print(\"==== loading data ====\")\n",
    "mrpc_set = h5py.File(os.path.join('./test_data/test_data_mrpc.h5'), 'r')\n",
    "mrpc_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"amrozi accused his brother , whom he called `` the witness '' , of deliberately distorting his evidence .\",\n",
       " \"(ROOT (S (VP (VBG referring) (PP (TO to) (NP (PRP him))) (PP (IN as) (NP (NP (RB only) (`` ``) (NP (DT the) (NN witness)) ('' '')) (, ,) (SBAR (IN amrozi) (S (VP (VBN accused) (NP (NP (PRP$ his) (NN brother)) (PP (IN of) (S (ADVP (RB deliberately)) (VP (VBG distorting) (NP (PRP$ his) (NN evidence)))))))))))) (. .)))\",\n",
       " \"referring to him as only `` the witness '' , amrozi accused his brother of deliberately distorting his evidence .\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrpc_set['sents1'][0].decode(), mrpc_set['synts2'][0].decode(), mrpc_set['sents2'][0].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== loading data ====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['sents1', 'sents2', 'synts1', 'synts2']>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py, os\n",
    "print(\"==== loading data ====\")\n",
    "qq_set = h5py.File(os.path.join('./test_data/test_data_quora.h5'), 'r')\n",
    "qq_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what minor would complement a computer science major ?',\n",
       " '(ROOT (SBARQ (WHNP (WP what)) (SQ (VBZ is) (NP (NP (DT a) (JJ good) (JJ minor)) (SBAR (WHNP (WDT that)) (S (VP (MD will) (VP (VB compliment) (NP (DT a) (NN computer) (NN science)) (ADVP (JJ major)))))))) (. ?)))',\n",
       " 'what is a good minor that will compliment a computer science major ?')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq_set['sents1'][0].decode(), qq_set['synts2'][0].decode(), qq_set['sents2'][0].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\"\n",
    "\n",
    "def getleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "    \n",
    "    leaves = []\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                leaves.append(arr[n])\n",
    "\n",
    "    return leaves\n",
    "\n",
    "def deleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                arr[n + 1] = \"\"\n",
    "\n",
    "    nonleaves = \" \".join(arr)\n",
    "    return nonleaves.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer BPE\n",
    "from subwordnmt.apply_bpe import BPE, read_vocabulary\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "# load bpe codes\n",
    "bpe_codes = codecs.open('./data/bpe.codes', encoding='utf-8')\n",
    "bpe_vocab = codecs.open('./data/vocab.txt', encoding='utf-8')\n",
    "bpe_vocab = read_vocabulary(bpe_vocab, 50)\n",
    "bpe = BPE(bpe_codes, '@@', bpe_vocab, None)\n",
    "\n",
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "def bpe_tokenizer(sent_, target = False):\n",
    " # bpe segment and convert to tensor\n",
    "    sent_ = bpe.segment(sent_).split()\n",
    "    sent_ = [dictionary.word2idx[w] if w in dictionary.word2idx else dictionary.word2idx[\"<unk>\"] for w in sent_]\n",
    "    if target:\n",
    "        sent_ = [dictionary.word2idx[\"<sos>\"]] + sent_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return sent_\n",
    "\n",
    "#syntax to syntatic tokenzier\n",
    "from nltk import ParentedTree\n",
    "def parser_tokenizer(synt_):\n",
    "    synt_  = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    synt_ = [dictionary.word2idx[f\"<{w}>\"] for w in synt_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    synt_ = [dictionary.word2idx[\"<sos>\"]] + synt_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return synt_\n",
    "\n",
    "import pickle\n",
    "with open('synt_vocab.pkl', 'rb') as f:\n",
    "    synt_vocab = pickle.load(f)\n",
    "\n",
    "def bow(synt_):\n",
    "    synt_bow = np.ones(74)\n",
    "    synt_ = ['<sos>'] + deleaf(synt_) + ['<eos>']\n",
    "    for tag in synt_:\n",
    "        if tag != '<sos>' and tag != '<eos>':\n",
    "            synt_bow[synt_vocab[tag]-3] += 1\n",
    "    synt_bow /= synt_bow.sum()\n",
    "    return synt_bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_paraphrase_dataset(sent1, synt2, sent2):\n",
    "    lists_ = list()\n",
    "    for sen1, syn2, sen2 in tqdm(zip(sent1,synt2,sent2)):\n",
    "        sent_ = bpe_tokenizer(sen1.decode())\n",
    "        syn_  = parser_tokenizer(syn2.decode())\n",
    "        trg_  = bpe_tokenizer(sen2.decode(), target = True)\n",
    "        bow_  = bow(syn2.decode())\n",
    "        lists_.append((sent_, syn_, trg_, bow_))\n",
    "    return lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1920it [00:01, 1118.74it/s]\n"
     ]
    }
   ],
   "source": [
    "mrpc_dataset = prepare_paraphrase_dataset(mrpc_set['sents1'], mrpc_set['synts2'], mrpc_set['sents2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(mrpc_dataset)\n",
    "\n",
    "test_dataloader = DataLoader(mrpc_dataset, batch_size=16, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim   = len(vocab_dict)\n",
    "emb_dim     = 300  \n",
    "word_dropout = 0.4\n",
    "dropout      = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)\n",
    "\n",
    "save_path = f'models/adversary_nmt_modify.pt' #Change here\n",
    "model.load_state_dict(torch.load(save_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:20<00:00,  5.85it/s]\n"
     ]
    }
   ],
   "source": [
    "generate(model, test_dataloader, dictionary, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 160\n"
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents_adv.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs_adv.txt') as fp: \n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the face of president saddam hussein was added to iraqi currency after the 1991 gulf war .\\n',\n",
       " 'the the a the the be this the the meaningof you .\\n')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0], preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 9.747954332029078\n"
     ]
    }
   ],
   "source": [
    "scores = [cal_bleu(pred, targ, 1) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
