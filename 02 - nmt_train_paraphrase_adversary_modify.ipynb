{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generator with Adversarial Discriminator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "with open(\"data/aLL_bow_100k.pkl\", \"rb\") as file:\n",
    "    nmt_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmt_dataset = nmt_dataset[:10000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA & 3. Preprocessing\n",
    "- Done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.float32)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_dataset)\n",
    "\n",
    "train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "train_set = nmt_dataset[:train_range]\n",
    "val_set   = nmt_dataset[train_range:]\n",
    "# test_set = train_data[90:]\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_set, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sen, syn, trg, adv in train_dataloader:\n",
    "#     print(sen.shape)\n",
    "#     print(syn.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(adv.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        # sent_embeddings : batch_size, seq_len, hid_dim\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1) # batch_size, hid_dim\n",
    "        x = self.adv(x) # batch_size, hid_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, input_token_ids):\n",
    "        mask = input_token_ids != pad_idx\n",
    "        mean_mask = mask.float()/mask.float().sum(1, keepdim=True)\n",
    "        x = (x * mean_mask.unsqueeze(2)).sum(1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "\n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "        self.pooling = MeanPooling()\n",
    "\n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def load_embedding(self, embedding): \n",
    "            self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "            self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        #synts : batch_size, synt_len, emb_dim\n",
    "        #trgs  : batch_size, trg_len, emb_dim \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1)   - 2  # without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents_ = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents_).transpose(0, 1) * self.scale # sent_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale # synt_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings) # synt_len, batch_size, emb_dim\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0) # synt_len + seq_len, batch size, emb_size\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "\n",
    "        # target => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings) # trg_len, batch_size, emb_dim\n",
    "\n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "\n",
    "        # forward\n",
    "        outputs = self.transformer(encoder_embeddings, decoder_embeddings, src_mask=src_mask, tgt_mask=trg_mask) # trg_len, batch_size, emb_dim\n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1) # batch_size, trg_len, emb_dim\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim)) # batch_size*trg_len, input_dim\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim) # batch_size, trg_len, input_dim\n",
    "\n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "\n",
    "        adv_outputs = self.adversary(sent_embeds).transpose(0, 1) # batch_size, 74   \n",
    "\n",
    "        return outputs, adv_outputs\n",
    "        \n",
    "    def forward_token(self, sents):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(sents.shape)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        sent_embeddings = self.embedding_encoder(sents) # batch_size, sent_len, emb_dim\n",
    "        sent_embeddings = self.positional_encoder(sent_embeddings) # batch_size, sent_len, emb_dim\n",
    "\n",
    "        sent_embeddings = self.norm(sent_embeddings)\n",
    "        sent_embeddings = F.dropout(sent_embeddings, p=self.dropout)\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        for encoder_layer in self.transformer.encoder.layers:\n",
    "            sent_embeddings = encoder_layer(sent_embeddings)\n",
    "\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        return sent_embeddings \n",
    "    \n",
    "    def forward_adv(self, sents):\n",
    "\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "        \n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "        adv_outputs = self.adversary(sent_embeds)\n",
    "        return adv_outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len, emb_dim \n",
    "        #synts  : batch_size, seq_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings)\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(encoder_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            # if i % 5 == 0:\n",
    "            #     print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(decoder_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            decoder_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = 'data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "49621464 parameters\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "    #     print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6} parameters')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41394/329192520.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        \n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "        \n",
    "        #optimize adversarial\n",
    "        outputs = model.forward_adv(sents_) #batch_size, 74\n",
    "\n",
    "        loss = adv_criterion(outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        adv_total_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        if epoch > 1:\n",
    "            adv_optimizer.step()\n",
    "        adv_optimizer.zero_grad()\n",
    "\n",
    "        #optimize model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "\n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #seq_len, batch_size, 74\n",
    "\n",
    "        if epoch > 1: \n",
    "            loss -= 0.1 * adv_criterion(adv_outputs, adv_targs)\n",
    "\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        para_optimizer.step()\n",
    "        para_optimizer.zero_grad()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            \n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "            \n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-5 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:25<00:00, 17.32it/s]\n",
      "100%|██████████| 3750/3750 [00:53<00:00, 70.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 18s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:28<00:00, 17.20it/s]\n",
      " 27%|██▋       | 1027/3750 [00:14<00:38, 71.27it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 2\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt_modify.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(float(valid_loss))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAEoCAYAAADMhS+0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApqElEQVR4nO3de1xUdd4H8M+IzoDCDBCiuI4JKoqoSKK+kMpsMdQkNEs3WIXy8qioJYub1FNYlrCt61pGtFlpbbpU5m3DuyveghUvqE8QBnJzE82eZEBzQPg9f/RiHofrMMzMgTmf9+s1Lz3n/M7vfOc3wGfOZeYohBACREREMtBF6gKIiIhshaFHRESywdAjIiLZYOgREZFsMPSIiEg2GHpERCQbDD0iIpINhh4REckGQ4+IiGSDoUdERLLRVcqN9+/fHyUlJY3mL168GCkpKa2uX1dXhx9++AEuLi5QKBTWKJGIiDoBIQQqKyvRp08fdOnS/P6cpKGXnZ2N2tpaw/T//M//YOLEiXj66adNWv+HH36AVqu1VnlERNTJlJWVoW/fvs0ulzT0evbsaTSdnJyMAQMGYPz48Sat7+LiAuDXJ6lWqy1eHxERdQ46nQ5ardaQC82RNPTuVV1djc8++wxxcXHNHqrU6/XQ6/WG6crKSgCAWq1m6BERUaunujrMhSw7d+7EzZs3ERMT02ybpKQkaDQaw4OHNomIqC0UHeV+emFhYVAqlfjnP//ZbJuGe3r1u7MVFRXc0yMikjGdTgeNRtNqHnSIw5slJSU4dOgQtm/f3mI7lUoFlUplo6qIiMjedIjDm5s2bYKnpycef/xxqUshIiI7Jnno1dXVYdOmTYiOjkbXrh1ix5OIiOyU5KF36NAhlJaW4rnnnpO6FCIisnOS71o99thj6CDX0pA9EuLXB8T/T0O08i9aXtZh+2swv0P119y6LY1NS9u2ZH+tvD5t6q+1MWpLf6a0NbVNe38WGjy3dvfXYD4APLsHcHKDtUkeepK7/h1w6gO0/kICrb+Qrf2iNvcLa25/LfQv2S++KW1ba4N7+mvHLxYRdR51ta23sQCGnu4KcPojqaugTk8BKBT3/NvUvCb+Be6Z19I6pvQHE/ppuM3m/m3ueZnTH0zcZmvPE23YZmv9wcRtmvI8G85vT38wcZumPM8m1rVofxZ8ngoFoLLNx84Yem7ewPiVzbxAgHm/nE21QfNtW1pmrf4s+jzlOm739k9EnQFD774BwIQEqasgIiIbkPzqTSIiIlth6BERkWww9IiISDYYekREJBsMPSIikg2GHhERyQZDj4iIZIOhR0REssHQIyIi2WDoERGRbDD0iIhINhh6REQkGww9IiKSDYYeERHJBkOPiIhkg6FHRESyIXno/ec//8Hvf/973HfffXBycsLw4cNx+vRpqcsiIiI7JOmd03/++WeEhIRgwoQJ2Lt3L3r27Invv/8ebm5uUpZFRER2StLQ+9Of/gStVotNmzYZ5nl7e0tYERER2TNJD2/u3r0bQUFBePrpp+Hp6YnAwEBs3Lix2fZ6vR46nc7oQUREZCpJQ+/y5ctITU3FoEGDsH//fixatAjLli3DJ5980mT7pKQkaDQaw0Or1dq4YiIi6swUQggh1caVSiWCgoLwzTffGOYtW7YM2dnZyMzMbNRer9dDr9cbpnU6HbRaLSoqKqBWq21SMxERdTw6nQ4ajabVPJB0T8/LywtDhw41mufn54fS0tIm26tUKqjVaqMHERGRqSQNvZCQEOTn5xvNu3TpEu6//36JKiIiInsmaegtX74cWVlZWLNmDQoKCrB161Z88MEHiI2NlbIsIiKyU5KG3ujRo7Fjxw784x//wLBhw7B69WqsX78eUVFRUpZFRER2StILWdrL1BOXRERk3zrFhSxERES2xNAjIiLZYOgREZFsMPSIiEg2GHpERCQbDD0iIpINhh4REckGQ4+IiGSDoUdERLLB0CMiItlg6BERkWww9IiISDYYekREJBsMPSIikg2GHhERyQZDj4iIZIOhR0REssHQIyIi2WDoERGRbDD0iIhINhh6REQkG5KG3qpVq6BQKIweQ4YMkbIkIiKyY12lLsDf3x+HDh0yTHftKnlJRERkpyRPmK5du6J3795Sl0FERDIg+Tm977//Hn369IGPjw+ioqJQWlrabFu9Xg+dTmf0ICIiMpWkoTd27Fhs3rwZ+/btQ2pqKoqKivDQQw+hsrKyyfZJSUnQaDSGh1artXHFRETUmSmEEELqIurdvHkT999/P9atW4e5c+c2Wq7X66HX6w3TOp0OWq0WFRUVUKvVtiyViIg6EJ1OB41G02oeSH5O716urq7w9fVFQUFBk8tVKhVUKpWNqyIiInsh+Tm9e1VVVaGwsBBeXl5Sl0JERHZI0tCLj4/H0aNHUVxcjG+++QbTp0+Hg4MDnnnmGSnLIiIiOyXp4c0rV67gmWeewU8//YSePXviwQcfRFZWFnr27CllWUREZKckDb20tDQpN09ERDLToc7pERERWRNDj4iIZIOhR0REssHQIyIi2WDoERGRbDD0iIhINhh6REQkGww9IiKSDYYeERHJBkOPiIhkg6FHRESywdAjIiLZYOgREZFsMPSIiEg2GHpERCQbDD0iIpINhh4REckGQ4+IiGSjq9QFEBHZSm1tLWpqaqQug8zQrVs3ODg4tLsfhh4R2T0hBMrLy3Hz5k2pS6F2cHV1Re/evaFQKMzuo8OEXnJyMhISEvD8889j/fr1UpdDRHakPvA8PT3RvXv3dv3RJNsTQuD27du4fv06AMDLy8vsvjpE6GVnZ+Nvf/sbRowYIXUpRGRnamtrDYF33333SV0OmcnJyQkAcP36dXh6epp9qFPyC1mqqqoQFRWFjRs3ws3NTepyiMjO1J/D6969u8SVUHvVv4btOS8reejFxsbi8ccfR2hoaKtt9Xo9dDqd0YOIyBQ8pNn5WeI1lPTwZlpaGs6ePYvs7GyT2iclJeG1116zclVERGSvJNvTKysrw/PPP48tW7bA0dHRpHUSEhJQUVFheJSVlVm5SiIi+9G/f/92XyhoiT6kJNme3pkzZ3D9+nU88MADhnm1tbU4duwY3n33Xej1+kYnKlUqFVQqla1LJSKSxCOPPIKRI0daLGSys7PRo0cPi/TVWUkWer/97W9x8eJFo3nPPvsshgwZghdffNEiH0IkIrJ3QgjU1taia9fW/5z37NnTBhV1bJId3nRxccGwYcOMHj169MB9992HYcOGSVUWEVGHEBMTg6NHj+Ltt9+GQqGAQqFAcXExMjIyoFAosHfvXowaNQoqlQonTpxAYWEhIiIi0KtXLzg7O2P06NE4dOiQUZ8ND00qFAp8+OGHmD59Orp3745BgwZh9+7dbaqztLQUERERcHZ2hlqtxsyZM3Ht2jXD8vPnz2PChAlwcXGBWq3GqFGjcPr0aQBASUkJwsPD4ebmhh49esDf3x979uwxf9BM0CE+p0dEZEtCCPxSUyvJtp26OZh0FeLbb7+NS5cuYdiwYXj99dcB/LqnVlxcDABYuXIl1q5dCx8fH7i5uaGsrAxTpkzBm2++CZVKhU8//RTh4eHIz89Hv379mt3Oa6+9hrfeegt//vOfsWHDBkRFRaGkpATu7u6t1lhXV2cIvKNHj+Lu3buIjY3FrFmzkJGRAQCIiopCYGAgUlNT4eDggJycHHTr1g3Ar1fvV1dX49ixY+jRowdyc3Ph7Ozc6nbbo0OFXv0gERFZ0y81tRj66n5Jtp37ehi6K1v/06vRaKBUKtG9e3f07t270fLXX38dEydONEy7u7sjICDAML169Wrs2LEDu3fvxpIlS5rdTkxMDJ555hkAwJo1a/DOO+/g1KlTmDRpUqs1Hj58GBcvXkRRURG0Wi0A4NNPP4W/vz+ys7MxevRolJaWYsWKFRgyZAgAYNCgQYb1S0tLMWPGDAwfPhwA4OPj0+o220vyz+kREVHbBQUFGU1XVVUhPj4efn5+cHV1hbOzM/Ly8lBaWtpiP/d+E1aPHj2gVqsNX/fVmry8PGi1WkPgAcDQoUPh6uqKvLw8AEBcXBzmzZuH0NBQJCcno7Cw0NB22bJleOONNxASEoLExERcuHDBpO22R4fa0yMisgWnbg7IfT1Msm1bQsOrMOPj43Hw4EGsXbsWAwcOhJOTE5566ilUV1e32E/9ocZ6CoUCdXV1FqkRAFatWoXIyEikp6dj7969SExMRFpaGqZPn4558+YhLCwM6enpOHDgAJKSkvCXv/wFS5cutdj2GzJrT++TTz5Benq6YfqPf/wjXF1dMW7cOJSUlFisOCIia1AoFOiu7CrJoy3fKqJUKlFba9q5x5MnTyImJgbTp0/H8OHD0bt3b8P5P2vx8/NDWVmZ0Wemc3NzcfPmTQwdOtQwz9fXF8uXL8eBAwfw5JNPYtOmTYZlWq0WCxcuxPbt2/GHP/wBGzdutGrNZoXemjVrDF/+mZmZiZSUFLz11lvw8PDA8uXLLVogEZFc9e/fH//+979RXFyMGzdutLgHNmjQIGzfvh05OTk4f/48IiMjLbrH1pTQ0FAMHz4cUVFROHv2LE6dOoU5c+Zg/PjxCAoKwi+//IIlS5YgIyMDJSUlOHnyJLKzs+Hn5wcAeOGFF7B//34UFRXh7NmzOHLkiGGZtZgVemVlZRg4cCAAYOfOnZgxYwYWLFiApKQkHD9+3KIFEhHJVXx8PBwcHDB06FD07NmzxfNz69atg5ubG8aNG4fw8HCEhYUZffmHNSgUCuzatQtubm54+OGHERoaCh8fH3z++ecAAAcHB/z000+YM2cOfH19MXPmTEyePNnwdZK1tbWIjY2Fn58fJk2aBF9fX7z33nvWrVkIIdq6kqenJ/bv34/AwEAEBgYiLi4Os2fPRmFhIQICAlBVVWWNWhvR6XTQaDSoqKiAWq22yTaJqHO5c+cOioqK4O3tbfJXHlLH1NJraWoemHUhy8SJEzFv3jwEBgbi0qVLmDJlCgDg22+/Rf/+/c3pkoiIyOrMOryZkpKC4OBg/Pjjj/jqq68MN2Y8c+aM4fMeREREHY1Ze3qurq549913G83nbX+IiKgjM2tPb9++fThx4oRhOiUlBSNHjkRkZCR+/vlnixVHRERkSWaF3ooVKwx3Lb948SL+8Ic/YMqUKSgqKkJcXJxFCyQiIrIUsw5vFhUVGT54+NVXX2Hq1KlYs2YNzp49a7iohYiIqKMxa09PqVTi9u3bAIBDhw7hscceA/DrF57W7wESERF1NGbt6T344IOIi4tDSEgITp06Zfgg4qVLl9C3b1+LFkhERGQpZu3pvfvuu+jatSu2bduG1NRU/OY3vwEA7N2716TbURAREUnBrNDr168fvv76a5w/fx5z5841zP/rX/+Kd955x2LFERFR+zR1t/SdO3c22764uBgKhQI5OTkm99mZmH1rodraWuzcudNwzyR/f3888cQTcHCwzG0ziIjI8q5evQo3Nzepy5CMWaFXUFCAKVOm4D//+Q8GDx4MAEhKSoJWq0V6ejoGDBhg0SKJiMgymroLu5yYdXhz2bJlGDBgAMrKynD27FmcPXsWpaWl8Pb2xrJlyyxdIxGR7HzwwQfo06dPo9sDRURE4LnnngMAFBYWIiIiAr169YKzszNGjx6NQ4cOtdhvw8Obp06dQmBgIBwdHREUFIRz5861udbS0lJERETA2dkZarUaM2fOxLVr1wzLz58/jwkTJsDFxQVqtRqjRo3C6dOnAQAlJSUIDw+Hm5sbevToAX9/f+zZs6fNNZjKrD29o0ePIisrC+7u7oZ59913H5KTkxESEmKx4oiIrEIIoOa2NNvu1h0w4UayTz/9NJYuXYojR47gt7/9LQDgf//3f7Fv3z5DKFRVVWHKlCl48803oVKp8OmnnyI8PBz5+fno169fq9uoqqrC1KlTMXHiRHz22WcoKirC888/36anU1dXZwi8o0eP4u7du4iNjcWsWbOQkZEBAIiKikJgYCBSU1Ph4OCAnJwcwx3bY2NjUV1djWPHjqFHjx7Izc2Fs7Nzm2poC7NCT6VSobKystH8qqoqKJVKk/tJTU1Famqq4e6+/v7+ePXVVzF58mRzyiIiMk3NbWBNH2m2/dIPgLJHq83c3NwwefJkbN261RB627Ztg4eHByZMmAAACAgIQEBAgGGd1atXY8eOHdi9ezeWLFnS6ja2bt2Kuro6fPTRR3B0dIS/vz+uXLmCRYsWmfx0Dh8+jIsXL6KoqAharRYA8Omnn8Lf3x/Z2dkYPXo0SktLsWLFCgwZMgTArze8rVdaWooZM2Zg+PDhAAAfHx+Tt20Osw5vTp06FQsWLMC///1vCCEghEBWVhYWLlyIJ554wuR++vbti+TkZJw5cwanT5/Go48+ioiICHz77bfmlEVEZFeioqLw1VdfQa/XAwC2bNmC3/3ud+jS5dc/3VVVVYiPj4efnx9cXV3h7OyMvLy8Fm82e6+8vDyMGDHC6N50wcHBbaoxLy8PWq3WEHgAMHToULi6uhoudIyLi8O8efMQGhqK5ORkFBYWGtouW7YMb7zxBkJCQpCYmIgLFy60afttZdae3jvvvIPo6GgEBwcbdlFramoQERHRpstYw8PDjabffPNNpKamIisrC/7+/uaURkTUum7df93jkmrbJgoPD4cQAunp6Rg9ejSOHz+Ov/71r4bl8fHxOHjwINauXYuBAwfCyckJTz31FKqrq61RudlWrVqFyMhIpKenY+/evUhMTERaWhqmT5+OefPmISwsDOnp6Thw4ACSkpLwl7/8BUuXLrVKLWbfWmjXrl0oKCgwJLmfnx8GDhxodiG1tbX48ssvcevWrWbfaej1esM7HgD8yjMiMo9CYdIhRqk5OjriySefxJYtW1BQUIDBgwfjgQceMCw/efIkYmJiMH36dAC/7vnVny4yhZ+fH/7+97/jzp07hr29rKysNtXo5+eHsrIylJWVGfb2cnNzcfPmTcN3NAOAr68vfH19sXz5cjzzzDPYtGmToW6tVouFCxdi4cKFSEhIwMaNG6UPvdbunnDkyBHD/9etW2dyARcvXkRwcDDu3LkDZ2dn7Nixw2ig7pWUlMR79hGRrERFRWHq1Kn49ttv8fvf/95o2aBBg7B9+3aEh4dDoVDglVdeaXS1Z0siIyPx8ssvY/78+UhISEBxcTHWrl3bpvpCQ0MxfPhwREVFYf369bh79y4WL16M8ePHIygoCL/88gtWrFiBp556Ct7e3rhy5Qqys7MxY8YMAMALL7yAyZMnw9fXFz///DOOHDkCPz+/NtXQFiaHnqmXsSpMuCrpXoMHD0ZOTg4qKiqwbds2REdH4+jRo00GX0JCglH46nQ6o+PIRET25tFHH4W7uzvy8/MRGRlptGzdunV47rnnMG7cOHh4eODFF19s0xEwZ2dn/POf/8TChQsRGBiIoUOH4k9/+pMhkEyhUCiwa9cuLF26FA8//DC6dOmCSZMmYcOGDQAABwcH/PTTT5gzZw6uXbsGDw8PPPnkk4YdmNraWsTGxuLKlStQq9WYNGmS0SFcS1MIIYTVejdDaGgoBgwYgL/97W+tttXpdNBoNKioqIBarbZBdUTU2dy5cwdFRUXw9vY2umCDOp+WXktT88Csqzetqa6uzui8HRERkaWY/d2blpCQkIDJkyejX79+qKysxNatW5GRkYH9+/dLWRYREdkpSUPv+vXrmDNnDq5evQqNRoMRI0Zg//79mDhxopRlERGRnZI09D766CMpN09ERDLT4c7pERFZQwe7Zo/MYInXkKFHRHat/lujbt+W6AumyWLqX8P619Qckh7eJCKyNgcHB7i6uuL69esAgO7du7f588QkLSEEbt++jevXr8PV1bVdNytn6BGR3au/cWp98FHn5Orq2u6b4DL0iMjuKRQKeHl5wdPTEzU1NVKXQ2bo1q1bu/bw6jH0iEg2HBwcLPKHkzovXshCRESywdAjIiLZYOgREZFsMPSIiEg2GHpERCQbDD0iIpINhh4REckGQ4+IiGSDoUdERLLB0CMiItlg6BERkWww9IiISDYYekREJBsMPSIikg2GHhERyYakoZeUlITRo0fDxcUFnp6emDZtGvLz86UsiYiI7JikoXf06FHExsYiKysLBw8eRE1NDR577DHcunVLyrKIiMhOKYQQQuoi6v3444/w9PTE0aNH8fDDD7faXqfTQaPRoKKiAmq12gYVEhFRR2RqHnS1YU2tqqioAAC4u7s3uVyv10Ov1xumdTqdTeoiIiL70GEuZKmrq8MLL7yAkJAQDBs2rMk2SUlJ0Gg0hodWq7VxlURE1Jl1mMObixYtwt69e3HixAn07du3yTZN7elptVoe3iQikrlOdXhzyZIl+Prrr3Hs2LFmAw8AVCoVVCqVDSsjIiJ7ImnoCSGwdOlS7NixAxkZGfD29payHCIisnOShl5sbCy2bt2KXbt2wcXFBeXl5QAAjUYDJycnKUsjIiI7JOk5PYVC0eT8TZs2ISYmptX1+ZEFIiICOsk5vQ5yDQ0REclEh/nIAhERkbUx9IiISDYYekREJBsMPSIikg2GHhERyQZDj4iIZIOhR0REssHQIyIi2WDoERGRbDD0iIhINhh6REQkGww9IiKSDYYeERHJBkOPiIhkg6FHRESywdAjIiLZYOgREZFsMPSIiEg2GHpERCQbDD0iIpINSUPv2LFjCA8PR58+faBQKLBz504pyyEiIjsnaejdunULAQEBSElJkbIMIiKSia5Sbnzy5MmYPHmylCUQEZGMSBp6baXX66HX6w3TOp1OwmqIiKiz6VQXsiQlJUGj0RgeWq1W6pKIiKgT6VShl5CQgIqKCsOjrKxM6pKIiKgT6VSHN1UqFVQqldRlEBFRJ9Wp9vSIiIjaQ9I9vaqqKhQUFBimi4qKkJOTA3d3d/Tr10/CyoiIyB5JGnqnT5/GhAkTDNNxcXEAgOjoaGzevFmiqoiIyF5JGnqPPPIIhBBSlkBERDLCc3pERCQbDD0iIpINhh4REckGQ4+IiGSDoUdERLLB0CMiItlg6BERkWww9IiISDYYekREJBsMPSIikg2GHhERyQZDj4iIZIOhR0REssHQIyIi2WDoERGRbDD0iIhINhh6REQkGww9IiKSDYYeERHJBkOPiIhko6vUBQBASkoK/vznP6O8vBwBAQHYsGEDxowZY5NtV9yuwffXK43mKRRGU83Mv3cJoLhnYYNmRuspWujPlHUaLTO1XTPzGy5tbp1flzX9HBuPixnPsUHD5vo39Tk2LL65mizxHNHM69G4D4l+RpqpofGy5tsR2QvJQ+/zzz9HXFwc3n//fYwdOxbr169HWFgY8vPz4enpafXt51y5ieiPT1l9O0T2wpw3DY3Xa76T9r7haSnYW9isce0WeGNk+pvJe5e1/U12o622+02xaePX0nrmPMfPFwRD070brE3y0Fu3bh3mz5+PZ599FgDw/vvvIz09HR9//DFWrlxp9e07dXOAt0cPw7QQ4v//f087ce8EAHHP0nuXNWxntE4zfTfqo5m+W67JuKFxf03X0FJ/LbVDM303XM+c8Wt5W5Z9jmSe5sa90cKWe7FQNWQvam30yylp6FVXV+PMmTNISEgwzOvSpQtCQ0ORmZnZqL1er4derzdM63S6dtcwxtsdR+IfaXc/1PkZBbaVg924P9PWaS7YW3oDZc03DQ37a/nNUNPrmfwczRi/xsua7rvxsuaWtNRfg5pMfFPcIX5GzHqTbd7vQUs/I84q28SRpKF348YN1NbWolevXkbze/Xqhe+++65R+6SkJLz22mu2Ko9kprnDW020tHotRGQdnerqzYSEBFRUVBgeZWVlUpdERESdiKR7eh4eHnBwcMC1a9eM5l+7dg29e/du1F6lUkGlUtmqPCIisjOS7ukplUqMGjUKhw8fNsyrq6vD4cOHERwcLGFlRERkjyS/ejMuLg7R0dEICgrCmDFjsH79ety6dctwNScREZGlSB56s2bNwo8//ohXX30V5eXlGDlyJPbt29fo4hYiIqL2UoiG1+52IjqdDhqNBhUVFVCr1VKXQ0REEjE1DzrV1ZtERETtIfnhzfao30m1xIfUiYio86rPgdYOXnbq0Kus/PWLorVarcSVEBFRR1BZWQmNRtPs8k59Tq+urg4//PADXFxc2vWt8DqdDlqtFmVlZTw3eA+OS/M4Nk3juDSPY9M0S42LEAKVlZXo06cPunRp/sxdp97T69KlC/r27Wux/tRqNX8Ym8BxaR7Hpmkcl+ZxbJpmiXFpaQ+vHi9kISIi2WDoERGRbDD08Ot3eiYmJvJ7PRvguDSPY9M0jkvzODZNs/W4dOoLWYiIiNqCe3pERCQbDD0iIpINhh4REckGQ4+IiGRDNqGXkpKC/v37w9HREWPHjsWpU6dabP/ll19iyJAhcHR0xPDhw7Fnzx4bVWpbbRmXjRs34qGHHoKbmxvc3NwQGhra6jh2Zm39mamXlpYGhUKBadOmWbdAibR1XG7evInY2Fh4eXlBpVLB19fXLn+f2jou69evx+DBg+Hk5AStVovly5fjzp07NqrWdo4dO4bw8HD06dMHCoUCO3fubHWdjIwMPPDAA1CpVBg4cCA2b95suYKEDKSlpQmlUik+/vhj8e2334r58+cLV1dXce3atSbbnzx5Ujg4OIi33npL5Obmiv/+7/8W3bp1ExcvXrRx5dbV1nGJjIwUKSkp4ty5cyIvL0/ExMQIjUYjrly5YuPKra+tY1OvqKhI/OY3vxEPPfSQiIiIsE2xNtTWcdHr9SIoKEhMmTJFnDhxQhQVFYmMjAyRk5Nj48qtq63jsmXLFqFSqcSWLVtEUVGR2L9/v/Dy8hLLly+3ceXWt2fPHvHyyy+L7du3CwBix44dLba/fPmy6N69u4iLixO5ubliw4YNwsHBQezbt88i9cgi9MaMGSNiY2MN07W1taJPnz4iKSmpyfYzZ84Ujz/+uNG8sWPHiv/6r/+yap221tZxaeju3bvCxcVFfPLJJ9YqUTLmjM3du3fFuHHjxIcffiiio6PtMvTaOi6pqanCx8dHVFdX26pESbR1XGJjY8Wjjz5qNC8uLk6EhIRYtU6pmRJ6f/zjH4W/v7/RvFmzZomwsDCL1GD3hzerq6tx5swZhIaGGuZ16dIFoaGhyMzMbHKdzMxMo/YAEBYW1mz7zsiccWno9u3bqKmpgbu7u7XKlIS5Y/P666/D09MTc+fOtUWZNmfOuOzevRvBwcGIjY1Fr169MGzYMKxZswa1tbW2KtvqzBmXcePG4cyZM4ZDoJcvX8aePXswZcoUm9TckVn772+n/sJpU9y4cQO1tbXo1auX0fxevXrhu+++a3Kd8vLyJtuXl5dbrU5bM2dcGnrxxRfRp0+fRj+gnZ05Y3PixAl89NFHyMnJsUGF0jBnXC5fvox//etfiIqKwp49e1BQUIDFixejpqYGiYmJtijb6swZl8jISNy4cQMPPvgghBC4e/cuFi5ciJdeeskWJXdozf391el0+OWXX+Dk5NSu/u1+T4+sIzk5GWlpadixYwccHR2lLkdSlZWVmD17NjZu3AgPDw+py+lQ6urq4OnpiQ8++ACjRo3CrFmz8PLLL+P999+XujRJZWRkYM2aNXjvvfdw9uxZbN++Henp6Vi9erXUpdk9u9/T8/DwgIODA65du2Y0/9q1a+jdu3eT6/Tu3btN7Tsjc8al3tq1a5GcnIxDhw5hxIgR1ixTEm0dm8LCQhQXFyM8PNwwr66uDgDQtWtX5OfnY8CAAdYt2gbM+Znx8vJCt27d4ODgYJjn5+eH8vJyVFdXQ6lUWrVmWzBnXF555RXMnj0b8+bNAwAMHz4ct27dwoIFC/Dyyy+3eD84e9fc31+1Wt3uvTxABnt6SqUSo0aNwuHDhw3z6urqcPjwYQQHBze5TnBwsFF7ADh48GCz7Tsjc8YFAN566y2sXr0a+/btQ1BQkC1Ktbm2js2QIUNw8eJF5OTkGB5PPPEEJkyYgJycHGi1WluWbzXm/MyEhISgoKDA8CYAAC5dugQvLy+7CDzAvHG5fft2o2Crf2MgZP51yFb/+2uRy2E6uLS0NKFSqcTmzZtFbm6uWLBggXB1dRXl5eVCCCFmz54tVq5caWh/8uRJ0bVrV7F27VqRl5cnEhMT7fYjC20Zl+TkZKFUKsW2bdvE1atXDY/KykqpnoLVtHVsGrLXqzfbOi6lpaXCxcVFLFmyROTn54uvv/5aeHp6ijfeeEOqp2AVbR2XxMRE4eLiIv7xj3+Iy5cviwMHDogBAwaImTNnSvUUrKayslKcO3dOnDt3TgAQ69atE+fOnRMlJSVCCCFWrlwpZs+ebWhf/5GFFStWiLy8PJGSksKPLJhjw4YNol+/fkKpVIoxY8aIrKwsw7Lx48eL6Ohoo/ZffPGF8PX1FUqlUvj7+4v09HQbV2wbbRmX+++/XwBo9EhMTLR94TbQ1p+Ze9lr6AnR9nH55ptvxNixY4VKpRI+Pj7izTffFHfv3rVx1dbXlnGpqakRq1atEgMGDBCOjo5Cq9WKxYsXi59//tn2hVvZkSNHmvy7UT8e0dHRYvz48Y3WGTlypFAqlcLHx0ds2rTJYvXw1kJERCQbdn9Oj4iIqB5Dj4iIZIOhR0REssHQIyIi2WDoERGRbDD0iIhINhh6REQkGww9IiKSDYYeUSdSXFwMhUJh17cwIrImhh6RnYuJicG0adOkLoOoQ2DoERGRbDD0iKykf//+WL9+vdG8kSNHYtWqVQAAhUKB1NRUTJ48GU5OTvDx8cG2bduM2p86dQqBgYFwdHREUFAQzp07Z7S8trYWc+fOhbe3N5ycnDB48GC8/fbbhuWrVq3CJ598gl27dkGhUEChUCAjIwMAUFZWhpkzZ8LV1RXu7u6IiIhAcXGxYd2MjAyMGTMGPXr0gKurK0JCQlBSUmKx8SGSAkOPSEKvvPIKZsyYgfPnzyMqKgq/+93vkJeXBwCoqqrC1KlTMXToUJw5cwarVq1CfHy80fp1dXXo27cvvvzyS+Tm5uLVV1/FSy+9hC+++AIAEB8fj5kzZ2LSpEm4evUqrl69inHjxqGmpgZhYWFwcXHB8ePHcfLkSTg7O2PSpEmorq7G3bt3MW3aNIwfPx4XLlxAZmYmFixYAIVCYfMxIrIku79zOlFH9vTTTxvunr169WocPHgQGzZswHvvvYetW7eirq4OH330ERwdHeHv748rV65g0aJFhvW7deuG1157zTDt7e2NzMxMfPHFF5g5cyacnZ3h5OQEvV5vdBfvzz77DHV1dfjwww8NQbZp0ya4uroiIyMDQUFBqKiowNSpUw13fffz87PFkBBZFff0iCTU8G7QwcHBhj29vLw8jBgxAo6Ojs22B4CUlBSMGjUKPXv2hLOzMz744AOUlpa2uN3z58+joKAALi4ucHZ2hrOzM9zd3XHnzh0UFhbC3d0dMTExCAsLQ3h4ON5++21cvXrVAs+YSFoMPSIr6dKlCxrerrKmpsai20hLS0N8fDzmzp2LAwcOICcnB88++yyqq6tbXK+qqgqjRo1CTk6O0ePSpUuIjIwE8OueX2ZmJsaNG4fPP/8cvr6+yMrKsmj9RLbG0COykp49exrtHel0OhQVFRm1aRgiWVlZhsOIfn5+uHDhAu7cudNs+5MnT2LcuHFYvHgxAgMDMXDgQBQWFhq1USqVqK2tNZr3wAMP4Pvvv4enpycGDhxo9NBoNIZ2gYGBSEhIwDfffINhw4Zh69atZowEUcfB0COykkcffRR///vfcfz4cVy8eBHR0dFwcHAwavPll1/i448/xqVLl5CYmIhTp05hyZIlAIDIyEgoFArMnz8fubm52LNnD9auXWu0/qBBg3D69Gns378fly5dwiuvvILs7GyjNv3798eFCxeQn5+PGzduoKamBlFRUfDw8EBERASOHz+OoqIiZGRkYNmyZbhy5QqKioqQkJCAzMxMlJSU4MCBA/j+++95Xo86P0FEVlFRUSFmzZol1Gq10Gq1YvPmzSIgIEAkJiYKIYQAIFJSUsTEiROFSqUS/fv3F59//rlRH5mZmSIgIEAolUoxcuRI8dVXXwkA4ty5c0IIIe7cuSNiYmKERqMRrq6uYtGiRWLlypUiICDA0Mf169fFxIkThbOzswAgjhw5IoQQ4urVq2LOnDnCw8NDqFQq4ePjI+bPny8qKipEeXm5mDZtmvDy8hJKpVLcf//94tVXXxW1tbU2GDki61EI0eCkAxHZhEKhwI4dO/htKUQ2xMObREQkGww9IiKSDX44nUgiPLNAZHvc0yMiItlg6BERkWww9IiISDYYekREJBsMPSIikg2GHhERyQZDj4iIZIOhR0REsvF/cOaFi/FDdAAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, loader_length, vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/target_sents_adv.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/target_synts_adv.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/outputs_adv.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # Put input into device\n",
    "                sents_ = sents_.to(device)\n",
    "                synts_ = synts_.to(device)\n",
    "                trgs_ = trgs_.to(device)\n",
    "                adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    # fp1.write(targ+'\\n')\n",
    "                    # fp2.write(synt_+'\\n')\n",
    "                    # fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')\n",
    "                    \n",
    "                    convert_sent = reverse_bpe(sent2str(sent.tolist(), vocab_transform).split()) + '\\n'\n",
    "                    convert_synt = synt2str(synt_[1:].tolist(), vocab_transform).replace(\"<pad>\", \"\") + '\\n' \n",
    "                    convert_idx = synt2str(idx, vocab_transform) +'\\n'\n",
    "                    \n",
    "                    fp1.write(convert_sent)\n",
    "                    fp2.write(convert_synt)\n",
    "                    fp3.write(convert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:04<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/adversary_nmt_modify.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "generate(model, valid_dataloader, val_loader_length, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 300\n"
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents_adv.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs_adv.txt') as fp: \n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '. on ,\\n',\n",
       " ', , it , , .\\n',\n",
       " '\\n',\n",
       " \"'m , . the , it a\\n\",\n",
       " ',\\n',\n",
       " ',\\n',\n",
       " ', the . it it the\\n',\n",
       " '.\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"down there , i 'm only heinz , the cook 's mate .\\n\",\n",
       " 'you know what the best thing is about a pimp ?\\n',\n",
       " \"you 're ziemowit , piast 's son .\\n\",\n",
       " 'i shall review your case .\\n',\n",
       " \"and if you do that he 'll shoot you .\\n\",\n",
       " \"it had been her teeth that tore into ms. harrison 's neck .\\n\",\n",
       " 'here they are . a girl is always prepared .\\n',\n",
       " \"i 'm going to do what the fuck i want .\\n\",\n",
       " \"i 'll reach you later .\\n\",\n",
       " 'let me guess . sumei ?\\n']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 2.003212084672482\n"
     ]
    }
   ],
   "source": [
    "scores = [cal_bleu(pred, targ, 1) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
