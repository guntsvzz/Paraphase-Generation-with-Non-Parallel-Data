{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "with open(\"data/aLL_bow_100k.pkl\", \"rb\") as file:\n",
    "    nmt_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.float32)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_dataset)\n",
    "\n",
    "train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "train_set = nmt_dataset[:train_range]\n",
    "val_set   = nmt_dataset[train_range:]\n",
    "# test_set = train_data[90:]\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_set, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18])\n",
      "torch.Size([8, 87])\n",
      "torch.Size([8, 20])\n",
      "torch.Size([8, 74])\n"
     ]
    }
   ],
   "source": [
    "for idx, (sen, syn, trg, adv) in enumerate(train_dataloader):\n",
    "    print(sen.shape)\n",
    "    print(syn.shape)\n",
    "    print(trg.shape)\n",
    "    print(adv.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        # sent_embeddings : batch_size, seq_len, hid_dim\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1) # batch_size, hid_dim\n",
    "        x = self.adv(x) # batch_size, hid_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, input_token_ids):\n",
    "        mask = input_token_ids != pad_idx\n",
    "        mean_mask = mask.float()/mask.float().sum(1, keepdim=True)\n",
    "        x = (x * mean_mask.unsqueeze(2)).sum(1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "\n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "        self.pooling = MeanPooling()\n",
    "\n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def load_embedding(self, embedding): \n",
    "            self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "            self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        #synts : batch_size, synt_len, emb_dim\n",
    "        #trgs  : batch_size, trg_len, emb_dim \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1)   - 2  # without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents_ = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents_).transpose(0, 1) * self.scale # sent_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale # synt_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings) # synt_len, batch_size, emb_dim\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0) # synt_len + seq_len, batch size, emb_size\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "\n",
    "        # target => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings) # trg_len, batch_size, emb_dim\n",
    "\n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "\n",
    "        # forward\n",
    "        outputs = self.transformer(encoder_embeddings, decoder_embeddings, src_mask=src_mask, tgt_mask=trg_mask) # trg_len, batch_size, emb_dim\n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1) # batch_size, trg_len, emb_dim\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim)) # batch_size*trg_len, input_dim\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim) # batch_size, trg_len, input_dim\n",
    "\n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "\n",
    "        adv_outputs = self.adversary(sent_embeds).transpose(0, 1) # batch_size, 74   \n",
    "\n",
    "        return outputs, adv_outputs\n",
    "        \n",
    "    def forward_token(self, sents):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(sents.shape)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        sent_embeddings = self.embedding_encoder(sents) # batch_size, sent_len, emb_dim\n",
    "        sent_embeddings = self.positional_encoder(sent_embeddings) # batch_size, sent_len, emb_dim\n",
    "\n",
    "        sent_embeddings = self.norm(sent_embeddings)\n",
    "        sent_embeddings = F.dropout(sent_embeddings, p=self.dropout)\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        for encoder_layer in self.transformer.encoder.layers:\n",
    "            sent_embeddings = encoder_layer(sent_embeddings)\n",
    "\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        return sent_embeddings \n",
    "    \n",
    "    def forward_adv(self, sents):\n",
    "\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "        \n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "        adv_outputs = self.adversary(sent_embeds)\n",
    "        return adv_outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len, emb_dim \n",
    "        #synts  : batch_size, seq_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings)\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(encoder_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            # if i % 5 == 0:\n",
    "            #     print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(decoder_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            decoder_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = 'data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "49621464\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "    #     print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40113/1520844641.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        \n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "        \n",
    "        #optimize adversarial\n",
    "        outputs = model.forward_adv(sents_) #batch_size, 74\n",
    "\n",
    "        loss = adv_criterion(outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        adv_total_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        adv_optimizer.step()\n",
    "        adv_optimizer.zero_grad()\n",
    "\n",
    "        #optimize model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "\n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #seq_len, batch_size, 74\n",
    "\n",
    "        if epoch > 1:\n",
    "            loss -= 0.1 * adv_criterion(adv_outputs, adv_targs)\n",
    "\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        para_optimizer.step()\n",
    "        para_optimizer.zero_grad()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            \n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "            \n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-5 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8750it [08:36, 16.95it/s]\n",
      "100%|██████████| 3750/3750 [00:54<00:00, 68.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 31s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt_modify.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(float(valid_loss))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, loader_length, vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/target_sents_adv.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/target_synts_adv.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/outputs_adv.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # Put input into device\n",
    "                sents_ = sents_.to(device)\n",
    "                synts_ = synts_.to(device)\n",
    "                trgs_ = trgs_.to(device)\n",
    "                adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    # fp1.write(targ+'\\n')\n",
    "                    # fp2.write(synt_+'\\n')\n",
    "                    # fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')\n",
    "                    \n",
    "                    convert_sent = reverse_bpe(sent2str(sent.tolist(), vocab_transform).split()) + '\\n'\n",
    "                    convert_synt = synt2str(synt_[1:].tolist(), vocab_transform).replace(\"<pad>\", \"\") + '\\n' \n",
    "                    convert_idx = synt2str(idx, vocab_transform) +'\\n'\n",
    "                    \n",
    "                    fp1.write(convert_sent)\n",
    "                    fp2.write(convert_synt)\n",
    "                    fp3.write(convert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [06:40<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/adversary_nmt_modify.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "generate(model, valid_dataloader, val_loader_length, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 30000\n"
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents_adv.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs_adv.txt') as fp: \n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i have .\\n',\n",
       " 'i the , the , the is the the the , the to the of the .\\n',\n",
       " 'i .\\n',\n",
       " 'he the .\\n',\n",
       " ', i , , .\\n',\n",
       " 'no , .\\n',\n",
       " 'i -- of a , .\\n',\n",
       " 'i , .\\n',\n",
       " 'i .\\n',\n",
       " 'i you .\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they think that it is the work of a madman .\\n',\n",
       " \"i 'll stay for a minute , but i can promise you that no fun will be had by me at all .\\n\",\n",
       " \"just do n't hit me any more in my nuts .\\n\",\n",
       " 'this is why , in my understanding , the issue did not need to be tackled in br<unk> stle .\\n',\n",
       " \"taylor : i 'm sure it wo n't be long now .\\n\",\n",
       " \"i mean , we 're not spying on casey . we 're watching his back .\\n\",\n",
       " \"hub and garth did n't rob any banks .\\n\",\n",
       " \"he 'll try and hunt us down .\\n\",\n",
       " 'you must have a lot of courage , vicomte ... ... to come here after you sent that ... ... lunatic harp teacher to kill me .\\n',\n",
       " 'now what do you think ? what do you think about this ?\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 2.8697976556093194e-155\n"
     ]
    }
   ],
   "source": [
    "scores = [cal_bleu(pred, targ, 0) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
