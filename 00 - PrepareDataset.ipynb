{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ParaNMT\n",
    "with open('./Datasets/para-nmt-5m-processed.txt', encoding=\"utf8\") as file:\n",
    "    nmt = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"so , unless that 's gon na be feasible , then ...\",\n",
       " 'of course you did .',\n",
       " \"by now , singh 's probably been arrested .\",\n",
       " 'not our shit . i swear .',\n",
       " 'â€œ why not ?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_sentences = list()\n",
    "for idx, sentence in enumerate(nmt):\n",
    "    nmt_sentences.append(nmt[idx].split(\"\\t\")[0]) #non-parallel data\n",
    "nmt_sentences[:5] #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_sentences) #5370128 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1\n",
       "0  How does the Surface Pro himself 4 compare wit...\n",
       "1  Should I have a hair transplant at age 24? How...\n",
       "2  What but is the best way to send money from Ch...\n",
       "3                        Which food not emulsifiers?\n",
       "4                   How \"aberystwyth\" start reading?"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Quora\n",
    "import pandas as pd\n",
    "qq = pd.read_csv('./Datasets/quora_question.csv')\n",
    "qq.drop(columns=['test_id','question2'], inplace=True) #non-parallel data\n",
    "qq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345796, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.shape #2345796 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does the Surface Pro himself 4 compare with iPad Pro?',\n",
       " 'Should I have a hair transplant at age 24? How much would it cost?',\n",
       " 'What but is the best way to send money from China to the US?',\n",
       " 'Which food not emulsifiers?',\n",
       " 'How \"aberystwyth\" start reading?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq_sentences = qq['question1'].values.tolist()\n",
    "qq_sentences[:5] #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# benepar.download('benepar_en3')\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\"\n",
    "\n",
    "def getleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "    \n",
    "    leaves = []\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                leaves.append(arr[n])\n",
    "\n",
    "    return leaves\n",
    "\n",
    "def deleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                arr[n + 1] = \"\"\n",
    "\n",
    "    nonleaves = \" \".join(arr)\n",
    "    return nonleaves.split()\n",
    "\n",
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "#Setence to syntax\n",
    "def constituency_parser(text):\n",
    "    doc = nlp(text)\n",
    "    sent = list(doc.sents)[0]\n",
    "    return \"(ROOT \"+sent._.parse_string+\")\"\n",
    "\n",
    "#syntax to syntatic tokenzier\n",
    "from nltk import ParentedTree\n",
    "def parser_tokenizer(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    # synt_ = [f'<{w}>' for w in synt_]\n",
    "    synt_ = [dictionary.word2idx[f\"<{w}>\"] for w in synt_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    return synt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer BPE\n",
    "from subwordnmt.apply_bpe import BPE, read_vocabulary\n",
    "import codecs\n",
    "\n",
    "# load bpe codes\n",
    "bpe_codes = codecs.open('./data/bpe.codes', encoding='utf-8')\n",
    "bpe_vocab = codecs.open('./data/vocab.txt', encoding='utf-8')\n",
    "bpe_vocab = read_vocabulary(bpe_vocab, 50)\n",
    "bpe = BPE(bpe_codes, '@@', bpe_vocab, None)\n",
    "\n",
    "def bpe_tokenizer(sent_, target = False):\n",
    " # bpe segment and convert to tensor\n",
    "    sent_ = bpe.segment(sent_).split()\n",
    "    sent_ = [dictionary.word2idx[w] if w in dictionary.word2idx else dictionary.word2idx[\"<unk>\"] for w in sent_]\n",
    "    if target:\n",
    "        sent_ = [dictionary.word2idx[\"<sos>\"]] + sent_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return sent_\n",
    "\n",
    "# def pos_tag(sent_):\n",
    "#     return [token.ent_type_ if token.ent_type_ else \"\" for token in nlp(sent_)]\n",
    "\n",
    "# def dependency_parser(sent_):\n",
    "#     return [token.dep_ for token in nlp(sent_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_paraphrase_dataset(sentences):\n",
    "    sents, synts, trgs = list(), list(), list()\t\n",
    "    lists_ = list()\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "\n",
    "        sent_ = bpe_tokenizer(sentences[idx])\n",
    "        synt_ = parser_tokenizer(constituency_parser(sentences[idx]))\n",
    "        trg_  = bpe_tokenizer(sentences[idx], target = True)\n",
    "\n",
    "        # sents.append(sent_) #\n",
    "        # synts.append(synt_)\n",
    "        # trgs.append(trg_)\n",
    "        \n",
    "        lists_.append((sent_, synt_, trg_))\n",
    "    # return {'sentences':sents, 'syntatic':synts, 'targets': trgs}\n",
    "    return lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(6969)\n",
    "# random.shuffle(nmt_sentences)\n",
    "\n",
    "# # nmt_dataset = prepare_paraphrase_dataset(nmt_sentences[:100000]) #100000 sentences\n",
    "# nmt_dataset = prepare_paraphrase_dataset(nmt_sentences[600000:1000000]) #100000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(SBARQ (WHADJP (WRB How) (JJ tall)) (SQ (VBZ is) (NP (DT a) (JJ standard) (JJS worst) (NN frame))) (. ?))'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(qq_sentences[15914])\n",
    "sent = list(doc.sents)[0]\n",
    "sent._.parse_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]c:\\Python3.10.4\\lib\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49275/100000 [1:13:42<1:15:52, 11.14it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\retokenization.py:74\u001b[0m, in \u001b[0;36mretokenize\u001b[1;34m(tokenizer, words, space_after, return_attention_mask, return_offsets_mapping, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     token_idx, (token_start, token_end) \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(offset_mapping_iter)\n\u001b[0;32m     75\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\00 - PrepareDataset.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m6969\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m random\u001b[39m.\u001b[39mshuffle(qq_sentences)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m qq_dataset \u001b[39m=\u001b[39m prepare_paraphrase_dataset(qq_sentences[\u001b[39m200000\u001b[39;49m:\u001b[39m300000\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\00 - PrepareDataset.ipynb Cell 14\u001b[0m in \u001b[0;36mprepare_paraphrase_dataset\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sentences))):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     sent_ \u001b[39m=\u001b[39m bpe_tokenizer(sentences[idx])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     synt_ \u001b[39m=\u001b[39m parser_tokenizer(constituency_parser(sentences[idx]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     trg_  \u001b[39m=\u001b[39m bpe_tokenizer(sentences[idx], target \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# sents.append(sent_) #\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# synts.append(synt_)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# trgs.append(trg_)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\00 - PrepareDataset.ipynb Cell 14\u001b[0m in \u001b[0;36mconstituency_parser\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstituency_parser\u001b[39m(text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     sent \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(doc\u001b[39m.\u001b[39msents)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/00%20-%20PrepareDataset.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m(ROOT \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39msent\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mparse_string\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\spacy\\language.py:1031\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1031\u001b[0m     error_handler(name, proc, [doc], e)\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(doc, Doc):\n\u001b[0;32m   1033\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE005\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, returned_type\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc)))\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\spacy\\util.py:1670\u001b[0m, in \u001b[0;36mraise_error\u001b[1;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[0;32m   1669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[1;32m-> 1670\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\spacy\\language.py:1026\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1025\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1026\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1028\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\integrations\\spacy_plugin.py:151\u001b[0m, in \u001b[0;36mBeneparComponent.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    147\u001b[0m constituent_data \u001b[39m=\u001b[39m PartialConstituentData()\n\u001b[0;32m    148\u001b[0m wrapped_sents \u001b[39m=\u001b[39m [SentenceWrapper(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n\u001b[0;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m sent, parse \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    150\u001b[0m     doc\u001b[39m.\u001b[39msents,\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parser\u001b[39m.\u001b[39;49mparse(\n\u001b[0;32m    152\u001b[0m         wrapped_sents,\n\u001b[0;32m    153\u001b[0m         return_compressed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    154\u001b[0m         subbatch_max_tokens\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubbatch_max_tokens,\n\u001b[0;32m    155\u001b[0m     ),\n\u001b[0;32m    156\u001b[0m ):\n\u001b[0;32m    157\u001b[0m     constituent_data\u001b[39m.\u001b[39mstarts\u001b[39m.\u001b[39mappend(parse\u001b[39m.\u001b[39mstarts \u001b[39m+\u001b[39m sent\u001b[39m.\u001b[39mstart)\n\u001b[0;32m    158\u001b[0m     constituent_data\u001b[39m.\u001b[39mends\u001b[39m.\u001b[39mappend(parse\u001b[39m.\u001b[39mends \u001b[39m+\u001b[39m sent\u001b[39m.\u001b[39mstart)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\parse_chart.py:414\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[1;34m(self, examples, return_compressed, return_scores, subbatch_max_tokens)\u001b[0m\n\u001b[0;32m    412\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[0;32m    413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[1;32m--> 414\u001b[0m encoded \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(example) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples]\n\u001b[0;32m    415\u001b[0m \u001b[39mif\u001b[39;00m subbatch_max_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     res \u001b[39m=\u001b[39m subbatching\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_encoded,\n\u001b[0;32m    418\u001b[0m         examples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         return_scores\u001b[39m=\u001b[39mreturn_scores,\n\u001b[0;32m    424\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\parse_chart.py:414\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    412\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[0;32m    413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[1;32m--> 414\u001b[0m encoded \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(example) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples]\n\u001b[0;32m    415\u001b[0m \u001b[39mif\u001b[39;00m subbatch_max_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     res \u001b[39m=\u001b[39m subbatching\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_encoded,\n\u001b[0;32m    418\u001b[0m         examples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         return_scores\u001b[39m=\u001b[39mreturn_scores,\n\u001b[0;32m    424\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\parse_chart.py:193\u001b[0m, in \u001b[0;36mChartParser.encode\u001b[1;34m(self, example)\u001b[0m\n\u001b[0;32m    191\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretokenizer(example\u001b[39m.\u001b[39mwords, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretokenizer(example\u001b[39m.\u001b[39;49mwords, example\u001b[39m.\u001b[39;49mspace_after)\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m example\u001b[39m.\u001b[39mtree \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     encoded[\u001b[39m\"\u001b[39m\u001b[39mspan_labels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mchart_from_tree(example\u001b[39m.\u001b[39mtree)\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\retokenization.py:150\u001b[0m, in \u001b[0;36mRetokenizer.__call__\u001b[1;34m(self, words, space_after, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, words, space_after, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 150\u001b[0m     example \u001b[39m=\u001b[39m retokenize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, words, space_after, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_t5:\n\u001b[0;32m    152\u001b[0m         \u001b[39m# decoder_input_ids (which are shifted wrt input_ids) will be created after\u001b[39;00m\n\u001b[0;32m    153\u001b[0m         \u001b[39m# padding, but we adjust words_from_tokens now, in anticipation.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(example[\u001b[39m\"\u001b[39m\u001b[39mwords_from_tokens\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\benepar\\retokenization.py:76\u001b[0m, in \u001b[0;36mretokenize\u001b[1;34m(tokenizer, words, space_after, return_attention_mask, return_offsets_mapping, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m             token_idx, (token_start, token_end) \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(offset_mapping_iter)\n\u001b[0;32m     75\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m             \u001b[39massert\u001b[39;00m word_idx \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(words) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     77\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(6969)\n",
    "random.shuffle(qq_sentences)\n",
    "\n",
    "qq_dataset = prepare_paraphrase_dataset(qq_sentences[200000:300000]) #100000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/qq_dataset_200000_300000.pkl', 'wb') as f:\n",
    "    pickle.dump(qq_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./data/nmt_dataset_100000.pkl', 'wb') as f:\n",
    "#     pickle.dump(nmt_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('./data/qq_dataset_100000.pkl', 'wb') as f:\n",
    "#     pickle.dump(qq_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "def syntax_tensor(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    synt_ = [dictionary.word2idx[f\"<{w}>\"] for w in synt_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    synt_ = [dictionary.word2idx[\"<sos>\"]] + synt_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return synt_\n",
    "\n",
    "def tag_sequence(sent_):\n",
    "    sent_ = ParentedTree.fromstring(sent_)\n",
    "    sent_ = getleaf(sent_)\n",
    "    sent_ = [dictionary.word2idx[f\"<{w}>\"] for w in sent_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    return sent_\n",
    "\n",
    "#parse syntax and get template\n",
    "from nltk import ParentedTree\n",
    "\n",
    "def tree2tmpl(tree, level, mlevel):\n",
    "    if level == mlevel:\n",
    "        for idx, n in enumerate(tree):\n",
    "            if isinstance(n, ParentedTree):\n",
    "                tree[idx] = \"(\" + n.label() + \")\"\n",
    "    else:\n",
    "        for n in tree:\n",
    "            tree2tmpl(n, level + 1, mlevel)\n",
    "\n",
    "def template(tmpl_):\n",
    "    tmpl_ = ParentedTree.fromstring(tmpl_)\n",
    "    tree2tmpl(tmpl_, 1, 2)\n",
    "    tmpl_ = str(tmpl_).replace(\")\", \" )\").replace(\"(\", \"( \").split(\" \")\n",
    "    tmpl_ = [dictionary.word2idx[f\"<{w}>\"] for w in tmpl_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    tmpl_ = [dictionary.word2idx[\"<sos>\"]] + tmpl_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return tmpl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_parse_dataset(sentences):\n",
    "    sents, tmpls, synts = list(), list(), list()\t\n",
    "    lists_ = list()\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "        parser = constituency_parser(sentences[idx])\n",
    "        sent_ = tag_sequence(parser)\n",
    "        tmpl_ = template(parser)\n",
    "        synt_ = syntax_tensor(parser)\n",
    "\n",
    "        # sents.append(sent_)  #sents\n",
    "        # tmpls.append(tmpl_)  #synts\n",
    "        # synts.append(synt_)  #targs\n",
    "        lists_.append((sent_, tmpl_, synt_))\n",
    "    # return {'sentences':sents, 'templates':tmpls, 'syntatic': synts}\n",
    "    return lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–Ž        | 13617/100000 [19:06<1:51:42, 12.89it/s]"
     ]
    }
   ],
   "source": [
    "nmt_parse = prepare_parse_dataset(nmt_sentences[:100000]) #10000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:14<00:00, 13.41it/s]\n"
     ]
    }
   ],
   "source": [
    "qq_parse = prepare_parse_dataset(qq_sentences[:1000]) #10000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Datasets/nmt_parse_100000.pkl', 'wb') as f:\n",
    "    pickle.dump(nmt_parse, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('./Datasets/qq_parse.pkl', 'wb') as f:\n",
    "#     pickle.dump(qq_parse, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
