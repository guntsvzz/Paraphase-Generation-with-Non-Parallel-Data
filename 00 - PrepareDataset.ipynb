{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ParaNMT\n",
    "with open('./Datasets/para-nmt-5m-processed.txt', encoding=\"utf8\") as file:\n",
    "    nmt = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"so , unless that 's gon na be feasible , then ...\",\n",
       " 'of course you did .',\n",
       " \"by now , singh 's probably been arrested .\",\n",
       " 'not our shit . i swear .',\n",
       " '“ why not ?']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_sentences = list()\n",
    "for idx, sentence in enumerate(nmt):\n",
    "    nmt_sentences.append(nmt[idx].split(\"\\t\")[0]) #non-parallel data\n",
    "nmt_sentences[:5] #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370128"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_sentences) #5370128 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1\n",
       "0  How does the Surface Pro himself 4 compare wit...\n",
       "1  Should I have a hair transplant at age 24? How...\n",
       "2  What but is the best way to send money from Ch...\n",
       "3                        Which food not emulsifiers?\n",
       "4                   How \"aberystwyth\" start reading?"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Quora\n",
    "import pandas as pd\n",
    "qq = pd.read_csv('./Datasets/quora_question.csv')\n",
    "qq.drop(columns=['test_id','question2'], inplace=True) #non-parallel data\n",
    "qq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345796, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.shape #2345796 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does the Surface Pro himself 4 compare with iPad Pro?',\n",
       " 'Should I have a hair transplant at age 24? How much would it cost?',\n",
       " 'What but is the best way to send money from China to the US?',\n",
       " 'Which food not emulsifiers?',\n",
       " 'How \"aberystwyth\" start reading?']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq_sentences = qq['question1'].values.tolist()\n",
    "qq_sentences[:5] #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# benepar.download('benepar_en3')\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\"\n",
    "\n",
    "def getleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "    \n",
    "    leaves = []\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                leaves.append(arr[n])\n",
    "\n",
    "    return leaves\n",
    "\n",
    "def deleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                arr[n + 1] = \"\"\n",
    "\n",
    "    nonleaves = \" \".join(arr)\n",
    "    return nonleaves.split()\n",
    "\n",
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "#Setence to syntax\n",
    "def constituency_parser(text):\n",
    "    doc = nlp(text)\n",
    "    sent = list(doc.sents)[0]\n",
    "    return \"(ROOT \"+sent._.parse_string+\")\"\n",
    "\n",
    "#syntax to syntatic tokenzier\n",
    "from nltk import ParentedTree\n",
    "def parser_tokenizer(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    # synt_ = [f'<{w}>' for w in synt_]\n",
    "    synt_ = [dictionary.word2idx[f\"<{w}>\"] for w in synt_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    return synt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer BPE\n",
    "from subwordnmt.apply_bpe import BPE, read_vocabulary\n",
    "import codecs\n",
    "\n",
    "# load bpe codes\n",
    "bpe_codes = codecs.open('./data/bpe.codes', encoding='utf-8')\n",
    "bpe_vocab = codecs.open('./data/vocab.txt', encoding='utf-8')\n",
    "bpe_vocab = read_vocabulary(bpe_vocab, 50)\n",
    "bpe = BPE(bpe_codes, '@@', bpe_vocab, None)\n",
    "\n",
    "def bpe_tokenizer(sent_, target = False):\n",
    " # bpe segment and convert to tensor\n",
    "    sent_ = bpe.segment(sent_).split()\n",
    "    sent_ = [dictionary.word2idx[w] if w in dictionary.word2idx else dictionary.word2idx[\"<unk>\"] for w in sent_]\n",
    "    if target:\n",
    "        sent_ = [dictionary.word2idx[\"<sos>\"]] + sent_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return sent_\n",
    "\n",
    "# def pos_tag(sent_):\n",
    "#     return [token.ent_type_ if token.ent_type_ else \"\" for token in nlp(sent_)]\n",
    "\n",
    "# def dependency_parser(sent_):\n",
    "#     return [token.dep_ for token in nlp(sent_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_paraphrase_dataset(sentences):\n",
    "    sents, synts, trgs = list(), list(), list()\t\n",
    "    lists_ = list()\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "\n",
    "        sent_ = bpe_tokenizer(sentences[idx])\n",
    "        synt_ = parser_tokenizer(constituency_parser(sentences[idx]))\n",
    "        trg_  = bpe_tokenizer(sentences[idx], target = True)\n",
    "\n",
    "        # sents.append(sent_) #\n",
    "        # synts.append(synt_)\n",
    "        # trgs.append(trg_)\n",
    "        \n",
    "        lists_.append((sent_, synt_, trg_))\n",
    "    # return {'sentences':sents, 'syntatic':synts, 'targets': trgs}\n",
    "    return lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:15<00:00, 13.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_sentences)\n",
    "\n",
    "nmt_dataset = prepare_paraphrase_dataset(nmt_sentences[:1000]) #1000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187, 110, 131, 252, 1726, 132, 2060, 322, 188, 248, 124, 166, 127, 229, 210, 1775, 109]\n",
      "[106, 6, 106, 60, 106, 9, 106, 55, 107, 107, 106, 95, 107, 106, 41, 106, 48, 107, 107, 106, 80, 106, 33, 107, 106, 80, 106, 73, 107, 106, 45, 106, 24, 107, 106, 75, 107, 106, 41, 106, 37, 107, 107, 107, 106, 9, 106, 55, 107, 107, 106, 61, 106, 24, 107, 106, 60, 106, 41, 106, 48, 107, 107, 106, 80, 106, 33, 107, 106, 80, 106, 73, 107, 106, 41, 106, 7, 106, 17, 107, 106, 27, 107, 107, 106, 36, 107, 107, 107, 107, 107, 107, 107, 107, 106, 94, 107, 107, 107]\n",
      "[1, 187, 110, 131, 252, 1726, 132, 2060, 322, 188, 248, 124, 166, 127, 229, 210, 1775, 109, 2]\n"
     ]
    }
   ],
   "source": [
    "# print(nmt_dataset['sentences'][4])\n",
    "# print(nmt_dataset['syntatic'][4])\n",
    "# print(nmt_dataset['targets'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:15<00:00, 13.20it/s]\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(qq_sentences)\n",
    "\n",
    "qq_dataset = prepare_paraphrase_dataset(qq_sentences[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15787, 1014, 14861, 735, 7144, 5532, 615, 263, 130, 763, 161, 115, 4963, 1750, 813, 119, 10035, 2108, 3, 948, 116, 266, 172, 115, 14861, 735, 7144, 173, 948, 172, 163, 111, 1036, 1418, 15849, 119]\n",
      "[106, 6, 106, 60, 106, 41, 106, 50, 107, 106, 38, 107, 106, 36, 107, 106, 36, 107, 107, 106, 80, 106, 33, 107, 106, 80, 106, 73, 107, 106, 80, 106, 75, 107, 106, 45, 106, 24, 107, 106, 45, 106, 24, 107, 106, 41, 106, 36, 107, 107, 107, 107, 107, 107, 107, 106, 94, 107, 107, 107]\n",
      "[1, 15787, 1014, 14861, 735, 7144, 5532, 615, 263, 130, 763, 161, 115, 4963, 1750, 813, 119, 10035, 2108, 3, 948, 116, 266, 172, 115, 14861, 735, 7144, 173, 948, 172, 163, 111, 1036, 1418, 15849, 119, 2]\n"
     ]
    }
   ],
   "source": [
    "print(qq_dataset['sentences'][1])\n",
    "print(qq_dataset['syntatic'][1])\n",
    "print(qq_dataset['targets'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Datasets/nmt_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(nmt_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./Datasets/qq_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(qq_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "def syntax_tensor(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    synt_ = [dictionary.word2idx[f\"<{w}>\"] for w in synt_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    synt_ = [dictionary.word2idx[\"<sos>\"]] + synt_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return synt_\n",
    "\n",
    "def tag_sequence(sent_):\n",
    "    sent_ = ParentedTree.fromstring(sent_)\n",
    "    sent_ = getleaf(sent_)\n",
    "    sent_ = [dictionary.word2idx[f\"<{w}>\"] for w in sent_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    return sent_\n",
    "\n",
    "#parse syntax and get template\n",
    "from nltk import ParentedTree\n",
    "\n",
    "def tree2tmpl(tree, level, mlevel):\n",
    "    if level == mlevel:\n",
    "        for idx, n in enumerate(tree):\n",
    "            if isinstance(n, ParentedTree):\n",
    "                tree[idx] = \"(\" + n.label() + \")\"\n",
    "    else:\n",
    "        for n in tree:\n",
    "            tree2tmpl(n, level + 1, mlevel)\n",
    "\n",
    "def template(tmpl_):\n",
    "    tmpl_ = ParentedTree.fromstring(tmpl_)\n",
    "    tree2tmpl(tmpl_, 1, 2)\n",
    "    tmpl_ = str(tmpl_).replace(\")\", \" )\").replace(\"(\", \"( \").split(\" \")\n",
    "    tmpl_ = [dictionary.word2idx[f\"<{w}>\"] for w in tmpl_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    tmpl_ = [dictionary.word2idx[\"<sos>\"]] + tmpl_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return tmpl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_parse_dataset(sentences):\n",
    "    sents, tmpls, synts = list(), list(), list()\t\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "        parser = constituency_parser(sentences[idx])\n",
    "        sent_ = tag_sequence(parser)\n",
    "        tmpl_ = template(parser)\n",
    "        synt_ = syntax_tensor(parser)\n",
    "\n",
    "        sents.append(sent_)  #sents\n",
    "        tmpls.append(tmpl_)  #synts\n",
    "        synts.append(synt_)  #targs\n",
    "    return {'sentences':sents, 'templates':tmpls, 'syntatic': synts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]c:\\Python3.10.4\\lib\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "100%|██████████| 1000/1000 [01:21<00:00, 12.28it/s]\n"
     ]
    }
   ],
   "source": [
    "nmt_parse = prepare_parse_dataset(nmt_sentences[:1000]) #1000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 38, 55, 74, 68, 73, 11, 73, 48, 94]\n",
      "[1, 106, 6, 106, 60, 106, 41, 107, 106, 9, 107, 106, 80, 107, 106, 94, 107, 107, 107, 2]\n",
      "[1, 106, 6, 106, 60, 106, 41, 106, 36, 107, 106, 38, 107, 107, 106, 9, 106, 55, 107, 107, 106, 80, 106, 74, 107, 106, 60, 106, 80, 106, 68, 107, 106, 80, 106, 80, 106, 73, 107, 107, 106, 11, 107, 106, 73, 107, 106, 41, 106, 48, 107, 107, 107, 107, 107, 107, 106, 94, 107, 107, 107, 2]\n"
     ]
    }
   ],
   "source": [
    "print(nmt_parse['sentences'][0])\n",
    "print(nmt_parse['templates'][0])\n",
    "print(nmt_parse['syntatic'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:20<00:00, 12.43it/s]\n"
     ]
    }
   ],
   "source": [
    "qq_parse = prepare_parse_dataset(qq_sentences[:1000]) #1000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86, 77, 17, 26, 37, 26, 37, 24, 38, 77, 94]\n",
      "[1, 106, 6, 106, 62, 106, 84, 107, 106, 65, 107, 106, 94, 107, 107, 107, 2]\n",
      "[1, 106, 6, 106, 62, 106, 84, 106, 86, 107, 107, 106, 65, 106, 77, 107, 106, 41, 106, 41, 106, 17, 107, 106, 26, 107, 106, 37, 107, 107, 106, 61, 106, 60, 106, 41, 106, 41, 106, 26, 107, 106, 37, 107, 107, 106, 45, 106, 24, 107, 106, 41, 106, 38, 107, 107, 107, 107, 106, 80, 106, 77, 107, 107, 107, 107, 107, 107, 106, 94, 107, 107, 107, 2]\n"
     ]
    }
   ],
   "source": [
    "print(qq_parse['sentences'][0])\n",
    "print(qq_parse['templates'][0])\n",
    "print(qq_parse['syntatic'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Datasets/nmt_parse.pkl', 'wb') as f:\n",
    "    pickle.dump(nmt_parse, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./Datasets/qq_parse.pkl', 'wb') as f:\n",
    "    pickle.dump(qq_parse, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
