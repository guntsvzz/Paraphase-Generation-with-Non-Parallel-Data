{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1):\n",
    "        super(seq2seqTransformer, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        # self.scale = torch.sqrt(torch.IntTensor([self.hid_dim])).to(device)\n",
    "\n",
    "        # vcocabulary embedding\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 8, dropout = dropout)\n",
    "        # linear Transformation\n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "        # self.init_weights()\n",
    "\n",
    "    def load_embedding(self, embedding):  #synPG applied with GloVe glove.840B.300d.txt\n",
    "        self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "        self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding))  \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        #trgs   : batch_size, seq_len \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2    # count without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1) - 2      # count without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, -1e10)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        #sent_emb = [seq_len, batch size, emb_size]\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings) \n",
    "        #synt_emb = [seq_len, batch size, emb_size]\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        #en_emb = [seq_len, batch size, emb_size*2]\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # target => embedding\n",
    "        de_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = self.transformer(en_embeddings, de_embeddings, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "        \n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim))\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim)\n",
    "        #output = [batch size, trg_len, vocab_size]\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings)\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        de_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(en_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            if i % 5 == 0:\n",
    "                print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(de_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            de_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            de_embeddings = self.pos_encoder(de_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple') ##Load fasttext with language=simple\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab_transform.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "input_dim = 1000 #len(vocab)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "model = seq2seqTransformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model.embedding_encoder.weight.data = fast_embedding #apply fasttext instead of Glove 840b 300d.txt (5.56 GB) TT\n",
    "model.embedding_decoder.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5\n",
      "epoch : 10\n",
      "epoch : 15\n",
      "epoch : 20\n",
      "epoch : 25\n",
      "epoch : 30\n"
     ]
    }
   ],
   "source": [
    "#check_model\n",
    "# sents   = torch.LongTensor(16, 30).random_(0, 10) #sents: [batch size, seq_len]\n",
    "# synts   = torch.LongTensor(16, 30).random_(0, 10) #synts: [batch size, seq_len]\n",
    "# output  = model.generate(sents, synts)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for sents_, synts_, trgs_ in loader:\n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward \n",
    "        outputs = model(sents_, synts_, trgs_)\n",
    "\n",
    "        # calculate loss\n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(outputs_, targs_)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_ in loader:\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            #forward \n",
    "            outputs = model(sents_, synts_, trgs_)\n",
    "\n",
    "            # calculate loss\n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "            \n",
    "            loss = criterion(outputs_, targs_)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-5 #Following SynPG\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training\n",
    "    train_loss = train()\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding\n",
    "\n",
    "def generate(model, loader, criterion, loader_length,vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_ in loader:\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # generate\n",
    "            idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "            \n",
    "            # write output\n",
    "            for sent, idx, synt in zip(sents_.cpu().numpy(), idxs.cpu().numpy(), synts_.cpu().numpy()):\n",
    "                print(synt2str(synt[1:], vocab_transform)+'\\n')\n",
    "                print(sent2str(sent, vocab_transform)+'\\n')\n",
    "                print(synt2str(idx, vocab_transform)+'\\n')\n",
    "                print(\"--\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
