{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ParaNMT\n",
    "with open('./data/para-nmt-5m-processed.txt', encoding=\"utf8\") as file:\n",
    "    nmt = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"so , unless that 's gon na be feasible , then ...\",\n",
       " 'of course you did .',\n",
       " \"by now , singh 's probably been arrested .\",\n",
       " 'not our shit . i swear .',\n",
       " 'â€œ why not ?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_sentences = list()\n",
    "for idx, sentence in enumerate(nmt):\n",
    "    nmt_sentences.append(nmt[idx].split(\"\\t\")[0]) #non-parallel data\n",
    "nmt_sentences[:5] #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_sentences) #5370128 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# benepar.download('benepar_en3')\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\"\n",
    "\n",
    "def getleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "    \n",
    "    leaves = []\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                leaves.append(arr[n])\n",
    "\n",
    "    return leaves\n",
    "\n",
    "def deleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                arr[n + 1] = \"\"\n",
    "\n",
    "    nonleaves = \" \".join(arr)\n",
    "    return nonleaves.split()\n",
    "\n",
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "#Setence to syntax\n",
    "def constituency_parser(text):\n",
    "    doc = nlp(text)\n",
    "    sent = list(doc.sents)[0]\n",
    "    return \"(ROOT \"+sent._.parse_string+\")\"\n",
    "\n",
    "#syntax to syntatic tokenzier\n",
    "from nltk import ParentedTree\n",
    "def parser_tokenizer(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    # synt_ = [f'<{w}>' for w in synt_]\n",
    "    synt_ = [dictionary.word2idx[f\"<{w}>\"] for w in synt_ if f\"<{w}>\" in dictionary.word2idx]\n",
    "    return synt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer BPE\n",
    "from subwordnmt.apply_bpe import BPE, read_vocabulary\n",
    "import codecs\n",
    "\n",
    "# load bpe codes\n",
    "bpe_codes = codecs.open('./data/bpe.codes', encoding='utf-8')\n",
    "bpe_vocab = codecs.open('./data/vocab.txt', encoding='utf-8')\n",
    "bpe_vocab = read_vocabulary(bpe_vocab, 50)\n",
    "bpe = BPE(bpe_codes, '@@', bpe_vocab, None)\n",
    "\n",
    "def bpe_tokenizer(sent_, target = False):\n",
    " # bpe segment and convert to tensor\n",
    "    sent_ = bpe.segment(sent_).split()\n",
    "    sent_ = [dictionary.word2idx[w] if w in dictionary.word2idx else dictionary.word2idx[\"<unk>\"] for w in sent_]\n",
    "    if target:\n",
    "        sent_ = [dictionary.word2idx[\"<sos>\"]] + sent_ + [dictionary.word2idx[\"<eos>\"]]\n",
    "    return sent_\n",
    "\n",
    "# def pos_tag(sent_):\n",
    "#     return [token.ent_type_ if token.ent_type_ else \"\" for token in nlp(sent_)]\n",
    "\n",
    "# def dependency_parser(sent_):\n",
    "#     return [token.dep_ for token in nlp(sent_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, '<pad>': 1, '</s>': 2, '(': 3, 'ROOT': 4, 'S': 5, 'ADVP': 6, 'RB': 7, ')': 8, ',': 9, 'INTJ': 10, 'UH': 11, 'FW': 12, 'VP': 13, 'VBP': 14, 'ADJP': 15, 'JJ': 16, ':': 17, 'NP': 18, 'NN': 19, '.': 20, 'PRP': 21, 'VBD': 22, 'VBG': 23, 'TO': 24, 'VB': 25, 'PRT': 26, 'FRAG': 27, 'SBAR': 28, 'IN': 29, 'QP': 30, 'VBZ': 31, 'CD': 32, 'PP': 33, 'VBN': 34, 'DT': 35, 'CC': 36, 'NNS': 37, 'PRP$': 38, 'WHNP': 39, 'WP': 40, 'LS': 41, 'NNP': 42, 'SINV': 43, 'PRN': 44, '``': 45, \"''\": 46, 'JJR': 47, 'WDT': 48, 'POS': 49, 'MD': 50, 'SQ': 51, 'SBARQ': 52, 'WHADVP': 53, 'WRB': 54, 'RP': 55, 'EX': 56, 'JJS': 57, 'X': 58, 'LST': 59, '-LRB-': 60, '-RRB-': 61, 'RBS': 62, 'UCP': 63, 'RBR': 64, 'WHPP': 65, 'PDT': 66, 'WHADJP': 67, 'NX': 68, 'CONJP': 69, '$': 70, 'WP$': 71, '#': 72, 'SYM': 73, 'NNPS': 74, 'RRC': 75, 'NAC': 76}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import pickle\n",
    "with open('synt_vocab.pkl', 'rb') as f:\n",
    "    synt_vocab = pickle.load(f)\n",
    "\n",
    "print(synt_vocab)\n",
    "\n",
    "def bow(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    synt_bow = np.ones(74)\n",
    "    #non-inculde <s> , <pad>, </s>\n",
    "    #start from '(': 3 instead\n",
    "    for tag in synt_:\n",
    "        if tag != '<sos>' and tag != '<eos>':\n",
    "            try:\n",
    "                synt_bow[synt_vocab[tag]-3] += 1\n",
    "            except:\n",
    "                pass\n",
    "    synt_bow /= synt_bow.sum()\n",
    "    return synt_bow "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversary Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_paraphrase_dataset(sentences):\n",
    "    lists_ = list()\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "\n",
    "        sent_ = bpe_tokenizer(sentences[idx])\n",
    "        synt_ = parser_tokenizer(constituency_parser(sentences[idx]))\n",
    "        trg_  = bpe_tokenizer(sentences[idx], target = True)\n",
    "        bow_  = bow(constituency_parser(sentences[idx]))\n",
    "        lists_.append((sent_, synt_, trg_, bow_))\n",
    "    return lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_1m = prepare_paraphrase_dataset(nmt_sentences[:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_paraphrase_dataset_onlybow(sentences):\n",
    "    lists_ = list()\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "\n",
    "        # sent_ = bpe_tokenizer(sentences[idx])\n",
    "        # synt_ = parser_tokenizer(constituency_parser(sentences[idx]))\n",
    "        # trg_  = bpe_tokenizer(sentences[idx], target = True)\n",
    "        bow_  = bow(constituency_parser(sentences[idx]))\n",
    "        # lists_.append((sent_, synt_, trg_, bow_))\n",
    "        lists_.append((bow_))\n",
    "    return lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4570128 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "  0%|          | 168/4570128 [00:10<86:06:24, 14.74it/s]"
     ]
    }
   ],
   "source": [
    "bow_900k = prepare_paraphrase_dataset_onlybow(nmt_sentences[800000:9000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/bow_900k.pkl', 'wb') as f:\n",
    "    pickle.dump(bow_900k, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
