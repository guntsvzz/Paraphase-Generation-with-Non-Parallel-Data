{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generator with Adversarial Discriminator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "with open(\"data/aLL_bow_100k.pkl\", \"rb\") as file:\n",
    "    nmt_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<msk>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dictionary.word2idx.keys())[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31414"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA & 3. Preprocessing\n",
    "- Done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.float32)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_dataset)\n",
    "\n",
    "train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "train_set = nmt_dataset[:train_range]\n",
    "val_set   = nmt_dataset[train_range:]\n",
    "# test_set = train_data[90:]\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_set, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sen, syn, trg, adv in train_dataloader:\n",
    "#     print(sen.shape)\n",
    "#     print(syn.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(adv.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        # sent_embeddings : batch_size, seq_len, hid_dim\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1) # batch_size, hid_dim\n",
    "        x = self.adv(x) # batch_size, hid_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, input_token_ids):\n",
    "        mask = input_token_ids != pad_idx\n",
    "        mean_mask = mask.float()/mask.float().sum(1, keepdim=True)\n",
    "        x = (x * mean_mask.unsqueeze(2)).sum(1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "\n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "        self.pooling = MeanPooling()\n",
    "\n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def load_embedding(self, embedding): \n",
    "            self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "            self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        #synts : batch_size, synt_len, emb_dim\n",
    "        #trgs  : batch_size, trg_len, emb_dim \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1)   - 2  # without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents_ = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents_).transpose(0, 1) * self.scale # sent_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale # synt_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings) # synt_len, batch_size, emb_dim\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0) # synt_len + seq_len, batch size, emb_size\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "\n",
    "        # target => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings) # trg_len, batch_size, emb_dim\n",
    "\n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "\n",
    "        # forward\n",
    "        outputs = self.transformer(encoder_embeddings, decoder_embeddings, src_mask=src_mask, tgt_mask=trg_mask) # trg_len, batch_size, emb_dim\n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1) # batch_size, trg_len, emb_dim\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim)) # batch_size*trg_len, input_dim\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim) # batch_size, trg_len, input_dim\n",
    "\n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "\n",
    "        adv_outputs = self.adversary(sent_embeds).transpose(0, 1) # batch_size, 74   \n",
    "\n",
    "        return outputs, adv_outputs\n",
    "        \n",
    "    def forward_token(self, sents):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(sents.shape)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 4) #vocab_size <msk>\n",
    "\n",
    "        sent_embeddings = self.embedding_encoder(sents) # batch_size, sent_len, emb_dim\n",
    "        sent_embeddings = self.positional_encoder(sent_embeddings) # batch_size, sent_len, emb_dim\n",
    "\n",
    "        sent_embeddings = self.norm(sent_embeddings)\n",
    "        sent_embeddings = F.dropout(sent_embeddings, p=self.dropout)\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        for encoder_layer in self.transformer.encoder.layers:\n",
    "            sent_embeddings = encoder_layer(sent_embeddings)\n",
    "\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        return sent_embeddings \n",
    "    \n",
    "    def forward_adv(self, sents):\n",
    "\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "        \n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "        adv_outputs = self.adversary(sent_embeds)\n",
    "        return adv_outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len, emb_dim \n",
    "        #synts  : batch_size, seq_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings)\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(encoder_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            # if i % 5 == 0:\n",
    "            #     print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(decoder_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            decoder_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = 'data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "49621464 parameters\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "    #     print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6} parameters')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44406/1575877881.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for idx, (sents_, synts_, trgs_, adv_targs) in enumerate(tqdm(loader)):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        \n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "        #optimize adversarial\n",
    "        outputs = model.forward_adv(sents_) #batch_size, 74\n",
    "        loss = adv_criterion(outputs, adv_targs)\n",
    "    \n",
    "        loss.backward()\n",
    "        adv_total_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            if epoch > 0:\n",
    "                adv_optimizer.step()\n",
    "            adv_optimizer.zero_grad()\n",
    "\n",
    "        #optimize model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #seq_len, batch_size, 74\n",
    "        \n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "\n",
    "        if epoch > 0: \n",
    "            loss -= 0.1 * adv_criterion(adv_outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            para_optimizer.step()\n",
    "            para_optimizer.zero_grad()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "\n",
    "    # print('Para',para_loss)\n",
    "    # print('Adv', adv_loss)\n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-2 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:18<00:00, 17.54it/s]\n",
      "100%|██████████| 3750/3750 [01:02<00:00, 60.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 21s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 7.033 |  Val. PPL: 1133.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:55<00:00, 16.34it/s]\n",
      "100%|██████████| 3750/3750 [01:02<00:00, 59.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 9m 58s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 6.959 |  Val. PPL: 1052.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:55<00:00, 16.33it/s]\n",
      "100%|██████████| 3750/3750 [01:02<00:00, 59.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 9m 58s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 6.774 |  Val. PPL: 874.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:55<00:00, 16.33it/s]\n",
      "100%|██████████| 3750/3750 [01:02<00:00, 59.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 9m 58s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 6.762 |  Val. PPL: 864.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:55<00:00, 16.35it/s]\n",
      "100%|██████████| 3750/3750 [01:02<00:00, 59.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 9m 57s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: 6.772 |  Val. PPL: 873.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt_modify.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "para_losses = []\n",
    "adv_losses = [] \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length, accumulation_steps=4)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(float(valid_loss))\n",
    "    # para_losses.append(adv_loss)\n",
    "    # adv_losses.append(adv_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    # print(f'\\tPara. Loss: {valid_loss:.3f} | Para. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    # print(f'\\t Adv. Loss: {valid_loss:.3f} |  Adv. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, loader_length, vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/target_sents_adv.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/target_synts_adv.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/outputs_adv.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # Put input into device\n",
    "                sents_ = sents_.to(device)\n",
    "                synts_ = synts_.to(device)\n",
    "                trgs_ = trgs_.to(device)\n",
    "                adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    # fp1.write(targ+'\\n')\n",
    "                    # fp2.write(synt_+'\\n')\n",
    "                    # fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')\n",
    "                    \n",
    "                    convert_sent = reverse_bpe(sent2str(sent.tolist(), vocab_transform).split()) + '\\n'\n",
    "                    convert_synt = synt2str(synt_[1:].tolist(), vocab_transform).replace(\"<pad>\", \"\") + '\\n' \n",
    "                    convert_idx = synt2str(idx, vocab_transform) +'\\n'\n",
    "                    \n",
    "                    fp1.write(convert_sent)\n",
    "                    fp2.write(convert_synt)\n",
    "                    fp3.write(convert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [08:01<00:00,  7.79it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/adversary_nmt_modify.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "generate(model, valid_dataloader, val_loader_length, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 30000\n"
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents_adv.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs_adv.txt') as fp: \n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\n', 'of\\n', '.\\n', ', ? . in .\\n', '\\n', '\\n', '\\n', '\\n', ',\\n', '.\\n']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"man , you 've got forman , kelso , an old man and a naked butt .\\n\",\n",
       " \"shut up or i 'm gon na punch you in the mouth !\\n\",\n",
       " 'she had a daughter .\\n',\n",
       " \"i think you 're great and i 've had a ton of fun , but ... you know ,\\n\",\n",
       " 'mouse-manipulation : movement , drag and drop\\n',\n",
       " 'you need speed .\\n',\n",
       " \"let 's discuss this somewhere else .\\n\",\n",
       " 'and as this mysterious force moves across the southern valley ... it leaves behind only silence ... and death .\\n',\n",
       " \"michael , where are you ? we 're still in the hotel .\\n\",\n",
       " 'all right , come here .\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 7.423783288612225e-81\n"
     ]
    }
   ],
   "source": [
    "scores = [cal_bleu(pred, targ, 0) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
