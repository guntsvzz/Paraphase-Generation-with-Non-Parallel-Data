{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generator with Adversarial Discriminator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "with open(\"data/aLL_bow_100k.pkl\", \"rb\") as file:\n",
    "    nmt_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA & 3. Preprocessing\n",
    "- Done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.float32)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_dataset)\n",
    "\n",
    "train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "train_set = nmt_dataset[:train_range]\n",
    "val_set   = nmt_dataset[train_range:]\n",
    "# test_set = train_data[90:]\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_set, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sen, syn, trg, adv in train_dataloader:\n",
    "#     print(sen.shape)\n",
    "#     print(syn.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(adv.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        # sent_embeddings : batch_size, seq_len, hid_dim\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1) # batch_size, hid_dim\n",
    "        x = self.adv(x) # batch_size, hid_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, input_token_ids):\n",
    "        mask = input_token_ids != pad_idx\n",
    "        mean_mask = mask.float()/mask.float().sum(1, keepdim=True)\n",
    "        x = (x * mean_mask.unsqueeze(2)).sum(1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "\n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "        self.pooling = MeanPooling()\n",
    "\n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def load_embedding(self, embedding): \n",
    "            self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "            self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        #synts : batch_size, synt_len, emb_dim\n",
    "        #trgs  : batch_size, trg_len, emb_dim \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1)   - 2  # without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents_ = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents_).transpose(0, 1) * self.scale # sent_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale # synt_len, batch_size, emb_dim\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings) # synt_len, batch_size, emb_dim\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0) # synt_len + seq_len, batch size, emb_size\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "\n",
    "        # target => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings) # trg_len, batch_size, emb_dim\n",
    "\n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "\n",
    "        # forward\n",
    "        outputs = self.transformer(encoder_embeddings, decoder_embeddings, src_mask=src_mask, tgt_mask=trg_mask) # trg_len, batch_size, emb_dim\n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1) # batch_size, trg_len, emb_dim\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim)) # batch_size*trg_len, input_dim\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim) # batch_size, trg_len, input_dim\n",
    "\n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "\n",
    "        adv_outputs = self.adversary(sent_embeds).transpose(0, 1) # batch_size, 74   \n",
    "\n",
    "        return outputs, adv_outputs\n",
    "        \n",
    "    def forward_token(self, sents):\n",
    "        #sents : batch_size, sent_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(sents.shape)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        sent_embeddings = self.embedding_encoder(sents) # batch_size, sent_len, emb_dim\n",
    "        sent_embeddings = self.positional_encoder(sent_embeddings) # batch_size, sent_len, emb_dim\n",
    "\n",
    "        sent_embeddings = self.norm(sent_embeddings)\n",
    "        sent_embeddings = F.dropout(sent_embeddings, p=self.dropout)\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        for encoder_layer in self.transformer.encoder.layers:\n",
    "            sent_embeddings = encoder_layer(sent_embeddings)\n",
    "\n",
    "        sent_embeddings = sent_embeddings.transpose(0, 1) # batch_size, sent_len, 74\n",
    "        return sent_embeddings \n",
    "    \n",
    "    def forward_adv(self, sents):\n",
    "\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "        \n",
    "        x = self.forward_token(sents).detach()\n",
    "        sent_embeds = self.pooling(x, sents)\n",
    "        adv_outputs = self.adversary(sent_embeds)\n",
    "        return adv_outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len, emb_dim \n",
    "        #synts  : batch_size, seq_len, emb_dim\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.positional_encoder(synt_embeddings)\n",
    "        encoder_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        decoder_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(encoder_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            # if i % 5 == 0:\n",
    "            #     print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(decoder_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            decoder_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            decoder_embeddings = self.positional_encoder(decoder_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(decoder_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = 'data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "49621464 parameters\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "    #     print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6} parameters')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43884/1575877881.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for idx, (sents_, synts_, trgs_, adv_targs) in enumerate(tqdm(loader)):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        \n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "        #optimize adversarial\n",
    "        outputs = model.forward_adv(sents_) #batch_size, 74\n",
    "        loss = adv_criterion(outputs, adv_targs)\n",
    "    \n",
    "        loss.backward()\n",
    "        adv_total_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            if epoch > 0:\n",
    "                adv_optimizer.step()\n",
    "            adv_optimizer.zero_grad()\n",
    "\n",
    "        #optimize model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #seq_len, batch_size, 74\n",
    "        \n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "\n",
    "        if epoch > 0: \n",
    "            loss -= 0.1 * adv_criterion(adv_outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            para_optimizer.step()\n",
    "            para_optimizer.zero_grad()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "\n",
    "    # print('Para',para_loss)\n",
    "    # print('Adv', adv_loss)\n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-5 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [08:20<00:00, 17.49it/s]\n",
      "100%|██████████| 3750/3750 [01:02<00:00, 60.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 22s\n",
      "\tTrain Loss: 0.001 | Train PPL:   1.001\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 464/8750 [00:28<07:57, 17.33it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 2\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt_modify.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "para_losses = []\n",
    "adv_losses = [] \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length, accumulation_steps=4)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(float(valid_loss))\n",
    "    # para_losses.append(adv_loss)\n",
    "    # adv_losses.append(adv_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    # print(f'\\tPara. Loss: {valid_loss:.3f} | Para. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    # print(f'\\t Adv. Loss: {valid_loss:.3f} |  Adv. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m ax\u001b[39m.\u001b[39mplot(train_losses, label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m ax\u001b[39m.\u001b[39;49mplot(valid_losses, label \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mvalid loss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n\u001b[1;32m      8\u001b[0m ax\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mupdates\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py:496\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    494\u001b[0m     y \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m1\u001b[39m])\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     x, y \u001b[39m=\u001b[39m index_of(xy[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mxaxis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mxaxis\u001b[39m.\u001b[39mupdate_units(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py:1656\u001b[0m, in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1656\u001b[0m     y \u001b[39m=\u001b[39m _check_1d(y)\n\u001b[1;32m   1657\u001b[0m \u001b[39mexcept\u001b[39;00m (np\u001b[39m.\u001b[39mVisibleDeprecationWarning, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1658\u001b[0m     \u001b[39m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py:1348\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[39m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[39m# Note this will strip unit information.\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m         \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m-> 1348\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49matleast_1d(x)\n\u001b[1;32m   1349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m ary \u001b[39min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[39m=\u001b[39m asanyarray(ary)\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m ary\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[39m=\u001b[39m ary\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, loader_length, vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/target_sents_adv.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/target_synts_adv.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/outputs_adv.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # Put input into device\n",
    "                sents_ = sents_.to(device)\n",
    "                synts_ = synts_.to(device)\n",
    "                trgs_ = trgs_.to(device)\n",
    "                adv_targs = adv_targs.to(device) #batch_size, 74\n",
    "\n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    # fp1.write(targ+'\\n')\n",
    "                    # fp2.write(synt_+'\\n')\n",
    "                    # fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')\n",
    "                    \n",
    "                    convert_sent = reverse_bpe(sent2str(sent.tolist(), vocab_transform).split()) + '\\n'\n",
    "                    convert_synt = synt2str(synt_[1:].tolist(), vocab_transform).replace(\"<pad>\", \"\") + '\\n' \n",
    "                    convert_idx = synt2str(idx, vocab_transform) +'\\n'\n",
    "                    \n",
    "                    fp1.write(convert_sent)\n",
    "                    fp2.write(convert_synt)\n",
    "                    fp3.write(convert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [07:57<00:00,  7.85it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/adversary_nmt_modify.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "generate(model, valid_dataloader, val_loader_length, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 30000\n"
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents_adv.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs_adv.txt') as fp: \n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i have .\\n',\n",
       " 'i , .\\n',\n",
       " 'i . .\\n',\n",
       " 'i on . .\\n',\n",
       " 'i i . ,\\n',\n",
       " 'you . . . .\\n',\n",
       " 'i -- .\\n',\n",
       " 'i\\n',\n",
       " 'i . that .\\n',\n",
       " 'you . . . . .\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they think that it is the work of a madman .\\n',\n",
       " \"i 'll stay for a minute , but i can promise you that no fun will be had by me at all .\\n\",\n",
       " \"just do n't hit me any more in my nuts .\\n\",\n",
       " 'this is why , in my understanding , the issue did not need to be tackled in br<unk> stle .\\n',\n",
       " \"taylor : i 'm sure it wo n't be long now .\\n\",\n",
       " \"i mean , we 're not spying on casey . we 're watching his back .\\n\",\n",
       " \"hub and garth did n't rob any banks .\\n\",\n",
       " \"he 'll try and hunt us down .\\n\",\n",
       " 'you must have a lot of courage , vicomte ... ... to come here after you sent that ... ... lunatic harp teacher to kill me .\\n',\n",
       " 'now what do you think ? what do you think about this ?\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 5.275438701111479\n"
     ]
    }
   ],
   "source": [
    "scores = [cal_bleu(pred, targ, 1) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
