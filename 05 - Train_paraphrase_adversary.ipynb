{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generator with Adversarial Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "with open(\"./data/adversary_1000.pkl\", \"rb\") as file:\n",
    "    nmt_dataset = pickle.load(file)\n",
    "\n",
    "# with open(\"./Datasets/qq_dataset.pkl\", \"rb\") as file:\n",
    "#     qq_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.int64)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_dataset)\n",
    "\n",
    "train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "train_set = nmt_dataset[:train_range]\n",
    "val_set   = nmt_dataset[train_range:]\n",
    "# test_set = train_data[90:]\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_set, batch_size=32, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 26])\n",
      "torch.Size([32, 156])\n",
      "torch.Size([32, 28])\n",
      "torch.Size([32, 74])\n"
     ]
    }
   ],
   "source": [
    "for idx, (sen, syn, trg, adv) in enumerate(train_dataloader):\n",
    "    print(sen.shape)\n",
    "    print(syn.shape)\n",
    "    print(trg.shape)\n",
    "    print(adv.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1)\n",
    "        # print('Layer Norm',x.shape)\n",
    "        x = self.adv(x)\n",
    "        # print('Linear Layer',x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Adversary(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer_Adversary, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        # self.scale = torch.sqrt(torch.IntTensor([self.hid_dim])).to(device)\n",
    "        # synt\n",
    "        # vocabulary embedding\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim) # token embedding\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "        # positional embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, emb_dim) # position embedding\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        # linear Transformation\n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "    def load_embedding(self, embedding):  #synPG applied with GloVe glove.840B.300d.txt\n",
    "        self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "        self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding))  \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        #trgs   : batch_size, seq_len \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2    # count without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1) - 2      # count without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        #sent_emb = [seq_len, batch size, emb_size]\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings) \n",
    "        #synt_emb = [seq_len, batch size, emb_size]\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        #en_emb = [seq_len, batch size, emb_size*2]\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # target => embedding\n",
    "        de_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = self.transformer(en_embeddings, de_embeddings, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "\n",
    "        # encoder_outputs = self.transformer.encoder(sent_embeddings)\n",
    "\n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim))\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim)\n",
    "        #output = [batch size, trg_len, vocab_size]\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "        adv_outputs = self.adversary(sent_embeddings)        \n",
    "\n",
    "        return outputs, adv_outputs\n",
    "    \n",
    "    def forward_token(self, sents):\n",
    "        #sent : batch_size, seq_len, emb_dim\n",
    "        drop_mask = torch.bernoulli(self.word_dropout*torch.ones(sents.shape)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        batch_size, seq_len = sents.size(0), sents.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(sents)  # (len,) -> (bs, len)\n",
    "\n",
    "        emb_en = self.embedding_encoder(sents)\n",
    "        # print('emb_en',emb_en.shape)\n",
    "        emb_pos = self.pos_embed(pos)\n",
    "        # print('emb_pos',emb_pos.shape)\n",
    "        embedding = emb_en + emb_pos\n",
    "        # print('en_embeddings',embedding.shape)\n",
    "\n",
    "        # embedding = embedding.view(batch_size, seq_len * self.emb_dim)\n",
    "        # print('en_embeddings',embedding.shape)\n",
    "        x = self.norm(embedding)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        # x = x.transpose(0, 1)\n",
    "        for encoder_layer in self.transformer.encoder.layers:\n",
    "            x = encoder_layer(x)\n",
    "            \n",
    "        # x = x.transpose(0, 1)\n",
    "        return x\n",
    "\n",
    "    def forward_adv(self, sents):\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "        \n",
    "        # sent_embeddings = self.embedding_encoder(sents).transpose(0, 1).detach()\n",
    "        sent_embeddings = self.forward_token(sents).detach()\n",
    "        # print('Sentence Embedding', sent_embeddings.shape)\n",
    "\n",
    "        adv_outputs = self.adversary(sent_embeddings)\n",
    "        # print('Adversary', adv_outputs.shape)\n",
    "        return adv_outputs\n",
    "\n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings)\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        de_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.encoder(en_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            if i % 5 == 0:\n",
    "                print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(de_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            de_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            de_embeddings = self.pos_encoder(de_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import FastText\n",
    "# fast_vectors = FastText(language='simple') ##Load fasttext with language=simple\n",
    "# fast_embedding = fast_vectors.get_vecs_by_tokens(vocab_transform.get_itos()).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer_Adversary(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)\n",
    "# model.embedding_encoder.weight.data = fast_embedding #apply fasttext instead of Glove 840b 300d.txt (5.56 GB) TT\n",
    "# model.embedding_decoder.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_model\n",
    "# sents   = torch.LongTensor(16, 30).random_(0, 10) #sents: [batch size, seq_len]\n",
    "# synts   = torch.LongTensor(16, 30).random_(0, 10) #synts: [batch size, seq_len]\n",
    "# output  = model.generate(sents, synts)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for idx, (sents_, synts_, trgs_, adv_targs) in tqdm(enumerate(loader)):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device)\n",
    "        # print(adv_targs.shape)\n",
    "\n",
    "        #optimize adv\n",
    "        outputs = model.forward_adv(sents_)\n",
    "        # output = [batch size, trg_len, output dim]\n",
    "\n",
    "        adv_targs = adv_targs.unsqueeze(1).expand(batch_size, outputs.shape[1], 74).to(torch.float)\n",
    "        # output = [batch size, trg_len, output dim]\n",
    "        # print(adv_targs.dtype)\n",
    "        loss = adv_criterion(outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        adv_total_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            if epoch > 1:\n",
    "                adv_optimizer.step()\n",
    "        adv_optimizer.zero_grad()\n",
    "\n",
    "        #optimize model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "        # calculate loss\n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #batch_size, seq_len\n",
    "\n",
    "        if epoch > 1:\n",
    "            loss -= 0.1 * adv_criterion(adv_outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            para_optimizer.step()\n",
    "            para_optimizer.zero_grad()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            # print('adv output',adv_outputs.shape)\n",
    "            # print('adv trg',adv_targs.shape)\n",
    "            \n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "            adv_targs = adv_targs.unsqueeze(1).expand(batch_size, adv_outputs.shape[1], 74).to(torch.float)\n",
    "            \n",
    "            # print('adv trg',adv_targs.shape)\n",
    "            # print('adv output',adv_outputs.shape)\n",
    "            # adv output torch.Size([27, 30, 74])\n",
    "            # adv trg torch.Size([30, 74])\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "            \n",
    "        \n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-5 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:32, 10.89s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 34s\n",
      "\tTrain Loss: 1.886 | Train PPL:   6.592\n",
      "\t Val. Loss: 7.100 |  Val. PPL: 1211.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:32, 10.86s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 34s\n",
      "\tTrain Loss: 2.802 | Train PPL:  16.482\n",
      "\t Val. Loss: 7.040 |  Val. PPL: 1141.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:32, 10.77s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 34s\n",
      "\tTrain Loss: 1.760 | Train PPL:   5.810\n",
      "\t Val. Loss: 6.967 |  Val. PPL: 1061.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:37, 12.51s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 39s\n",
      "\tTrain Loss: 1.684 | Train PPL:   5.386\n",
      "\t Val. Loss: 6.876 |  Val. PPL: 968.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:36, 12.14s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 38s\n",
      "\tTrain Loss: 1.482 | Train PPL:   4.402\n",
      "\t Val. Loss: 6.697 |  Val. PPL: 809.713\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCannot execute code, session has been disposed. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# for sents_, synts_, trgs_ in valid_dataloader:\n",
    "#     break\n",
    "\n",
    "# sents_ = sents_[0:4]\n",
    "# synts_ = synts_[0:4]\n",
    "# trgs_  = trgs_[0:4]\n",
    "\n",
    "# outputs = model(sents_, synts_, trgs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCannot execute code, session has been disposed. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, criterion, loader_length, vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/target_sents.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/target_synts.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/outputs.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_ in loader:\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    fp1.write(targ+'\\n')\n",
    "                    fp2.write(synt_+'\\n')\n",
    "                    fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCannot execute code, session has been disposed. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "generate(model, valid_dataloader, criterion, val_loader_length, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCannot execute code, session has been disposed. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCannot execute code, session has been disposed. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs.txt') as fp:\n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")\n",
    "\n",
    "scores = [cal_bleu(pred, targ, 0) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
