{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, device, word_dropout = 0.4, dropout = 0.1):\n",
    "        super(seq2seqTransformer, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hid_dim = hid_dim \n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = np.sqrt(self.hid_dim)\n",
    "        # self.scale = torch.sqrt(torch.IntTensor([self.hid_dim])).to(device)\n",
    "\n",
    "        # vcocabulary embedding\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, hid_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, hid_dim)\n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(hid_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = hid_dim, nhead = 8, dropout = dropout)\n",
    "        # linear Transformation\n",
    "        self.linear = nn.Linear(hid_dim, input_dim)\n",
    "        # self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 # <sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        #trgs   : batch_size, seq_len \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2    # count without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1) - 2      # count without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, -1e10)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        #sent_emb = [seq_len, batch size, emb_size]\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings) \n",
    "        #synt_emb = [seq_len, batch size, emb_size]\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        #en_emb = [seq_len, batch size, emb_size*2]\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # target => embedding\n",
    "        de_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = self.transformer(en_embeddings, de_embeddings, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "        \n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.hid_dim))\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim)\n",
    "        #output = [batch size, trg_len, vocab_size]\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings)\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        de_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(en_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            if i % 5 == 0:\n",
    "                print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(de_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.hid_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            de_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            de_embeddings = self.pos_encoder(de_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\model.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_dim \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m#len(vocab)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m hid_dim \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m seq2seqTransformer(input_dim\u001b[39m=\u001b[39;49minput_dim, hid_dim\u001b[39m=\u001b[39;49mhid_dim, device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\model.ipynb Cell 5\u001b[0m in \u001b[0;36mseq2seqTransformer.__init__\u001b[1;34m(self, input_dim, hid_dim, device, word_dropout, dropout)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# positional encoding\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder \u001b[39m=\u001b[39m PositionalEncoding(hid_dim, dropout \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mTransformer(d_model \u001b[39m=\u001b[39;49m hid_dim, nhead \u001b[39m=\u001b[39;49m \u001b[39m12\u001b[39;49m, dropout \u001b[39m=\u001b[39;49m dropout)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# linear Transformation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hid_dim, input_dim)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:63\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation, custom_encoder, custom_decoder, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m custom_encoder\n\u001b[0;32m     62\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     encoder_layer \u001b[39m=\u001b[39m TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n\u001b[0;32m     64\u001b[0m                                             activation, layer_norm_eps, batch_first, norm_first,\n\u001b[0;32m     65\u001b[0m                                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n\u001b[0;32m     66\u001b[0m     encoder_norm \u001b[39m=\u001b[39m LayerNorm(d_model, eps\u001b[39m=\u001b[39mlayer_norm_eps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n\u001b[0;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:404\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m    402\u001b[0m factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[0;32m    403\u001b[0m \u001b[39msuper\u001b[39m(TransformerEncoderLayer, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 404\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m MultiheadAttention(d_model, nhead, dropout\u001b[39m=\u001b[39mdropout, batch_first\u001b[39m=\u001b[39mbatch_first,\n\u001b[0;32m    405\u001b[0m                                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n\u001b[0;32m    406\u001b[0m \u001b[39m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1 \u001b[39m=\u001b[39m Linear(d_model, dim_feedforward, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\activation.py:960\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39m=\u001b[39m batch_first\n\u001b[0;32m    959\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m=\u001b[39m embed_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_heads\n\u001b[1;32m--> 960\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m*\u001b[39m num_heads \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39m\"\u001b[39m\u001b[39membed_dim must be divisible by num_heads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m    963\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((embed_dim, embed_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "input_dim = 1000 #len(vocab)\n",
    "hid_dim = 256\n",
    "model = seq2seqTransformer(input_dim=input_dim, hid_dim=hid_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5\n",
      "epoch : 10\n",
      "epoch : 15\n",
      "epoch : 20\n",
      "epoch : 25\n",
      "epoch : 30\n"
     ]
    }
   ],
   "source": [
    "sents   = torch.LongTensor(16, 30).random_(0, 10) #sents: [batch size, seq_len]\n",
    "synts   = torch.LongTensor(16, 30).random_(0, 10) #synts: [batch size, seq_len]\n",
    "output  = model.generate(sents, synts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 31])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
