{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, device, word_dropout = 0.4, dropout = 0.1):\n",
    "        super(seq2seqTransformer, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hid_dim = hid_dim \n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        self.scale = torch.sqrt(torch.LongTensor([self.hid_dim])).to(device)\n",
    "        # self.scale = np.sqrt(self.hid_dim)\n",
    "\n",
    "        # vcocabulary embedding\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, hid_dim)\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, hid_dim)\n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(hid_dim, dropout = 0.0)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = hid_dim, nhead = 8, dropout = dropout)\n",
    "        # linear Transformation\n",
    "        self.linear = nn.Linear(hid_dim, input_dim)\n",
    "        # self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 # <sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        #trgs   : batch_size, seq_len \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2    # count without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1) - 2      # count without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, -1e10)\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        #sent_emb = [batch size, seq_len, emb_size]\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings) \n",
    "        #synt_emb = [batch size, seq_len, emb_size]\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        #en_emb = [batch size, seq_len, emb_size*2]\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # target => embedding\n",
    "        de_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = self.transformer(en_embeddings, de_embeddings, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "        \n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.hid_dim))\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim)\n",
    "        #output = [batch size, trg_len, vocab_size]\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, sents, synts, max_len = 100, sample=True, temp=0.5):\n",
    "            #sents  : batch_size, seq_len\n",
    "            #synts  : batch_size, seq_len\n",
    "            batch_size   = sents.size(0)\n",
    "            max_sent_len = sents.size(1)\n",
    "            max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "            max_targ_len = max_len\n",
    "            \n",
    "            # output index starts with <sos>\n",
    "            idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "            idxs[:, 0] = 1\n",
    "            \n",
    "            # sentence, syntax => embedding\n",
    "            sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "            synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "            synt_embeddings = self.pos_encoder(synt_embeddings)\n",
    "            en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "            \n",
    "            # do not allow cross attetion\n",
    "            src_mask = self.generate_square_mask(max_sent_len, max_synt_len).cuda()\n",
    "            \n",
    "            # starting index => embedding\n",
    "            de_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "            de_embeddings = self.pos_encoder(de_embeddings)\n",
    "            \n",
    "            # sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "            \n",
    "            # encode\n",
    "            memory = self.transformer.encoder(en_embeddings, mask=src_mask)\n",
    "            \n",
    "            # auto-regressively generate output\n",
    "            for i in range(1, max_targ_len+2):\n",
    "                # decode\n",
    "                outputs = self.transformer.decoder(de_embeddings, memory, tgt_mask=trg_mask)\n",
    "                outputs = self.linear(outputs[-1].contiguous().view(-1, self.hid_dim))\n",
    "                \n",
    "                # get argmax index or sample index\n",
    "                if not sample:\n",
    "                    values, idx = torch.max(outputs, 1)\n",
    "                else:\n",
    "                    probs = F.softmax(outputs/temp, dim=1)\n",
    "                    idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "                \n",
    "                # save to output index\n",
    "                idxs[:, i] = idx\n",
    "                \n",
    "                # concatenate index to decoding\n",
    "                de_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "                de_embeddings = self.pos_encoder(de_embeddings)\n",
    "                \n",
    "                # new sequential mask\n",
    "                trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "            \n",
    "            return idxs[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 300 #len(vocab)\n",
    "hid_dim = 256\n",
    "model = seq2seqTransformer(input_dim=input_dim, hid_dim=hid_dim, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\model.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sents   \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m300\u001b[39m) \u001b[39m#sents: [batch size, seq_len, emb_size]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m synts   \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m300\u001b[39m) \u001b[39m#synts: [batch size, seq_len, emb_size]\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mgenerate(sents, synts)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\model.ipynb Cell 6\u001b[0m in \u001b[0;36mseq2seqTransformer.generate\u001b[1;34m(self, sents, synts, max_len, sample, temp)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m idxs[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# sentence, syntax => embedding\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m sent_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_encoder(sents)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m synt_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_encoder(synts)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/model.ipynb#X36sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m synt_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(synt_embeddings)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "sents   = torch.randn(5, 1, 300) #sents: [batch size, seq_len, emb_size]\n",
    "synts   = torch.randn(5, 1, 300) #synts: [batch size, seq_len, emb_size]\n",
    "model.generate(sents, synts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
