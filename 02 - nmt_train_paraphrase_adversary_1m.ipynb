{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generator with Adversarial Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "with open(\"./data/nmt_bow_1m.pkl\", \"rb\") as file:\n",
    "    nmt_dataset = pickle.load(file)\n",
    "\n",
    "# with open(\"./Datasets/qq_dataset.pkl\", \"rb\") as file:\n",
    "#     qq_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list, adv_list = [], [], [], []\n",
    "    # print(len(batch))\n",
    "    # sens_, syns_, trgs_ = batch\n",
    "    # for sen_, syn_, trg_ in zip(sens_, syns_, trgs_):\n",
    "    for sen_, syn_, trg_, adv_ in batch:\n",
    "        processed_sent = torch.tensor(sen_, dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(syn_, dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(trg_, dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "        adv_ = torch.tensor(adv_, dtype=torch.int64)\n",
    "        adv_list.append(adv_)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True), pad_sequence(adv_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "random.seed(6969)\n",
    "random.shuffle(nmt_dataset)\n",
    "\n",
    "train_range = int(len(nmt_dataset) * 0.7)\n",
    "\n",
    "train_set = nmt_dataset[:train_range]\n",
    "val_set   = nmt_dataset[train_range:]\n",
    "# test_set = train_data[90:]\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(val_set, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (sen, syn, trg, adv) in enumerate(train_dataloader):\n",
    "#     print(sen.shape)\n",
    "#     print(syn.shape)\n",
    "#     print(trg.shape)\n",
    "#     print(adv.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 6969\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$\n",
    "PE_{pos,2i}     =   sin(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\\\\\n",
    "PE_{pos,2i+1}   =   cos(\\frac{pos}{10000^\\frac{2i}{d_{model}}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-np.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + pos encoding\n",
    "        token_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.sent_layernorm_embedding = nn.LayerNorm(emb_dim, elementwise_affine=False)\n",
    "        self.adv = nn.Linear(emb_dim, 74)\n",
    "        \n",
    "    def forward(self, sent_embeddings):\n",
    "        x = self.sent_layernorm_embedding(sent_embeddings).squeeze(1)\n",
    "        # print('Layer Norm',x.shape)\n",
    "        x = self.adv(x)\n",
    "        # print('Linear Layer',x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Adversary(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, device, word_dropout = 0.4, dropout = 0.1, max_len = 140):\n",
    "        super(Transformer_Adversary, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "        self.word_dropout = word_dropout\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = np.sqrt(self.emb_dim)\n",
    "        # self.scale = torch.sqrt(torch.IntTensor([self.hid_dim])).to(device)\n",
    "        # synt\n",
    "        # vocabulary embedding\n",
    "        self.embedding_encoder = nn.Embedding(input_dim, emb_dim) # token embedding\n",
    "        self.embedding_decoder = nn.Embedding(input_dim, emb_dim)\n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout = 0.0)\n",
    "        # positional embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, emb_dim) # position embedding\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = emb_dim, nhead = 12, dropout = dropout)\n",
    "\n",
    "        # linear Transformation\n",
    "        self.linear = nn.Linear(emb_dim, input_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.norm = nn.LayerNorm(emb_dim) \n",
    "        \n",
    "        self.adversary = Discriminator(emb_dim)\n",
    "\n",
    "    def load_embedding(self, embedding):  #synPG applied with GloVe glove.840B.300d.txt\n",
    "        self.embedding_encoder.weight.data.copy_(torch.from_numpy(embedding)) \n",
    "        self.embedding_decoder.weight.data.copy_(torch.from_numpy(embedding))  \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # initialize cocabulary matrix weight\n",
    "        self.embedding_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.embedding_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # initialize linear weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def generate_square_mask(self, max_sent_len, max_synt_len):\n",
    "        size = max_sent_len + max_synt_len + 2 #<sos> and <eos>\n",
    "        mask = torch.zeros((size, size))\n",
    "        mask[:max_sent_len, max_sent_len:] = float(\"-inf\")\n",
    "        mask[max_sent_len:, :max_sent_len] = float(\"-inf\")\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, sents, synts, trg):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        #trgs   : batch_size, seq_len \n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2    # count without <sos> and <eos>\n",
    "        max_targ_len = trg.size(1) - 2      # count without <sos> and <eos>\n",
    "\n",
    "        # apply word dropout\n",
    "        drop_mask = torch.bernoulli(self.word_dropout * torch.ones(max_sent_len)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        #sent_emb = [seq_len, batch size, emb_size]\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings) \n",
    "        #synt_emb = [seq_len, batch size, emb_size]\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        #en_emb = [seq_len, batch size, emb_size*2]\n",
    "\n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # target => embedding\n",
    "        de_embeddings = self.embedding_decoder(trg[:, :-1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(max_targ_len+1).to(self.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = self.transformer(en_embeddings, de_embeddings, src_mask=src_mask, tgt_mask=trg_mask)\n",
    "\n",
    "        # encoder_outputs = self.transformer.encoder(sent_embeddings)\n",
    "\n",
    "        # apply linear layer to vocabulary size\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        outputs = self.linear(outputs.contiguous().view(-1, self.emb_dim))\n",
    "        outputs = outputs.view(batch_size, max_targ_len + 1, self.input_dim)\n",
    "        #output = [batch size, trg_len, vocab_size]\n",
    "\n",
    "        # discriminator\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=False\n",
    "        adv_outputs = self.adversary(sent_embeddings)        \n",
    "\n",
    "        return outputs, adv_outputs\n",
    "    \n",
    "    def forward_token(self, sents):\n",
    "        #sent : batch_size, seq_len, emb_dim\n",
    "        drop_mask = torch.bernoulli(self.word_dropout*torch.ones(sents.shape)).bool().to(self.device)\n",
    "        sents = sents.masked_fill(drop_mask, 0)\n",
    "\n",
    "        batch_size, seq_len = sents.size(0), sents.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(sents)  # (len,) -> (bs, len)\n",
    "\n",
    "        emb_en = self.embedding_encoder(sents)\n",
    "        # print('emb_en',emb_en.shape)\n",
    "        emb_pos = self.pos_embed(pos)\n",
    "        # print('emb_pos',emb_pos.shape)\n",
    "        embedding = emb_en + emb_pos\n",
    "        # print('en_embeddings',embedding.shape)\n",
    "\n",
    "        # embedding = embedding.view(batch_size, seq_len * self.emb_dim)\n",
    "        # print('en_embeddings',embedding.shape)\n",
    "        x = self.norm(embedding)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        # x = x.transpose(0, 1)\n",
    "        for encoder_layer in self.transformer.encoder.layers:\n",
    "            x = encoder_layer(x)\n",
    "            \n",
    "        # x = x.transpose(0, 1)\n",
    "        return x\n",
    "\n",
    "    def forward_adv(self, sents):\n",
    "        for p in self.adversary.parameters():\n",
    "            p.required_grad=True\n",
    "        \n",
    "        # sent_embeddings = self.embedding_encoder(sents).transpose(0, 1).detach()\n",
    "        sent_embeddings = self.forward_token(sents).detach()\n",
    "        # print('Sentence Embedding', sent_embeddings.shape)\n",
    "\n",
    "        adv_outputs = self.adversary(sent_embeddings)\n",
    "        # print('Adversary', adv_outputs.shape)\n",
    "        return adv_outputs\n",
    "\n",
    "    def generate(self, sents, synts, max_len = 30, sample=True, temp=0.5):\n",
    "        #sents  : batch_size, seq_len\n",
    "        #synts  : batch_size, seq_len\n",
    "        batch_size   = sents.size(0)\n",
    "        max_sent_len = sents.size(1)\n",
    "        max_synt_len = synts.size(1) - 2  # count without <sos> and <eos>\n",
    "        max_targ_len = max_len\n",
    "        \n",
    "        # output index starts with <sos>\n",
    "        idxs = torch.zeros((batch_size, max_targ_len+2), dtype=torch.long).to(self.device)\n",
    "        idxs[:, 0] = 1\n",
    "        \n",
    "        # sentence, syntax => embedding\n",
    "        sent_embeddings = self.embedding_encoder(sents).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.embedding_encoder(synts).transpose(0, 1) * self.scale\n",
    "        synt_embeddings = self.pos_encoder(synt_embeddings)\n",
    "        en_embeddings = torch.cat((sent_embeddings, synt_embeddings), dim=0)\n",
    "        \n",
    "        # do not allow cross attetion\n",
    "        src_mask = self.generate_square_mask(max_sent_len, max_synt_len).to(self.device)\n",
    "        \n",
    "        # starting index => embedding\n",
    "        de_embeddings = self.embedding_decoder(idxs[:, :1]).transpose(0, 1) * self.scale\n",
    "        de_embeddings = self.pos_encoder(de_embeddings)\n",
    "        \n",
    "        # sequential mask\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        # encode\n",
    "        memory = self.transformer.encoder(en_embeddings, mask=src_mask)\n",
    "        \n",
    "        # auto-regressively generate output\n",
    "        for i in range(1, max_targ_len+2):\n",
    "            # if i % 5 == 0:\n",
    "            #     print(f'epoch : {i}')\n",
    "            # decode\n",
    "            outputs = self.transformer.decoder(de_embeddings, memory, tgt_mask=trg_mask)\n",
    "            outputs = self.linear(outputs[-1].contiguous().view(-1, self.emb_dim))\n",
    "            \n",
    "            # get argmax index or sample index\n",
    "            if not sample:\n",
    "                values, idx = torch.max(outputs, 1)\n",
    "            else:\n",
    "                probs = F.softmax(outputs/temp, dim=1)\n",
    "                idx = torch.multinomial(probs, 1).squeeze(1)\n",
    "            \n",
    "            # save to output index\n",
    "            idxs[:, i] = idx\n",
    "            \n",
    "            # concatenate index to decoding\n",
    "            de_embeddings = self.embedding_decoder(idxs[:, :i+1]).transpose(0, 1) * self.scale\n",
    "            de_embeddings = self.pos_encoder(de_embeddings)\n",
    "            \n",
    "            # new sequential mask\n",
    "            trg_mask = self.transformer.generate_square_subsequent_mask(de_embeddings.size(0)).to(self.device)\n",
    "        \n",
    "        return idxs[:, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import FastText\n",
    "# fast_vectors = FastText(language='simple') ##Load fasttext with language=simple\n",
    "# fast_embedding = fast_vectors.get_vecs_by_tokens(vocab_transform.get_itos()).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "# glove_file = datapath('glove.6B.300d.txt')\n",
    "glove_file = './Datasets/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dictionary.pkl\", \"rb\") as file:\n",
    "    dictionary = pickle.load(file)\n",
    "\n",
    "vocab_dict = dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "load 22696 of 31414 from pretrained word embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from utils import load_embedding\n",
    "\n",
    "input_dim = len(vocab_dict)\n",
    "emb_dim = 300  #fasttext\n",
    "word_dropout = 0.4 #following SynPG\n",
    "dropout     = 0.1\n",
    "\n",
    "embedding = load_embedding(glove_file, dictionary)\n",
    "\n",
    "model = Transformer_Adversary(input_dim=input_dim, emb_dim = emb_dim, device=device, word_dropout = word_dropout, dropout = dropout)\n",
    "model = model.to(device)\n",
    "model.load_embedding(embedding)\n",
    "# model.embedding_encoder.weight.data = fast_embedding #apply fasttext instead of Glove 840b 300d.txt (5.56 GB) TT\n",
    "# model.embedding_decoder.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_model\n",
    "# sents   = torch.LongTensor(16, 30).random_(0, 10) #sents: [batch size, seq_len]\n",
    "# synts   = torch.LongTensor(16, 30).random_(0, 10) #synts: [batch size, seq_len]\n",
    "# output  = model.generate(sents, synts)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps = 1):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for idx, (sents_, synts_, trgs_, adv_targs) in tqdm(enumerate(loader)):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        adv_total_loss = 0.0\t   \n",
    "\n",
    "        batch_size   = sents_.size(0)\n",
    "        max_sent_len = sents_.size(1)\n",
    "        max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "        adv_len = 74\n",
    "        # Put input into device\n",
    "        sents_ = sents_.to(device)\n",
    "        synts_ = synts_.to(device)\n",
    "        trgs_ = trgs_.to(device)\n",
    "        adv_targs = adv_targs.to(device)\n",
    "        # print(adv_targs.shape)\n",
    "\n",
    "        #optimize adv\n",
    "        outputs = model.forward_adv(sents_)\n",
    "        # output = [batch size, trg_len, output dim]\n",
    "\n",
    "        adv_targs = adv_targs.unsqueeze(1).expand(batch_size, outputs.shape[1], 74).to(torch.float)\n",
    "        # output = [batch size, trg_len, output dim]\n",
    "        # print(adv_targs.dtype)\n",
    "        loss = adv_criterion(outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        adv_total_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            if epoch > 1:\n",
    "                adv_optimizer.step()\n",
    "        adv_optimizer.zero_grad()\n",
    "\n",
    "        #optimize model\n",
    "        outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "        # calculate loss\n",
    "        targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "        outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = para_criterion(outputs_, targs_)\n",
    "\n",
    "        adv_outputs = adv_outputs.transpose(0,1) #batch_size, seq_len\n",
    "\n",
    "        if epoch > 1:\n",
    "            loss -= 0.1 * adv_criterion(adv_outputs, adv_targs)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            para_optimizer.step()\n",
    "            para_optimizer.zero_grad()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, para_criterion, adv_criterion, loader_length):\n",
    "\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    para_loss = 0\n",
    "    adv_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sents_, synts_, trgs_, adv_targs in tqdm(loader):\n",
    "\n",
    "            batch_size   = sents_.size(0)\n",
    "            max_sent_len = sents_.size(1)\n",
    "            max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "            \n",
    "            # Put input into device\n",
    "            sents_ = sents_.to(device)\n",
    "            synts_ = synts_.to(device)\n",
    "            trgs_ = trgs_.to(device)\n",
    "            adv_targs = adv_targs.to(device)\n",
    "            \n",
    "            #forward \n",
    "            outputs, adv_outputs = model(sents_, synts_, trgs_)\n",
    "            # print('adv output',adv_outputs.shape)\n",
    "            # print('adv trg',adv_targs.shape)\n",
    "            \n",
    "            targs_ = trgs_[:, 1:].contiguous().view(-1) #Without <SOS>\n",
    "            outputs_ = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "\n",
    "            adv_outputs = adv_outputs.transpose(0,1)\n",
    "            adv_targs = adv_targs.unsqueeze(1).expand(batch_size, adv_outputs.shape[1], 74).to(torch.float)\n",
    "            \n",
    "            # print('adv trg',adv_targs.shape)\n",
    "            # print('adv output',adv_outputs.shape)\n",
    "            # adv output torch.Size([27, 30, 74])\n",
    "            # adv trg torch.Size([30, 74])\n",
    "\n",
    "            para_loss += para_criterion(outputs_, targs_) \n",
    "            adv_loss += adv_criterion(adv_outputs, adv_targs)\n",
    "            \n",
    "        \n",
    "    return para_loss / loader_length, adv_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pad_idx = dictionary.word2idx['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "lr = 10e-4 #Following SynPG\n",
    "wd = 10e-5 #Following SynPG\n",
    "#training hyperparameters\n",
    "para_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "para_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)\n",
    "adv_criterion = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_dataloader)))\n",
    "val_loader_length   = len(list(iter(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [01:40,  3.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\02 - nmt_train_paraphrase_adversary_1m.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# training \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m para_loss, adv_loss \u001b[39m=\u001b[39m evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m valid_loss \u001b[39m=\u001b[39m para_loss \u001b[39m-\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m adv_loss\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\02 - nmt_train_paraphrase_adversary_1m.ipynb Cell 29\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, loader_length, accumulation_steps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     loss \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m adv_criterion(adv_outputs, adv_targs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/02%20-%20nmt_train_paraphrase_adversary_1m.ipynb#X40sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m (idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/adversary_nmt_1m.pt' #Change here\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, train_dataloader, epoch, para_optimizer, adv_optimizer, para_criterion, adv_criterion, clip, train_loader_length)\n",
    "    para_loss, adv_loss = evaluate(model, valid_dataloader, para_criterion, adv_criterion, val_loader_length)\n",
    "\n",
    "    valid_loss = para_loss - 0.1 * adv_loss\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sents_, synts_, trgs_ in valid_dataloader:\n",
    "#     break\n",
    "\n",
    "# sents_ = sents_[0:4]\n",
    "# synts_ = synts_[0:4]\n",
    "# trgs_  = trgs_[0:4]\n",
    "\n",
    "# outputs = model(sents_, synts_, trgs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import synt2str, sent2str, load_embedding, reverse_bpe\n",
    "    \n",
    "def generate(model, loader, loader_length, vocab_transform):\n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with open(\"./eval/target_sents_adv.txt\", \"w\") as fp1, \\\n",
    "         open(\"./eval/target_synts_adv.txt\", \"w\") as fp2, \\\n",
    "         open(\"./eval/outputs_adv.txt\", \"w\") as fp3:\n",
    "        with torch.no_grad():\n",
    "            for sents_, synts_, trgs_, trg_adv_ in tqdm(loader):\n",
    "\n",
    "                batch_size   = sents_.size(0)\n",
    "                max_sent_len = sents_.size(1)\n",
    "                max_synt_len = synts_.size(1) - 2  # count without <sos> and <eos>\n",
    "                \n",
    "                # generate\n",
    "                idxs = model.generate(sents_, synts_, sents_.size(1), temp=0.5)\n",
    "                \n",
    "                # write output\n",
    "                for sent, idx, targ, synt_ in zip(sents_, idxs.cpu().numpy(), trgs_, synts_):\n",
    "                    # fp1.write(targ+'\\n')\n",
    "                    # fp2.write(synt_+'\\n')\n",
    "                    # fp3.write(reverse_bpe(synt2str(idx, vocab_transform))+'\\n')\n",
    "                    \n",
    "                    convert_sent = reverse_bpe(sent2str(sent.tolist(), vocab_transform).split()) + '\\n'\n",
    "                    convert_synt = synt2str(synt_[1:].tolist(), vocab_transform).replace(\"<pad>\", \"\") + '\\n' \n",
    "                    convert_idx = synt2str(idx, vocab_transform) +'\\n'\n",
    "                    \n",
    "                    fp1.write(convert_sent)\n",
    "                    fp2.write(convert_synt)\n",
    "                    fp3.write(convert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:19<00:00, 13.96s/it]\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/adversary_nmt.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "generate(model, valid_dataloader, val_loader_length, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def cal_bleu(hypothesis, reference, n):\n",
    "    hypothesis = hypothesis.strip().split(' ')\n",
    "    reference = reference.strip().split(' ')\n",
    "\n",
    "    if n == 0:\n",
    "        return sentence_bleu([reference], hypothesis)\n",
    "    elif n == 1:\n",
    "        weights = (1, 0, 0, 0)\n",
    "    elif n == 2:\n",
    "        weights = (0, 1, 0, 0)\n",
    "    elif n == 3:\n",
    "        weights = (0, 0, 1, 0)\n",
    "    elif n == 4:\n",
    "        weights = (0, 0, 0, 1)\n",
    "\n",
    "    return sentence_bleu([reference], hypothesis, weights=weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python3.10.4\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python3.10.4\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 2.1898707901576775\n"
     ]
    }
   ],
   "source": [
    "with open('./eval/target_sents.txt') as fp:\n",
    "    targs = fp.readlines()\n",
    "with open('./eval/outputs.txt') as fp:\n",
    "    preds = fp.readlines()\n",
    "\n",
    "print(f\"number of examples: {len(preds)}\")\n",
    "\n",
    "scores = [cal_bleu(pred, targ, 0) for pred, targ in zip(preds, targs)]\n",
    "\n",
    "print(f\"BLEU: {np.mean(scores)*100.0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
