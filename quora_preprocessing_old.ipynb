{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345796, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "qq = pd.read_csv('./Datasets/quora_question.csv')\n",
    "qq.drop(columns=['test_id','question2'], inplace=True)\n",
    "qq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample = qq.sample(n=1000,random_state=6969) #try only 1000 samples\n",
    "random_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614123</th>\n",
       "      <td>Why won't China let Pope Francis visit?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795359</th>\n",
       "      <td>Is it common to say \"you are welcome\" in when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209942</th>\n",
       "      <td>Do G+ \"plus ones\" on posts actually do anythin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383030</th>\n",
       "      <td>Can llp give loan to its partners?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529755</th>\n",
       "      <td>How many medals become won in Olympics ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question1\n",
       "614123             Why won't China let Pope Francis visit?\n",
       "795359   Is it common to say \"you are welcome\" in when ...\n",
       "2209942  Do G+ \"plus ones\" on posts actually do anythin...\n",
       "1383030                 Can llp give loan to its partners?\n",
       "529755            How many medals become won in Olympics ?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq1000 = random_sample['question1'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# benepar.download('benepar_en3')\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "def constituency_parser(text):\n",
    "    doc = nlp(text)\n",
    "    sent = list(doc.sents)[0]\n",
    "    return  \"(ROOT \"+sent._.parse_string+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guntsv\\AppData\\Local\\Temp\\ipykernel_24524\\3282338791.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx in tqdm_notebook(range(len(qq1000))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd90e0da78a246ffb962565c2e8b1df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Python3.10.4\\lib\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "train_data = list()\n",
    "for idx in tqdm_notebook(range(len(qq1000))):\n",
    "    train_data.append([qq1000[idx],constituency_parser(qq1000[idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>parser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why won't China let Pope Francis visit?</td>\n",
       "      <td>(ROOT (SBARQ (WHADVP (WRB Why)) (SQ (MD wo) (R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is it common to say \"you are welcome\" in when ...</td>\n",
       "      <td>(ROOT (SQ (VBZ Is) (NP (NP (PRP it))) (ADJP (J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do G+ \"plus ones\" on posts actually do anythin...</td>\n",
       "      <td>(ROOT (SQ (VBP Do) (NP (NP (`` G+) (`` \") (CC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can llp give loan to its partners?</td>\n",
       "      <td>(ROOT (SQ (MD Can) (NP (NN llp)) (VP (VB give)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many medals become won in Olympics ?</td>\n",
       "      <td>(ROOT (SBARQ (WHNP (WHADJP (WRB How) (JJ many)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0            Why won't China let Pope Francis visit?   \n",
       "1  Is it common to say \"you are welcome\" in when ...   \n",
       "2  Do G+ \"plus ones\" on posts actually do anythin...   \n",
       "3                 Can llp give loan to its partners?   \n",
       "4           How many medals become won in Olympics ?   \n",
       "\n",
       "                                              parser  \n",
       "0  (ROOT (SBARQ (WHADVP (WRB Why)) (SQ (MD wo) (R...  \n",
       "1  (ROOT (SQ (VBZ Is) (NP (NP (PRP it))) (ADJP (J...  \n",
       "2  (ROOT (SQ (VBP Do) (NP (NP (`` G+) (`` \") (CC ...  \n",
       "3  (ROOT (SQ (MD Can) (NP (NN llp)) (VP (VB give)...  \n",
       "4  (ROOT (SBARQ (WHNP (WHADJP (WRB How) (JJ many)...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "train_data = pd.DataFrame(train_data)\n",
    "train_data.rename(columns={0:'sentence',1:'parser'},inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(text):\n",
    "    return  \"<SOS> \"+ text + \" <EOS>\"\n",
    "\n",
    "train_data['target'] = train_data['sentence'].apply(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>parser</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why won't China let Pope Francis visit?</td>\n",
       "      <td>(ROOT (SBARQ (WHADVP (WRB Why)) (SQ (MD wo) (R...</td>\n",
       "      <td>&lt;SOS&gt; Why won't China let Pope Francis visit? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is it common to say \"you are welcome\" in when ...</td>\n",
       "      <td>(ROOT (SQ (VBZ Is) (NP (NP (PRP it))) (ADJP (J...</td>\n",
       "      <td>&lt;SOS&gt; Is it common to say \"you are welcome\" in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do G+ \"plus ones\" on posts actually do anythin...</td>\n",
       "      <td>(ROOT (SQ (VBP Do) (NP (NP (`` G+) (`` \") (CC ...</td>\n",
       "      <td>&lt;SOS&gt; Do G+ \"plus ones\" on posts actually do a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can llp give loan to its partners?</td>\n",
       "      <td>(ROOT (SQ (MD Can) (NP (NN llp)) (VP (VB give)...</td>\n",
       "      <td>&lt;SOS&gt; Can llp give loan to its partners? &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many medals become won in Olympics ?</td>\n",
       "      <td>(ROOT (SBARQ (WHNP (WHADJP (WRB How) (JJ many)...</td>\n",
       "      <td>&lt;SOS&gt; How many medals become won in Olympics ?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0            Why won't China let Pope Francis visit?   \n",
       "1  Is it common to say \"you are welcome\" in when ...   \n",
       "2  Do G+ \"plus ones\" on posts actually do anythin...   \n",
       "3                 Can llp give loan to its partners?   \n",
       "4           How many medals become won in Olympics ?   \n",
       "\n",
       "                                              parser  \\\n",
       "0  (ROOT (SBARQ (WHADVP (WRB Why)) (SQ (MD wo) (R...   \n",
       "1  (ROOT (SQ (VBZ Is) (NP (NP (PRP it))) (ADJP (J...   \n",
       "2  (ROOT (SQ (VBP Do) (NP (NP (`` G+) (`` \") (CC ...   \n",
       "3  (ROOT (SQ (MD Can) (NP (NN llp)) (VP (VB give)...   \n",
       "4  (ROOT (SBARQ (WHNP (WHADJP (WRB How) (JJ many)...   \n",
       "\n",
       "                                              target  \n",
       "0  <SOS> Why won't China let Pope Francis visit? ...  \n",
       "1  <SOS> Is it common to say \"you are welcome\" in...  \n",
       "2  <SOS> Do G+ \"plus ones\" on posts actually do a...  \n",
       "3     <SOS> Can llp give loan to its partners? <EOS>  \n",
       "4  <SOS> How many medals become won in Olympics ?...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proprocessed Data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\"\n",
    "\n",
    "def deleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                arr[n + 1] = \"\"\n",
    "\n",
    "    nonleaves = \" \".join(arr)\n",
    "    return nonleaves.split()\n",
    "\n",
    "from nltk import ParentedTree\n",
    "\n",
    "def Parsertokenize(synt_):\n",
    "    synt_ = ParentedTree.fromstring(synt_)\n",
    "    synt_ = deleaf(synt_)\n",
    "    synt_ = [f'<{w}>' for w in synt_]\n",
    "    return synt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subwordnmt.apply_bpe import BPE, read_vocabulary\n",
    "import codecs\n",
    "\n",
    "# load bpe codes\n",
    "bpe_codes = codecs.open('./data/bpe.codes', encoding='utf-8')\n",
    "bpe_vocab = codecs.open('./data/vocab.txt', encoding='utf-8')\n",
    "bpe_vocab = read_vocabulary(bpe_vocab, 50)\n",
    "bpe = BPE(bpe_codes, '@@', bpe_vocab, None)\n",
    "\n",
    "def bpetokenize(sent_):\n",
    " # bpe segment and convert to tensor\n",
    "    sent_ = bpe.segment(sent_).split()\n",
    "    return sent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = pd.DataFrame()\n",
    "train_load['sentence'] = train_data['sentence'].apply(bpetokenize)\n",
    "train_load['parser'] = train_data['parser'].apply(Parsertokenize)\n",
    "train_load['target'] = train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>parser</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[W@@, hy, won@@, 't, C@@, h@@, ina, let, P@@, ...</td>\n",
       "      <td>[&lt;(&gt;, &lt;ROOT&gt;, &lt;(&gt;, &lt;SBARQ&gt;, &lt;(&gt;, &lt;WHADVP&gt;, &lt;(&gt;...</td>\n",
       "      <td>&lt;SOS&gt; Why won't China let Pope Francis visit? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I@@, s, it, common, to, say, \"@@, you, are, w...</td>\n",
       "      <td>[&lt;(&gt;, &lt;ROOT&gt;, &lt;(&gt;, &lt;SQ&gt;, &lt;(&gt;, &lt;VBZ&gt;, &lt;)&gt;, &lt;(&gt;,...</td>\n",
       "      <td>&lt;SOS&gt; Is it common to say \"you are welcome\" in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[D@@, o, G@@, +, \"@@, plus, on@@, es@@, \", on,...</td>\n",
       "      <td>[&lt;(&gt;, &lt;ROOT&gt;, &lt;(&gt;, &lt;SQ&gt;, &lt;(&gt;, &lt;VBP&gt;, &lt;)&gt;, &lt;(&gt;,...</td>\n",
       "      <td>&lt;SOS&gt; Do G+ \"plus ones\" on posts actually do a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[C@@, an, ll@@, p, give, loan, to, its, partne...</td>\n",
       "      <td>[&lt;(&gt;, &lt;ROOT&gt;, &lt;(&gt;, &lt;SQ&gt;, &lt;(&gt;, &lt;MD&gt;, &lt;)&gt;, &lt;(&gt;, ...</td>\n",
       "      <td>&lt;SOS&gt; Can llp give loan to its partners? &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[H@@, ow, many, medals, become, won, in, O@@, ...</td>\n",
       "      <td>[&lt;(&gt;, &lt;ROOT&gt;, &lt;(&gt;, &lt;SBARQ&gt;, &lt;(&gt;, &lt;WHNP&gt;, &lt;(&gt;, ...</td>\n",
       "      <td>&lt;SOS&gt; How many medals become won in Olympics ?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [W@@, hy, won@@, 't, C@@, h@@, ina, let, P@@, ...   \n",
       "1  [I@@, s, it, common, to, say, \"@@, you, are, w...   \n",
       "2  [D@@, o, G@@, +, \"@@, plus, on@@, es@@, \", on,...   \n",
       "3  [C@@, an, ll@@, p, give, loan, to, its, partne...   \n",
       "4  [H@@, ow, many, medals, become, won, in, O@@, ...   \n",
       "\n",
       "                                              parser  \\\n",
       "0  [<(>, <ROOT>, <(>, <SBARQ>, <(>, <WHADVP>, <(>...   \n",
       "1  [<(>, <ROOT>, <(>, <SQ>, <(>, <VBZ>, <)>, <(>,...   \n",
       "2  [<(>, <ROOT>, <(>, <SQ>, <(>, <VBP>, <)>, <(>,...   \n",
       "3  [<(>, <ROOT>, <(>, <SQ>, <(>, <MD>, <)>, <(>, ...   \n",
       "4  [<(>, <ROOT>, <(>, <SBARQ>, <(>, <WHNP>, <(>, ...   \n",
       "\n",
       "                                              target  \n",
       "0  <SOS> Why won't China let Pope Francis visit? ...  \n",
       "1  <SOS> Is it common to say \"you are welcome\" in...  \n",
       "2  <SOS> Do G+ \"plus ones\" on posts actually do a...  \n",
       "3     <SOS> Can llp give loan to its partners? <EOS>  \n",
       "4  <SOS> How many medals become won in Olympics ?...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/dictionary.pkl', 'rb') as f:\n",
    "    vocab_transform = pickle.load(f)\n",
    "vocab_dict = vocab_transform.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy',language='en_core_web_sm')\n",
    "text_pipeline = lambda x: vocab_transform(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "pad_idx = vocab_dict['<pad>'] ##get the pad index from the vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sent_list, synt_lst, trg_list = [], [], []\n",
    "    for sen_, syn_, trg_ in batch:\n",
    "        processed_sent = torch.tensor(text_pipeline(sen_), dtype=torch.int64)\n",
    "        sent_list.append(processed_sent)\n",
    "        processed_synt = torch.tensor(text_pipeline(syn_), dtype=torch.int64)\n",
    "        synt_lst.append(processed_synt)\n",
    "        processed_trg = torch.tensor(text_pipeline(trg_), dtype=torch.int64)\n",
    "        trg_list.append(processed_trg)\n",
    "\n",
    "    return pad_sequence(sent_list, padding_value=pad_idx, batch_first=True), pad_sequence(synt_lst, padding_value=pad_idx, batch_first=True), pad_sequence(trg_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# tokenizer = get_tokenizer('spacy',language='en_core_web_sm')\n",
    "\n",
    "# def yield_tokens(data_iter):\n",
    "#     for text, _ in data_iter.itertuples(index=False):\n",
    "#         yield tokenizer(text)\n",
    "#     #loop through the data_iter, \n",
    "#     # Mind that the data_iter in this case is pandas Dataframe\n",
    "#     # pass #remove this line and code here\n",
    "\n",
    "# specials = ['<unk>','<pad>','<bos>','<eos>'] #create array of special tags for the vocab\n",
    "# vocab_transform  = build_vocab_from_iterator(yield_tokens(train_data), specials = specials, special_first=True)\n",
    "\n",
    "# #set_default_index of the vocab to unknown tag\n",
    "# vocab_transform.set_default_index(vocab_transform[\"<unk>\"]) #if you don't the id of this word, set it unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert len(vocab_transform) == 3407 #only for 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<bos>', '<eos>', '?', 'the', 'What', 'a', 'is', 'I']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_transform.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<bos>', '<eos>', '?', 'the', 'What', 'a', 'is', 'I']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pickle\n",
    "# # # with open('vocab_transform_cnn.pickle', 'wb') as f:\n",
    "# # #     pickle.dump(vocab_transform, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('vocab_transform_cnn.pickle', 'rb') as f:\n",
    "#     vocab_transform = pickle.load(f)\n",
    "# vocab_transform.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<sos>',\n",
       " 2: '<eos>',\n",
       " 3: '<unk>',\n",
       " 4: '<msk>',\n",
       " 5: '<PAD>',\n",
       " 6: '<ROOT>',\n",
       " 7: '<ADJP>',\n",
       " 8: '<-ADV>',\n",
       " 9: '<ADVP>',\n",
       " 10: '<-BNF>',\n",
       " 11: '<CC>',\n",
       " 12: '<CD>',\n",
       " 13: '<-CLF>',\n",
       " 14: '<-CLR>',\n",
       " 15: '<CONJP>',\n",
       " 16: '<-DIR>',\n",
       " 17: '<DT>',\n",
       " 18: '<-DTV>',\n",
       " 19: '<EX>',\n",
       " 20: '<-EXT>',\n",
       " 21: '<FRAG>',\n",
       " 22: '<FW>',\n",
       " 23: '<-HLN>',\n",
       " 24: '<IN>',\n",
       " 25: '<INTJ>',\n",
       " 26: '<JJ>',\n",
       " 27: '<JJR>',\n",
       " 28: '<JJS>',\n",
       " 29: '<-LGS>',\n",
       " 30: '<-LOC>',\n",
       " 31: '<LS>',\n",
       " 32: '<LST>',\n",
       " 33: '<MD>',\n",
       " 34: '<-MNR>',\n",
       " 35: '<NAC>',\n",
       " 36: '<NN>',\n",
       " 37: '<NNS>',\n",
       " 38: '<NNP>',\n",
       " 39: '<NNPS>',\n",
       " 40: '<-NOM>',\n",
       " 41: '<NP>',\n",
       " 42: '<NX>',\n",
       " 43: '<PDT>',\n",
       " 44: '<POS>',\n",
       " 45: '<PP>',\n",
       " 46: '<-PRD>',\n",
       " 47: '<PRN>',\n",
       " 48: '<PRP>',\n",
       " 49: '<-PRP>',\n",
       " 50: '<PRP$>',\n",
       " 51: '<PRP-S>',\n",
       " 52: '<PRT>',\n",
       " 53: '<-PUT>',\n",
       " 54: '<QP>',\n",
       " 55: '<RB>',\n",
       " 56: '<RBR>',\n",
       " 57: '<RBS>',\n",
       " 58: '<RP>',\n",
       " 59: '<RRC>',\n",
       " 60: '<S>',\n",
       " 61: '<SBAR>',\n",
       " 62: '<SBARQ>',\n",
       " 63: '<-SBJ>',\n",
       " 64: '<SINV>',\n",
       " 65: '<SQ>',\n",
       " 66: '<SYM>',\n",
       " 67: '<-TMP>',\n",
       " 68: '<TO>',\n",
       " 69: '<-TPC>',\n",
       " 70: '<-TTL>',\n",
       " 71: '<UCP>',\n",
       " 72: '<UH>',\n",
       " 73: '<VB>',\n",
       " 74: '<VBD>',\n",
       " 75: '<VBG>',\n",
       " 76: '<VBN>',\n",
       " 77: '<VBP>',\n",
       " 78: '<VBZ>',\n",
       " 79: '<-VOC>',\n",
       " 80: '<VP>',\n",
       " 81: '<WDT>',\n",
       " 82: '<WHADJP>',\n",
       " 83: '<WHADVP>',\n",
       " 84: '<WHNP>',\n",
       " 85: '<WHPP>',\n",
       " 86: '<WP>',\n",
       " 87: '<WP$>',\n",
       " 88: '<WP-S>',\n",
       " 89: '<WRB>',\n",
       " 90: '<X>',\n",
       " 91: '<#>',\n",
       " 92: '<``>',\n",
       " 93: \"<''>\",\n",
       " 94: '<.>',\n",
       " 95: '<,>',\n",
       " 96: '<:>',\n",
       " 97: '<$>',\n",
       " 98: '<-LRB->',\n",
       " 99: '<-RRB->',\n",
       " 100: '<-NONE->',\n",
       " 101: '<*>',\n",
       " 102: '<0>',\n",
       " 103: '<T>',\n",
       " 104: '<NUL>',\n",
       " 105: '<>>',\n",
       " 106: '<(>',\n",
       " 107: '<)>',\n",
       " 108: '<EOP>',\n",
       " 109: '.',\n",
       " 110: ',',\n",
       " 111: 'the',\n",
       " 112: 'to',\n",
       " 113: 'you',\n",
       " 114: 'i',\n",
       " 115: 'of',\n",
       " 116: 'a',\n",
       " 117: 'and',\n",
       " 118: 'in',\n",
       " 119: '?',\n",
       " 120: 'it',\n",
       " 121: \"'s\",\n",
       " 122: 'that',\n",
       " 123: \"n't\",\n",
       " 124: 'he',\n",
       " 125: 'is',\n",
       " 126: 'for',\n",
       " 127: 'do',\n",
       " 128: '...',\n",
       " 129: 'was',\n",
       " 130: 'be',\n",
       " 131: 'we',\n",
       " 132: 'on',\n",
       " 133: 'have',\n",
       " 134: 'this',\n",
       " 135: 'with',\n",
       " 136: 'not',\n",
       " 137: 'me',\n",
       " 138: '``',\n",
       " 139: \"''\",\n",
       " 140: '!',\n",
       " 141: 'what',\n",
       " 142: 'but',\n",
       " 143: 'my',\n",
       " 144: 'your',\n",
       " 145: 'they',\n",
       " 146: 'are',\n",
       " 147: 'as',\n",
       " 148: \"'m\",\n",
       " 149: 'at',\n",
       " 150: 'if',\n",
       " 151: \"'re\",\n",
       " 152: 'his',\n",
       " 153: 'no',\n",
       " 154: 'there',\n",
       " 155: 'all',\n",
       " 156: '-RRB-',\n",
       " 157: '-LRB-',\n",
       " 158: 'she',\n",
       " 159: 'so',\n",
       " 160: 'know',\n",
       " 161: 'out',\n",
       " 162: 'him',\n",
       " 163: 'from',\n",
       " 164: \"'ll\",\n",
       " 165: 'did',\n",
       " 166: 'can',\n",
       " 167: 'like',\n",
       " 168: 'her',\n",
       " 169: 'by',\n",
       " 170: 'an',\n",
       " 171: 'just',\n",
       " 172: 'one',\n",
       " 173: 'or',\n",
       " 174: '-',\n",
       " 175: 'about',\n",
       " 176: 'will',\n",
       " 177: 'up',\n",
       " 178: 'get',\n",
       " 179: 'had',\n",
       " 180: 'here',\n",
       " 181: 'would',\n",
       " 182: 'when',\n",
       " 183: 'were',\n",
       " 184: 'go',\n",
       " 185: ':',\n",
       " 186: 'want',\n",
       " 187: 'now',\n",
       " 188: 'right',\n",
       " 189: 'them',\n",
       " 190: 'how',\n",
       " 191: 'could',\n",
       " 192: 'who',\n",
       " 193: 'think',\n",
       " 194: 'said',\n",
       " 195: 'time',\n",
       " 196: 'see',\n",
       " 197: \"'\",\n",
       " 198: 'got',\n",
       " 199: \"'ve\",\n",
       " 200: 'us',\n",
       " 201: 'then',\n",
       " 202: 'come',\n",
       " 203: 'been',\n",
       " 204: 'well',\n",
       " 205: 'has',\n",
       " 206: 'back',\n",
       " 207: 'should',\n",
       " 208: 'which',\n",
       " 209: 'going',\n",
       " 210: 'more',\n",
       " 211: 'take',\n",
       " 212: 'only',\n",
       " 213: 'into',\n",
       " 214: '`',\n",
       " 215: 'their',\n",
       " 216: 'let',\n",
       " 217: 'our',\n",
       " 218: 'why',\n",
       " 219: 'where',\n",
       " 220: 'na',\n",
       " 221: 'some',\n",
       " 222: 'man',\n",
       " 223: 'good',\n",
       " 224: 'other',\n",
       " 225: 'look',\n",
       " 226: '/',\n",
       " 227: 'something',\n",
       " 228: \"'d\",\n",
       " 229: 'any',\n",
       " 230: 'does',\n",
       " 231: 'ca',\n",
       " 232: 'down',\n",
       " 233: 'even',\n",
       " 234: 'way',\n",
       " 235: 'tell',\n",
       " 236: 'gon',\n",
       " 237: 'oh',\n",
       " 238: 'two',\n",
       " 239: 'because',\n",
       " 240: 'need',\n",
       " 241: 'people',\n",
       " 242: 'over',\n",
       " 243: 'say',\n",
       " 244: 'shall',\n",
       " 245: 'than',\n",
       " 246: 'first',\n",
       " 247: 'little',\n",
       " 248: 'before',\n",
       " 249: 'really',\n",
       " 250: 'these',\n",
       " 251: 'make',\n",
       " 252: 'must',\n",
       " 253: 'too',\n",
       " 254: 'never',\n",
       " 255: 'yeah',\n",
       " 256: 'okay',\n",
       " 257: 'commission',\n",
       " 258: 'still',\n",
       " 259: '--',\n",
       " 260: 'after',\n",
       " 261: '1',\n",
       " 262: 'very',\n",
       " 263: 'may',\n",
       " 264: 'much',\n",
       " 265: 'its',\n",
       " 266: 'new',\n",
       " 267: 'off',\n",
       " 268: 'those',\n",
       " 269: 'give',\n",
       " 270: 'find',\n",
       " 271: ';',\n",
       " 272: 'article',\n",
       " 273: 'thought',\n",
       " 274: 'thing',\n",
       " 275: 'yes',\n",
       " 276: 'such',\n",
       " 277: 'also',\n",
       " 278: 'put',\n",
       " 279: 'last',\n",
       " 280: 'work',\n",
       " 281: 'mean',\n",
       " 282: 'anything',\n",
       " 283: 'life',\n",
       " 284: 'day',\n",
       " 285: 'years',\n",
       " 286: 'again',\n",
       " 287: 'sure',\n",
       " 288: 'long',\n",
       " 289: 'maybe',\n",
       " 290: 'help',\n",
       " 291: 'made',\n",
       " 292: 'told',\n",
       " 293: 'away',\n",
       " 294: 'hey',\n",
       " 295: 'call',\n",
       " 296: 'case',\n",
       " 297: 'around',\n",
       " 298: 'use',\n",
       " 299: 'same',\n",
       " 300: 'under',\n",
       " 301: 'place',\n",
       " 302: 'keep',\n",
       " 303: 'through',\n",
       " 304: 'member',\n",
       " 305: 'great',\n",
       " 306: 'came',\n",
       " 307: 'better',\n",
       " 308: 'found',\n",
       " 309: '%',\n",
       " 310: 'talk',\n",
       " 311: 'love',\n",
       " 312: 'nothing',\n",
       " 313: 'everything',\n",
       " 314: '2',\n",
       " 315: 'night',\n",
       " 316: 'doing',\n",
       " 317: 'someone',\n",
       " 318: 'another',\n",
       " 319: 'three',\n",
       " 320: 'mr.',\n",
       " 321: 'please',\n",
       " 322: 'things',\n",
       " 323: 'name',\n",
       " 324: 'wo',\n",
       " 325: 'god',\n",
       " 326: 'old',\n",
       " 327: 'sorry',\n",
       " 328: 'always',\n",
       " 329: 'own',\n",
       " 330: 'wanted',\n",
       " 331: 'world',\n",
       " 332: 'used',\n",
       " 333: 'without',\n",
       " 334: 'took',\n",
       " 335: 'home',\n",
       " 336: 'left',\n",
       " 337: 'ever',\n",
       " 338: 'house',\n",
       " 339: 'guy',\n",
       " 340: 'every',\n",
       " 341: 'states',\n",
       " 342: 'information',\n",
       " 343: 'european',\n",
       " 344: 'done',\n",
       " 345: 'most',\n",
       " 346: 'c',\n",
       " 347: 'each',\n",
       " 348: 'stop',\n",
       " 349: 'am',\n",
       " 350: 'between',\n",
       " 351: 'might',\n",
       " 352: 'head',\n",
       " 353: 'feel',\n",
       " 354: 'regulation',\n",
       " 355: 'leave',\n",
       " 356: 'being',\n",
       " 357: 'part',\n",
       " 358: 'enough',\n",
       " 359: 'money',\n",
       " 360: 'many',\n",
       " 361: 'kind',\n",
       " 362: 'lot',\n",
       " 363: '3',\n",
       " 364: 'hand',\n",
       " 365: 'room',\n",
       " 366: 'since',\n",
       " 367: 'next',\n",
       " 368: 'looking',\n",
       " 369: 'end',\n",
       " 370: 'men',\n",
       " 371: 'once',\n",
       " 372: 'big',\n",
       " 373: 'number',\n",
       " 374: 'father',\n",
       " 375: 'went',\n",
       " 376: 'both',\n",
       " 377: 'state',\n",
       " 378: 'face',\n",
       " 379: 'knew',\n",
       " 380: 'sir',\n",
       " 381: 'bad',\n",
       " 382: 'show',\n",
       " 383: 'set',\n",
       " 384: 'believe',\n",
       " 385: 'until',\n",
       " 386: 'girl',\n",
       " 387: 'while',\n",
       " 388: 'saw',\n",
       " 389: 'eyes',\n",
       " 390: 'order',\n",
       " 391: 'course',\n",
       " 392: 'point',\n",
       " 393: 'today',\n",
       " 394: 'care',\n",
       " 395: '-@@',\n",
       " 396: 'else',\n",
       " 397: 'looked',\n",
       " 398: 'best',\n",
       " 399: 'decision',\n",
       " 400: 'car',\n",
       " 401: 'year',\n",
       " 402: 'wait',\n",
       " 403: 'together',\n",
       " 404: '|',\n",
       " 405: 'against',\n",
       " 406: 'called',\n",
       " 407: 'second',\n",
       " 408: 'woman',\n",
       " 409: 'son',\n",
       " 410: 'nice',\n",
       " 411: 'try',\n",
       " 412: 'already',\n",
       " 413: 'asked',\n",
       " 414: 'heard',\n",
       " 415: 'ask',\n",
       " 416: 'guys',\n",
       " 417: 'remember',\n",
       " 418: 'kill',\n",
       " 419: 'mind',\n",
       " 420: 'able',\n",
       " 421: 'eu',\n",
       " 422: 'hear',\n",
       " 423: 'trying',\n",
       " 424: 'uh',\n",
       " 425: 'seen',\n",
       " 426: 'taken',\n",
       " 427: 'anyone',\n",
       " 428: 'few',\n",
       " 429: 're@@',\n",
       " 430: 'un@@',\n",
       " 431: 'dead',\n",
       " 432: 'understand',\n",
       " 433: 'stay',\n",
       " 434: 'job',\n",
       " 435: 'real',\n",
       " 436: 'door',\n",
       " 437: 'different',\n",
       " 438: 'days',\n",
       " 439: 'court',\n",
       " 440: 'system',\n",
       " 441: 'boy',\n",
       " 442: 'yet',\n",
       " 443: '4',\n",
       " 444: 'l',\n",
       " 445: 'friend',\n",
       " 446: 'means',\n",
       " 447: 'k@@',\n",
       " 448: 'talking',\n",
       " 449: 'directive',\n",
       " 450: 'least',\n",
       " 451: 'whole',\n",
       " 452: 'start',\n",
       " 453: 'happened',\n",
       " 454: 'mother',\n",
       " 455: 'body',\n",
       " 456: 'bring',\n",
       " 457: 'side',\n",
       " 458: 'coming',\n",
       " 459: 'open',\n",
       " 460: 'b',\n",
       " 461: 'o',\n",
       " 462: 'however',\n",
       " 463: 'thank',\n",
       " 464: 'everyone',\n",
       " 465: 'family',\n",
       " 466: 's',\n",
       " 467: 'behind',\n",
       " 468: 'water',\n",
       " 469: '5',\n",
       " 470: 'taking',\n",
       " 471: 'baby',\n",
       " 472: '10',\n",
       " 473: 'possible',\n",
       " 474: 'change',\n",
       " 475: 'following',\n",
       " 476: 'hell',\n",
       " 477: '2@@',\n",
       " 478: 'listen',\n",
       " 479: '-LSB-',\n",
       " 480: 'data',\n",
       " 481: 'hands',\n",
       " 482: '-RSB-',\n",
       " 483: 'move',\n",
       " 484: 'question',\n",
       " 485: 'e@@',\n",
       " 486: 'dad',\n",
       " 487: 'far',\n",
       " 488: 'guess',\n",
       " 489: 'ago',\n",
       " 490: 'wrong',\n",
       " 491: 'light',\n",
       " 492: 'gave',\n",
       " 493: 'soon',\n",
       " 494: 'within',\n",
       " 495: 'fact',\n",
       " 496: 'himself',\n",
       " 497: 'meet',\n",
       " 498: 'live',\n",
       " 499: 'market',\n",
       " 500: 'measures',\n",
       " 501: 'idea',\n",
       " 502: 'national',\n",
       " 503: 'getting',\n",
       " 504: 'yourself',\n",
       " 505: 'law',\n",
       " 506: 'ec',\n",
       " 507: 'fucking',\n",
       " 508: 'mom',\n",
       " 509: 'control',\n",
       " 510: 'union',\n",
       " 511: 'morning',\n",
       " 512: 'fine',\n",
       " 513: 'line',\n",
       " 514: 'de@@',\n",
       " 515: 'general',\n",
       " 516: 'gone',\n",
       " 517: 'probably',\n",
       " 518: 'shit',\n",
       " 519: 'working',\n",
       " 520: 'hard',\n",
       " 521: 'o@@',\n",
       " 522: 'moment',\n",
       " 523: 'important',\n",
       " 524: 'run',\n",
       " 525: 'actually',\n",
       " 526: 'school',\n",
       " 527: 'lost',\n",
       " 528: 'says',\n",
       " 529: 'a@@',\n",
       " 530: 'therefore',\n",
       " 531: 'council',\n",
       " 532: 'business',\n",
       " 533: 'turned',\n",
       " 534: 'five',\n",
       " 535: '3@@',\n",
       " 536: 'happy',\n",
       " 537: 'power',\n",
       " 538: 'death',\n",
       " 539: 'problem',\n",
       " 540: 'friends',\n",
       " 541: 'times',\n",
       " 542: 'killed',\n",
       " 543: 'report',\n",
       " 544: 'public',\n",
       " 545: 'minutes',\n",
       " 546: 'play',\n",
       " 547: 'though',\n",
       " 548: 'small',\n",
       " 549: 'high',\n",
       " 550: 'alone',\n",
       " 551: 'answer',\n",
       " 552: 'during',\n",
       " 553: 'ship',\n",
       " 554: 'young',\n",
       " 555: 'hope',\n",
       " 556: 'turn',\n",
       " 557: 'front',\n",
       " 558: 'community',\n",
       " 559: 'looks',\n",
       " 560: 'myself',\n",
       " 561: 'city',\n",
       " 562: 'blood',\n",
       " 563: 'wants',\n",
       " 564: 'hours',\n",
       " 565: 'four',\n",
       " 566: 'ready',\n",
       " 567: 's@@',\n",
       " 568: 'area',\n",
       " 569: 'die',\n",
       " 570: '6',\n",
       " 571: 'phone',\n",
       " 572: 'country',\n",
       " 573: 'close',\n",
       " 574: 'almost',\n",
       " 575: 'person',\n",
       " 576: 'pretty',\n",
       " 577: 'tried',\n",
       " 578: 'matter',\n",
       " 579: 'started',\n",
       " 580: 'less',\n",
       " 581: 'office',\n",
       " 582: 'later',\n",
       " 583: '/@@',\n",
       " 584: 'heart',\n",
       " 585: 'necessary',\n",
       " 586: 'according',\n",
       " 587: 'period',\n",
       " 588: 'full',\n",
       " 589: 'security',\n",
       " 590: 'bit',\n",
       " 591: 'rules',\n",
       " 592: 'wife',\n",
       " 593: 'ta',\n",
       " 594: 'air',\n",
       " 595: 'financial',\n",
       " 596: 'fuck',\n",
       " 597: 'party',\n",
       " 598: 'human',\n",
       " 599: 'saying',\n",
       " 600: 'kids',\n",
       " 601: 'rest',\n",
       " 602: 'given',\n",
       " 603: 'cut',\n",
       " 604: 'doctor',\n",
       " 605: 'company',\n",
       " 606: 'pay',\n",
       " 607: 'miss',\n",
       " 608: 'fire',\n",
       " 609: 'hold',\n",
       " 610: 'd@@',\n",
       " 611: 'tomorrow',\n",
       " 612: 'couple',\n",
       " 613: 'in@@',\n",
       " 614: 'e',\n",
       " 615: 'watch',\n",
       " 616: 'p@@',\n",
       " 617: 'felt',\n",
       " 618: 'clear',\n",
       " 619: 'en',\n",
       " 620: 'support',\n",
       " 621: 'date',\n",
       " 622: 'level',\n",
       " 623: 'plan',\n",
       " 624: 'd',\n",
       " 625: 'paragraph',\n",
       " 626: 't@@',\n",
       " 627: 'read',\n",
       " 628: 'group',\n",
       " 629: 'z@@',\n",
       " 630: 'black',\n",
       " 631: '20',\n",
       " 632: 'brought',\n",
       " 633: 'tonight',\n",
       " 634: 'word',\n",
       " 635: 'particular',\n",
       " 636: 'table',\n",
       " 637: 'n@@',\n",
       " 638: 'aid',\n",
       " 639: 'v',\n",
       " 640: 'application',\n",
       " 641: 'check',\n",
       " 642: 'reason',\n",
       " 643: 'inside',\n",
       " 644: 'half',\n",
       " 645: 'action',\n",
       " 646: 'months',\n",
       " 647: 'voice',\n",
       " 648: 'white',\n",
       " 649: 'true',\n",
       " 650: 'brother',\n",
       " 651: 'having',\n",
       " 652: 'above',\n",
       " 653: 'women',\n",
       " 654: 'met',\n",
       " 655: 'police',\n",
       " 656: 'beautiful',\n",
       " 657: 'crazy',\n",
       " 658: '12',\n",
       " 659: 'exactly',\n",
       " 660: 'free',\n",
       " 661: 'captain',\n",
       " 662: 'services',\n",
       " 663: 'week',\n",
       " 664: 'annex',\n",
       " 665: 'fight',\n",
       " 666: 'thinking',\n",
       " 667: 'list',\n",
       " 668: 'others',\n",
       " 669: '7',\n",
       " 670: 'service',\n",
       " 671: 'thanks',\n",
       " 672: 'war',\n",
       " 673: 'supposed',\n",
       " 674: 'shot',\n",
       " 675: 'de',\n",
       " 676: 'whether',\n",
       " 677: 'account',\n",
       " 678: 'king',\n",
       " 679: 'food',\n",
       " 680: 'chance',\n",
       " 681: 'deal',\n",
       " 682: 'either',\n",
       " 683: 'afraid',\n",
       " 684: 'm',\n",
       " 685: 'i@@',\n",
       " 686: 'procedure',\n",
       " 687: 'c@@',\n",
       " 688: 'knows',\n",
       " 689: 'making',\n",
       " 690: '8',\n",
       " 691: 'minute',\n",
       " 692: 'sleep',\n",
       " 693: 'happen',\n",
       " 694: 'seems',\n",
       " 695: 'accordance',\n",
       " 696: 'mo@@',\n",
       " 697: 'children',\n",
       " 698: 'hit',\n",
       " 699: 'subject',\n",
       " 700: 'sent',\n",
       " 701: 'makes',\n",
       " 702: 'g@@',\n",
       " 703: 'present',\n",
       " 704: '*',\n",
       " 705: 'whatever',\n",
       " 706: 't',\n",
       " 707: 'um',\n",
       " 708: 'authorities',\n",
       " 709: 'quite',\n",
       " 710: 'conditions',\n",
       " 711: 'certain',\n",
       " 712: 'b@@',\n",
       " 713: 'using',\n",
       " 714: 'main',\n",
       " 715: 'products',\n",
       " 716: 'game',\n",
       " 717: 'six',\n",
       " 718: 'comes',\n",
       " 719: 'send',\n",
       " 720: 'third',\n",
       " 721: 'special',\n",
       " 722: 'p.',\n",
       " 723: 'needs',\n",
       " 724: 'hello',\n",
       " 725: 'ma@@',\n",
       " 726: '0@@',\n",
       " 727: 'gun',\n",
       " 728: 'al@@',\n",
       " 729: 'waiting',\n",
       " 730: 'basis',\n",
       " 731: '4@@',\n",
       " 732: 'words',\n",
       " 733: 'appropriate',\n",
       " 734: 'hurt',\n",
       " 735: 'u@@',\n",
       " 736: 'feet',\n",
       " 737: 'form',\n",
       " 738: 'concerned',\n",
       " 739: 'late',\n",
       " 740: 'across',\n",
       " 741: 'seemed',\n",
       " 742: '15',\n",
       " 743: 'drink',\n",
       " 744: 'legal',\n",
       " 745: 'respect',\n",
       " 746: 'to@@',\n",
       " 747: 'k',\n",
       " 748: 'eat',\n",
       " 749: 'risk',\n",
       " 750: 'red',\n",
       " 751: 'en@@',\n",
       " 752: '.@@',\n",
       " 753: 'ground',\n",
       " 754: 'la@@',\n",
       " 755: 'specific',\n",
       " 756: 'easy',\n",
       " 757: 'based',\n",
       " 758: '30',\n",
       " 759: 'further',\n",
       " 760: 'provide',\n",
       " 761: 'view',\n",
       " 762: 'referred',\n",
       " 763: 'running',\n",
       " 764: 'er',\n",
       " 765: '$',\n",
       " 766: 'bed',\n",
       " 767: 'dark',\n",
       " 768: '9',\n",
       " 769: 'finally',\n",
       " 770: 'v@@',\n",
       " 771: 'government',\n",
       " 772: 'dr.',\n",
       " 773: 'ok',\n",
       " 774: 'test',\n",
       " 775: 'h@@',\n",
       " 776: 'g',\n",
       " 777: 'including',\n",
       " 778: 'land',\n",
       " 779: 'field',\n",
       " 780: 'stand',\n",
       " 781: 'hour',\n",
       " 782: 'provided',\n",
       " 783: 'become',\n",
       " 784: 'ro@@',\n",
       " 785: 'u',\n",
       " 786: 'died',\n",
       " 787: 'trust',\n",
       " 788: 'sometimes',\n",
       " 789: '5@@',\n",
       " 790: 'return',\n",
       " 791: 'co@@',\n",
       " 792: 'known',\n",
       " 793: 'available',\n",
       " 794: 'sit',\n",
       " 795: 'held',\n",
       " 796: 'agreement',\n",
       " 797: '7@@',\n",
       " 798: \"'cause\",\n",
       " 799: 'child',\n",
       " 800: 'million',\n",
       " 801: 'di@@',\n",
       " 802: 'situation',\n",
       " 803: 'outside',\n",
       " 804: 'story',\n",
       " 805: 'walk',\n",
       " 806: 'x',\n",
       " 807: 'break',\n",
       " 808: 'town',\n",
       " 809: 'oj',\n",
       " 810: 'eye',\n",
       " 811: 'longer',\n",
       " 812: 'countries',\n",
       " 813: 'y@@',\n",
       " 814: 'truth',\n",
       " 815: 'began',\n",
       " 816: '6@@',\n",
       " 817: 'damn',\n",
       " 818: 'goes',\n",
       " 819: 'along',\n",
       " 820: 'shut',\n",
       " 821: 'l@@',\n",
       " 822: 'several',\n",
       " 823: 'le@@',\n",
       " 824: 'position',\n",
       " 825: 'future',\n",
       " 826: 'kid',\n",
       " 827: 'huh',\n",
       " 828: 'team',\n",
       " 829: 'm@@',\n",
       " 830: 'treatment',\n",
       " 831: 'development',\n",
       " 832: 'past',\n",
       " 833: 'moved',\n",
       " 834: 'gets',\n",
       " 835: 'n',\n",
       " 836: 'mine',\n",
       " 837: 'costs',\n",
       " 838: 'li@@',\n",
       " 839: '11',\n",
       " 840: 'ra@@',\n",
       " 841: 'board',\n",
       " 842: 'act',\n",
       " 843: 'wish',\n",
       " 844: 'product',\n",
       " 845: 'committee',\n",
       " 846: 'policy',\n",
       " 847: 'force',\n",
       " 848: 'ten',\n",
       " 849: 'economic',\n",
       " 850: 'evidence',\n",
       " 851: 'book',\n",
       " 852: 'hair',\n",
       " 853: 'ti@@',\n",
       " 854: 'safe',\n",
       " 855: 'value',\n",
       " 856: 'anyway',\n",
       " 857: 'ar@@',\n",
       " 858: 'amount',\n",
       " 859: 'activities',\n",
       " 860: 'ran',\n",
       " 861: 'needed',\n",
       " 862: 'girls',\n",
       " 863: 'lo@@',\n",
       " 864: 'se@@',\n",
       " 865: 'key',\n",
       " 866: 'fun',\n",
       " 867: 'applicant',\n",
       " 868: 'cases',\n",
       " 869: 'apply',\n",
       " 870: 'single',\n",
       " 871: 'ensure',\n",
       " 872: 'wan',\n",
       " 873: 'common',\n",
       " 874: 'sound',\n",
       " 875: 'i.',\n",
       " 876: 'me@@',\n",
       " 877: 'forget',\n",
       " 878: 'pro@@',\n",
       " 879: 'fell',\n",
       " 880: 'access',\n",
       " 881: 'save',\n",
       " 882: 'relevant',\n",
       " 883: 'type',\n",
       " 884: 'lady',\n",
       " 885: 'sense',\n",
       " 886: 'protection',\n",
       " 887: 'parties',\n",
       " 888: 'president',\n",
       " 889: 'non-@@',\n",
       " 890: 'decided',\n",
       " 891: 'f@@',\n",
       " 892: 'stopped',\n",
       " 893: 'p',\n",
       " 894: 'worry',\n",
       " 895: 'trade',\n",
       " 896: 'management',\n",
       " 897: 'energy',\n",
       " 898: 'lead',\n",
       " 899: 'sa@@',\n",
       " 900: 'jack',\n",
       " 901: 'street',\n",
       " 902: 'caught',\n",
       " 903: 'stuff',\n",
       " 904: 'do@@',\n",
       " 905: 'stood',\n",
       " 906: 'file',\n",
       " 907: 'lives',\n",
       " 908: 'road',\n",
       " 909: 'top',\n",
       " 910: 'large',\n",
       " 911: 'speak',\n",
       " 912: 'pick',\n",
       " 913: '2009',\n",
       " 914: 'carried',\n",
       " 915: 'building',\n",
       " 916: 'lord',\n",
       " 917: 'ca@@',\n",
       " 918: 'news',\n",
       " 919: 'interest',\n",
       " 920: 'earth',\n",
       " 921: 'provisions',\n",
       " 922: 'official',\n",
       " 923: 'ass',\n",
       " 924: '2006',\n",
       " 925: 'r',\n",
       " 926: 'anymore',\n",
       " 927: 'patients',\n",
       " 928: 'meeting',\n",
       " 929: 'r@@',\n",
       " 930: 'questions',\n",
       " 931: 'si@@',\n",
       " 932: 'daughter',\n",
       " 933: 'implementation',\n",
       " 934: 'cold',\n",
       " 935: 'alive',\n",
       " 936: 'wall',\n",
       " 937: 'f',\n",
       " 938: 'forward',\n",
       " 939: 'kept',\n",
       " 940: 'perhaps',\n",
       " 941: 'production',\n",
       " 942: 'paid',\n",
       " 943: 'required',\n",
       " 944: 'j@@',\n",
       " 945: 'husband',\n",
       " 946: 'requirements',\n",
       " 947: 'ha@@',\n",
       " 948: 'buy',\n",
       " 949: 'fall',\n",
       " 950: 'considered',\n",
       " 951: 'floor',\n",
       " 952: 'telling',\n",
       " 953: 'figure',\n",
       " 954: 'rather',\n",
       " 955: 'rights',\n",
       " 956: 'mouth',\n",
       " 957: 'dear',\n",
       " 958: 'yours',\n",
       " 959: 'mi@@',\n",
       " 960: 'moving',\n",
       " 961: 'mrs.',\n",
       " 962: 'strong',\n",
       " 963: 'authority',\n",
       " 964: 'dog',\n",
       " 965: 'stupid',\n",
       " 966: 'hot',\n",
       " 967: '13',\n",
       " 968: 'programme',\n",
       " 969: 'be@@',\n",
       " 970: '2008',\n",
       " 971: 'living',\n",
       " 972: 'trouble',\n",
       " 973: 'changed',\n",
       " 974: 'total',\n",
       " 975: 'weeks',\n",
       " 976: 'entire',\n",
       " 977: 'te@@',\n",
       " 978: 'boys',\n",
       " 979: 'somewhere',\n",
       " 980: 'bank',\n",
       " 981: 'ai',\n",
       " 982: 'ri@@',\n",
       " 983: 'sign',\n",
       " 984: 'social',\n",
       " 985: 'section',\n",
       " 986: 'secret',\n",
       " 987: 'price',\n",
       " 988: 'bitch',\n",
       " 989: 'john',\n",
       " 990: 'miles',\n",
       " 991: 'space',\n",
       " 992: 'serious',\n",
       " 993: 'dinner',\n",
       " 994: 'age',\n",
       " 995: 'led',\n",
       " 996: 'mark',\n",
       " 997: 'catch',\n",
       " 998: 'lose',\n",
       " 999: 'international',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_transform.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\wiki.simple.vec: 293MB [00:25, 11.4MB/s]                               \n",
      "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████| 111051/111051 [00:11<00:00, 9527.14it/s] \n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# from torchtext.vocab import FastText\n",
    "# fast_vectors = FastText(language='simple') ##Load fasttext with language = simple\n",
    "# fast_embedding = fast_vectors.get_vecs_by_tokens(vocab_transform.get_itos()).to(device)\n",
    "\n",
    "# #since the fasttext  has 300 embedding\n",
    "# assert fast_embedding.shape == (len(vocab_transform), 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DataWrap(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "torch.manual_seed(6969)\n",
    "\n",
    "train = DataWrap(train_load)\n",
    "# val   = DataWrap(train_load.iloc[800:900] )\n",
    "# test  = DataWrap(train_load.iloc[900:])\n",
    "train_dataloader = DataLoader(train,batch_size=16, shuffle=True)\n",
    "# test_dataloader = DataLoader(val,  batch_size=16)\n",
    "# test_dataloader = DataLoader(test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_dataloader:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import ParentedTree\n",
    "# synt_ = '(ROOT (NP (NP (NN Finance)) (: :) (WHNP (WDT Which)) (SQ (VBZ is) (VP (VB be) (NP (DT an) (NNP Auroville) (NN resident)) (PP (IN for) (NP (DT a) (JJ long) (NN time))))) (. ?)))'\n",
    "# synt_ = ParentedTree.fromstring(synt_)\n",
    "# synt_ = deleaf(synt_)\n",
    "# synt_ = [f'<{w}>' for w in synt_]\n",
    "# synt_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
