{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "para_data = h5py.File('data.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"data.h5\" (mode r)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import deleaf\n",
    "num = 100000\n",
    "synts1 = list(para_data['train_synts1'][:num])\n",
    "\n",
    "synt1 = ['<s>'] + deleaf(synts1[2]) + ['</s>'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', \"b'(\", '(', 'FRAG', '(', 'SBAR', '(', 'IN', ')', '(', 'NP', '(', 'NP', '(', 'QP', '(', 'VBZ', ')', '(', 'IN', ')', '(', 'CD', ')', ')', '(', 'NN', ')', ')', '(', 'PP', '(', 'IN', ')', '(', 'PP', '(', 'IN', ')', ')', ')', ')', ')', '(', '.', ')', ')', \")'\", '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(synt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('synt_vocab.pkl', 'rb') as f:\n",
    "    synt_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import deleaf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_dataset(para_data, tokenizer, num):\n",
    "\n",
    "    max_sent_len = 40\n",
    "    max_synt_len = 160\n",
    "    \n",
    "    sents1 = list(para_data['train_sents1'][:num])\n",
    "    synts1 = list(para_data['train_synts1'][:num])\n",
    "    sents2 = list(para_data['train_sents2'][:num])\n",
    "    synts2 = list(para_data['train_synts2'][:num])\n",
    "\n",
    "    sent1_token_ids = torch.ones((num, max_sent_len+2), dtype=torch.long) \n",
    "    sent2_token_ids = torch.ones((num, max_sent_len+2), dtype=torch.long)    \t\t\n",
    "    synt1_token_ids = torch.ones((num, max_synt_len+2), dtype=torch.long) \n",
    "    synt2_token_ids = torch.ones((num, max_synt_len+2), dtype=torch.long)\n",
    "    synt1_bow = torch.ones((num, 74))\n",
    "    synt2_bow = torch.ones((num, 74))\n",
    "        \n",
    "    bsz = 64\n",
    "    \n",
    "    for i in tqdm(range(0, num, bsz)):\n",
    "        sent1_inputs = tokenizer(sents1[i:i+bsz], padding='max_length', truncation=True, max_length=max_sent_len+2, return_tensors=\"pt\")\n",
    "        sent2_inputs = tokenizer(sents2[i:i+bsz], padding='max_length', truncation=True, max_length=max_sent_len+2, return_tensors=\"pt\")\n",
    "        sent1_token_ids[i:i+bsz] = sent1_inputs['input_ids']\n",
    "        sent2_token_ids[i:i+bsz] = sent2_inputs['input_ids']\n",
    "\n",
    "    for i in tqdm(range(num)):\n",
    "        synt1 = ['<s>'] + deleaf(synts1[i]) + ['</s>']\n",
    "        synt1_token_ids[i, :len(synt1)] = torch.tensor([synt_vocab[tag] for tag in synt1])[:max_synt_len+2]\n",
    "        synt2 = ['<s>'] + deleaf(synts2[i]) + ['</s>']\n",
    "        synt2_token_ids[i, :len(synt2)] = torch.tensor([synt_vocab[tag] for tag in synt2])[:max_synt_len+2]\n",
    "        \n",
    "        for tag in synt1:\n",
    "            if tag != '<s>' and tag != '</s>':\n",
    "                synt1_bow[i][synt_vocab[tag]-3] += 1\n",
    "        for tag in synt2:\n",
    "            if tag != '<s>' and tag != '</s>':\n",
    "                synt2_bow[i][synt_vocab[tag]-3] += 1\n",
    "\n",
    "    synt1_bow /= synt1_bow.sum(1, keepdim=True)\n",
    "    synt2_bow /= synt2_bow.sum(1, keepdim=True)\n",
    "    \n",
    "    sum = 0\n",
    "    for i in range(num):\n",
    "        if torch.equal(synt1_bow[i], synt2_bow[i]):\n",
    "            sum += 1\n",
    "\n",
    "    return {'sent1':sent1_token_ids, 'sent2':sent2_token_ids, 'synt1': synt1_token_ids, 'synt2': synt2_token_ids,\n",
    "            'synt1bow': synt1_bow, 'synt2bow': synt2_bow}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir=\"./bart-base/\")\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1 = list(para_data['train_sents1'][:num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\check_data.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/check_data.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m prepare_dataset(para_data, tokenizer, num)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Thai-Paraphase\\check_data.ipynb Cell 8\u001b[0m in \u001b[0;36mprepare_dataset\u001b[1;34m(para_data, tokenizer, num)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/check_data.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m bsz \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/check_data.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num, bsz)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/check_data.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     sent1_inputs \u001b[39m=\u001b[39m tokenizer(sents1[i:i\u001b[39m+\u001b[39;49mbsz], padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49mmax_sent_len\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/check_data.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     sent2_inputs \u001b[39m=\u001b[39m tokenizer(sents2[i:i\u001b[39m+\u001b[39mbsz], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39mmax_sent_len\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Thai-Paraphase/check_data.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     sent1_token_ids[i:i\u001b[39m+\u001b[39mbsz] \u001b[39m=\u001b[39m sent1_inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2484\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2482\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2484\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2485\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2542\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2539\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2545\u001b[0m     )\n\u001b[0;32m   2547\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2549\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2550\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2551\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "dataset = prepare_dataset(para_data, tokenizer, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "check = torch.ones((1000000, 74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                arr[n + 1] = \"\"\n",
    "\n",
    "    nonleaves = \" \".join(arr)\n",
    "    return nonleaves.split()\n",
    "\n",
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SQ\n",
      "    (MD can)\n",
      "    (NP (PRP you))\n",
      "    (VP (VB adjust) (NP (DT the) (NNS cameras)))\n",
      "    (. ?)))\n",
      "['(', 'ROOT', '(', 'SQ', '(', 'MD', ')', '(', 'NP', '(', 'PRP', ')', ')', '(', 'VP', '(', 'VB', ')', '(', 'NP', '(', 'DT', ')', '(', 'NNS', ')', ')', ')', '(', '.', ')', ')', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk import ParentedTree\n",
    "\n",
    "# parse syntax and convert to tensor\n",
    "synt_ = '(ROOT (SQ (MD can) (NP (PRP you)) (VP (VB adjust) (NP (DT the) (NNS cameras))) (. ?)))'\n",
    "synt_ = ParentedTree.fromstring(synt_)\n",
    "print(synt_)\n",
    "synt_ = deleaf(synt_)\n",
    "print(synt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SQ\n",
      "    (MD can)\n",
      "    (NP (PRP you))\n",
      "    (VP (VB adjust) (NP (DT the) (NNS cameras)))\n",
      "    (. ?)))\n",
      "(ROOT (SQ (MD) (NP) (VP) (.)))\n"
     ]
    }
   ],
   "source": [
    "#parse syntax and get template\n",
    "from nltk import ParentedTree\n",
    "\n",
    "def tree2tmpl(tree, level, mlevel):\n",
    "    if level == mlevel:\n",
    "        for idx, n in enumerate(tree):\n",
    "            if isinstance(n, ParentedTree):\n",
    "                tree[idx] = \"(\" + n.label() + \")\"\n",
    "    else:\n",
    "        for n in tree:\n",
    "            tree2tmpl(n, level + 1, mlevel)\n",
    "\n",
    "\n",
    "tmpl_ = '(ROOT (SQ (MD can) (NP (PRP you)) (VP (VB adjust) (NP (DT the) (NNS cameras))) (. ?)))'\n",
    "tmpl_ = ParentedTree.fromstring(tmpl_)\n",
    "print(tmpl_)\n",
    "tree2tmpl(tmpl_, 1, 2)\n",
    "print(tmpl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paren(tok):\n",
    "    return tok == \")\" or tok == \"(\"\n",
    "\n",
    "def getleaf(tree):\n",
    "    nonleaves = ''\n",
    "    for w in str(tree).replace('\\n', '').split():\n",
    "        w = w.replace('(', '( ').replace(')', ' )')\n",
    "        nonleaves += w + ' '\n",
    "    \n",
    "    leaves = []\n",
    "    arr = nonleaves.split()\n",
    "    for n, i in enumerate(arr):\n",
    "        if n + 1 < len(arr):\n",
    "            tok1 = arr[n]\n",
    "            tok2 = arr[n + 1]\n",
    "            if not is_paren(tok1) and not is_paren(tok2):\n",
    "                leaves.append(arr[n])\n",
    "\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SQ\n",
      "    (MD can)\n",
      "    (NP (PRP you))\n",
      "    (VP (VB adjust) (NP (DT the) (NNS cameras)))\n",
      "    (. ?)))\n",
      "['MD', 'PRP', 'VB', 'DT', 'NNS', '.']\n"
     ]
    }
   ],
   "source": [
    "#tag Sequence\n",
    "sent_  = '(ROOT (SQ (MD can) (NP (PRP you)) (VP (VB adjust) (NP (DT the) (NNS cameras))) (. ?)))'\n",
    "sent_ = ParentedTree.fromstring(sent_)\n",
    "print(sent_)\n",
    "sent_ = getleaf(sent_)\n",
    "print(sent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MD', 'PRP', 'VB', 'DT', 'NNS', '.']\n",
      "(ROOT (SQ (MD) (NP) (VP) (.)))\n",
      "['(', 'ROOT', '(', 'SQ', '(', 'MD', ')', '(', 'NP', '(', 'PRP', ')', ')', '(', 'VP', '(', 'VB', ')', '(', 'NP', '(', 'DT', ')', '(', 'NNS', ')', ')', ')', '(', '.', ')', ')', ')']\n"
     ]
    }
   ],
   "source": [
    "print(sent_) #tag1 is the tag sequence of x1\n",
    "print(tmpl_) #t2 is the template of p2\n",
    "print(synt_) #target X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
